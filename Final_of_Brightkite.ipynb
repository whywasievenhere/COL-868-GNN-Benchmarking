{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final of Brightkight_LP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajPbHY22fXYA",
        "outputId": "da242d8c-117f-46ea-de85-3803f3240aa6"
      },
      "source": [
        "!pip install --no-cache-dir torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-geometric\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.6)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.51.2)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.21.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.8.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "--2021-04-02 17:37:58--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.20.200.43, 54.159.163.191, 3.211.152.205, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.20.200.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14746350 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.06M  18.2MB/s    in 0.8s    \n",
            "\n",
            "2021-04-02 17:37:59 (18.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [14746350/14746350]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZmE3qfIfYtm",
        "outputId": "8f35f8e9-7e27-4ac1-fb82-a53e64db3322"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9MqOA2ll4fu"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GATLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
        "\n",
        "    But, it's hopefully much more readable! (and of similar performance)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We'll use these constants in many functions so just extracting them here as member fields\n",
        "    src_nodes_dim = 0  # position of source nodes in edge index\n",
        "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
        "\n",
        "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
        "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
        "    head_dim = 1       # attention head dim\n",
        "\n",
        "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
        "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_of_heads = num_of_heads\n",
        "        self.num_out_features = num_out_features\n",
        "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
        "        self.add_skip_connection = add_skip_connection\n",
        "\n",
        "        #\n",
        "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
        "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
        "        #\n",
        "\n",
        "        # You can treat this one matrix as num_of_heads independent W matrices\n",
        "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "\n",
        "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
        "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
        "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
        "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
        "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "\n",
        "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
        "        if bias and concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
        "        elif bias and not concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        if add_skip_connection:\n",
        "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "        else:\n",
        "            self.register_parameter('skip_proj', None)\n",
        "\n",
        "        #\n",
        "        # End of trainable weights\n",
        "        #\n",
        "\n",
        "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
        "        self.activation = activation\n",
        "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
        "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
        "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
        "\n",
        "        self.init_params()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        #\n",
        "        # Step 1: Linear Projection + regularization\n",
        "        #\n",
        "\n",
        "        in_nodes_features, edge_index = data  # unpack data\n",
        "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
        "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
        "\n",
        "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
        "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
        "        in_nodes_features = self.dropout(in_nodes_features)\n",
        "\n",
        "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
        "        # We project the input node features into NH independent output features (one for each attention head)\n",
        "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
        "\n",
        "        #\n",
        "        # Step 2: Edge attention calculation\n",
        "        #\n",
        "\n",
        "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
        "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
        "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
        "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
        "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
        "\n",
        "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
        "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
        "        # by the edge index.\n",
        "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
        "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
        "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
        "\n",
        "        # shape = (E, NH, 1)\n",
        "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
        "        # Add stochasticity to neighborhood aggregation\n",
        "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
        "\n",
        "        #\n",
        "        # Step 3: Neighborhood aggregation\n",
        "        #\n",
        "\n",
        "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
        "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
        "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
        "\n",
        "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
        "        # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
        "\n",
        "        #\n",
        "        # Step 4: Residual/skip connections, concat and bias\n",
        "        #\n",
        "\n",
        "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
        "        return (out_nodes_features, edge_index)\n",
        "\n",
        "    #\n",
        "    # Helper functions (without comments there is very little code so don't be scared!)\n",
        "    #\n",
        "\n",
        "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
        "        \"\"\"\n",
        "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
        "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
        "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
        "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
        "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
        "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
        "\n",
        "        Note:\n",
        "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
        "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
        "        Check out this link for more details:\n",
        "\n",
        "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
        "\n",
        "        \"\"\"\n",
        "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
        "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
        "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
        "\n",
        "        # Calculate the denominator. shape = (E, NH)\n",
        "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
        "\n",
        "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
        "        # possibility of the computer rounding a very small number all the way to 0.\n",
        "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
        "\n",
        "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
        "        return attentions_per_edge.unsqueeze(-1)\n",
        "\n",
        "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
        "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
        "\n",
        "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
        "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes\n",
        "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
        "\n",
        "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
        "        # target index)\n",
        "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
        "\n",
        "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
        "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
        "        # shape = (N, NH) -> (E, NH)\n",
        "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
        "\n",
        "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
        "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
        "\n",
        "        # shape = (E) -> (E, NH, FOUT)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
        "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
        "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
        "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
        "\n",
        "        return out_nodes_features\n",
        "\n",
        "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
        "        \"\"\"\n",
        "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
        "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
        "\n",
        "        \"\"\"\n",
        "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
        "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
        "\n",
        "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
        "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
        "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
        "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
        "\n",
        "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
        "\n",
        "    def explicit_broadcast(self, this, other):\n",
        "        # Append singleton dimensions until this.dim() == other.dim()\n",
        "        for _ in range(this.dim(), other.dim()):\n",
        "            this = this.unsqueeze(-1)\n",
        "\n",
        "        # Explicitly expand so that shapes are the same\n",
        "        return this.expand_as(other)\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"\n",
        "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
        "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
        "\n",
        "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
        "        Feel free to experiment - there may be better initializations depending on your problem.\n",
        "\n",
        "        \"\"\"\n",
        "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
        "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
        "            self.attention_weights = attention_coefficients\n",
        "\n",
        "        if self.add_skip_connection:  # add skip or residual connection\n",
        "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
        "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
        "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
        "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
        "            else:\n",
        "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
        "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
        "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        if self.concat:\n",
        "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
        "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
        "        else:\n",
        "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
        "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out_nodes_features += self.bias\n",
        "\n",
        "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVgU-bQEmJeQ"
      },
      "source": [
        "class GAT1(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The most interesting and hardest implementation is implementation #3.\n",
        "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
        "\n",
        "    So I'll focus on imp #3 in this notebook.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,dropout=0.6, log_attention_weights=False):\n",
        "        super().__init__()\n",
        "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
        "\n",
        "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
        "        self.lin1 = nn.Linear(32, 1)\n",
        "        self.lin2 = nn.Linear(256, 1)\n",
        "        gat_layers = []  # collect GAT layers\n",
        "        for i in range(num_of_layers):\n",
        "            layer = GATLayer(\n",
        "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
        "                num_out_features=num_features_per_layer[i+1],\n",
        "                num_of_heads=num_heads_per_layer[i+1],\n",
        "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
        "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
        "                dropout_prob=dropout,\n",
        "                add_skip_connection=add_skip_connection,\n",
        "                bias=bias,\n",
        "                log_attention_weights=log_attention_weights\n",
        "            )\n",
        "            gat_layers.append(layer)\n",
        "\n",
        "        self.gat_net = nn.Sequential(\n",
        "            *gat_layers,\n",
        "        )\n",
        "\n",
        "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
        "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
        "    def forward(self, data):\n",
        "        return self.gat_net(data)\n",
        "\n",
        "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
        "        #z = (z - torch.mean(z))/(torch.std(z))\n",
        "        edge_list = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        #logits = (z[edge_list[0]] * z[edge_list[1]]).sum(dim=-1)\n",
        "        logits = torch.cat([z[edge_list[0]], z[edge_list[1]]], dim=-1)\n",
        "        logits = self.lin1(logits)\n",
        "        #logits = F.tanh(logits)\n",
        "        #logits = self.lin2(logits)\n",
        "        return logits"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUh4gUR2jcDg"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def nx_to_tg_data(graphs, features, edge_labels=None):\n",
        "    data_list = []\n",
        "    for i in range(len(graphs)):\n",
        "        feature = features[i]\n",
        "        graph = graphs[i].copy()\n",
        "        # graph.remove_edges_from(graph.selfloop_edges())\n",
        "\n",
        "        # relabel graphs\n",
        "        keys = list(graph.nodes)\n",
        "        vals = range(graph.number_of_nodes())\n",
        "        mapping = dict(zip(keys, vals))\n",
        "        nx.relabel_nodes(graph, mapping, copy=False)\n",
        "\n",
        "        x = np.zeros(feature.shape)\n",
        "        graph_nodes = list(graph.nodes)\n",
        "        for m in range(feature.shape[0]):\n",
        "            x[graph_nodes[m]] = feature[m]\n",
        "        x = torch.from_numpy(x).float()\n",
        "\n",
        "        # get edges\n",
        "        edge_index = np.array(list(graph.edges))\n",
        "        edge_index = np.concatenate((edge_index, edge_index[:,::-1]), axis=0)\n",
        "        edge_index = torch.from_numpy(edge_index).long().permute(1,0)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "        data_list.append(data)\n",
        "    return data_list"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ucNLKi1l0EG"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def split_edges(edges, remove_ratio):\n",
        "    e = edges.shape[1]\n",
        "    edges = edges[:, np.random.permutation(e)]\n",
        "    if True:\n",
        "        unique, counts = np.unique(edges, return_counts=True)\n",
        "        node_count = dict(zip(unique, counts))\n",
        "\n",
        "        index_train = []\n",
        "        index_val = []\n",
        "        for i in range(e):\n",
        "            node1 = edges[0,i]\n",
        "            node2 = edges[1,i]\n",
        "            if node_count[node1]>1 and node_count[node2]>1:\n",
        "                index_val.append(i)\n",
        "                node_count[node1] -= 1\n",
        "                node_count[node2] -= 1\n",
        "                if len(index_val) == int(e * remove_ratio):\n",
        "                    break\n",
        "            else:\n",
        "                index_train.append(i)\n",
        "        index_train = index_train + list(range(i + 1, e))\n",
        "\n",
        "        edges_train = edges[:, index_train]\n",
        "        edges_val = edges[:, index_val]\n",
        "        \n",
        "    return edges_train, edges_val"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtFPivBoXbLO",
        "outputId": "356fd42c-1f12-43ab-f842-88436b06c32a"
      },
      "source": [
        "import pickle\n",
        "pickle_file = open('/content/drive/My Drive/brightkite/updated_actual_features_dict.pickle', 'rb')\n",
        "features_dict = pickle.load(pickle_file)\n",
        "print(len(features))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSEC073geR7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d730796-7b6b-4bb8-83dd-1cb809e8eb25"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "DATA_DIR = '/content/drive/My Drive/brightkite/Brightkite_edges.txt'\n",
        "with open(DATA_DIR, 'rb') as f:\n",
        "    graph = nx.read_edgelist(f)\n",
        "print(graph.number_of_nodes(),\"*\")\n",
        "n = graph.number_of_nodes()\n",
        "#feature = np.ones((n, 1))\n",
        "feature = np.array([v for k, v in features_dict.items()])\n",
        "feature = (feature- np.mean(feature))/np.std(feature)\n",
        "graphs, features = [graph], [feature]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58228 *\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRDq9I9h1QR",
        "outputId": "64a4770c-4787-42c7-9685-6797a4f9f81d"
      },
      "source": [
        "data = nx_to_tg_data(graphs, features)[0]\n",
        "print(data)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 428156], x=[58228, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCaqx5ehrBsR",
        "outputId": "f6734d85-25e0-4b4c-a9ea-a85b40e006dc"
      },
      "source": [
        "data.true_edge_index = data.edge_index\n",
        "true_edge_index = data.edge_index.cpu().detach().numpy()\n",
        "data.edge_index, data.train_edges_pos = split_edges(true_edge_index, 0.5)\n",
        "data.edge_index, data.train_edges_pos = torch.tensor(data.edge_index), data.train_edges_pos\n",
        "print(data)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 214078], train_edges_pos=[[ 1306  9760  7175 ... 14781 18508 34331]\n",
            " [ 2040  2628  4805 ... 23312  1586  6789]], true_edge_index=[2, 428156], x=[58228, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSgYEMLAmdkW"
      },
      "source": [
        "def get_link_labels(pos_edge_index, neg_edge_index):\n",
        "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
        "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
        "    link_labels[:pos_edge_index.size(1)] = 1.\n",
        "    return link_labels\n",
        "\n",
        "def train_or_test(model, batch, loss_func, optimizer, phase):\n",
        "    (node_features, edge_index, mask_link_positive) = batch.x, batch.edge_index, batch.train_edges_pos\n",
        "    mask_link_negative = negative_sampling(edge_index= batch.true_edge_index, num_nodes=batch.num_nodes, num_neg_samples=mask_link_positive.shape[1])\n",
        "    node_features, edge_index = node_features.to(device), edge_index.to(device)\n",
        "    mask_link_positive, mask_link_negative = mask_link_positive.to(device), mask_link_negative.to(device)\n",
        "    graph_data = (node_features, edge_index)\n",
        "    \n",
        "    if phase=='Train':\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    \n",
        "    embeddings = model(graph_data)[0]\n",
        "    unnormalized_scores = model.decode(embeddings, mask_link_positive, mask_link_negative)\n",
        "    unnormalized_scores = unnormalized_scores.squeeze(1)\n",
        "    batch_target = get_link_labels(mask_link_positive, mask_link_negative)\n",
        "    loss = loss_func(unnormalized_scores, batch_target)\n",
        "    \n",
        "    if phase=='Train':\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    #Changing to Numpy to measure metrics\n",
        "    batch_pred = (unnormalized_scores > 0).float().cpu().numpy()\n",
        "    batch_target = batch_target.cpu().numpy()\n",
        "    unnormalized_scores =  unnormalized_scores.float()\n",
        "    unnormalized_scores = unnormalized_scores.cpu()\n",
        "    unnormalized_scores = unnormalized_scores.detach()\n",
        "    unnormalized_scores = unnormalized_scores.numpy() \n",
        "    #Metrics\n",
        "    micro_f1 = f1_score(batch_target, batch_pred, average='micro')\n",
        "    return [batch_pred, batch_target, micro_f1, loss.item(), unnormalized_scores]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUCu2GCWmrEb",
        "outputId": "94c0246a-5568-408d-dd17-71b0710020db"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
        "import numpy as np\n",
        "import time as time\n",
        "\n",
        "num_features = data.x.shape[1]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "config = {\n",
        "        'number_of_epochs': 1000,\n",
        "        'patience_period': 20,\n",
        "        'path': '/content/drive/My Drive/ppi_lp_models/',\n",
        "        'num_of_layers': 2,  # PPI has got 42% of nodes with all 0 features - that's why 3 layers are useful\n",
        "        'num_heads_per_layer': [1, 1],  # other values may give even better results from the reported ones\n",
        "        'num_features_per_layer': [num_features, 16, 16],  # 64 would also give ~0.975 uF1!\n",
        "        'add_skip_connection': True,  # skip connection is very important! (keep it otherwise micro-F1 is almost 0)\n",
        "        'bias': True,  # bias doesn't matter that much\n",
        "        'dropout': 0.0,  # dropout hurts the performance (best to keep it at 0)\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "dataset = np.empty(len(dataset_1), dtype=np.object)\n",
        "dataset[:] = dataset_1\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "val_acc_fin = []\n",
        "val_f1_fin = []\n",
        "val_roc_auc_fin = []\n",
        "\n",
        "train_acc_fin = []\n",
        "train_f1_fin = []\n",
        "train_roc_auc_fin = []\n",
        "\n",
        "#begin = time.time()\n",
        "avg_test_loss, avg_test_micro_f1, avg_test_micro_recall, avg_test_micro_precision, avg_test_micro_roc_auc_score  = [0, 0, 0, 0, 0]\n",
        "fold = 0\n",
        "\n",
        "begin = time.time()\n",
        "\n",
        "for dev_index, test_index in kf.split(np.arange(data.train_edges_pos.shape[1])):\n",
        "    fold = fold+1\n",
        "    \n",
        "    dev_edges = data.train_edges_pos[:, dev_index]\n",
        "    train_index, val_index = train_test_split(np.arange(dev_edges.shape[1]), test_size=0.2)\n",
        "    train_edges, val_edges = dev_edges[:, train_index], dev_edges[:, val_index]\n",
        "    test_edges = data.train_edges_pos[:, test_index]\n",
        "    \n",
        "    print(train_edges.shape, val_edges.shape, test_edges.shape)\n",
        "    \n",
        "    model = GAT1(\n",
        "          num_of_layers=config['num_of_layers'],\n",
        "          num_heads_per_layer=config['num_heads_per_layer'],\n",
        "          num_features_per_layer=config['num_features_per_layer'],\n",
        "          add_skip_connection=config['add_skip_connection'],\n",
        "          bias=config['bias'],\n",
        "          dropout=config['dropout'],\n",
        "          log_attention_weights=False  # no need to store attentions, used only in playground.py for visualizations\n",
        "          ).to(device)\n",
        "    \n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, eps=1e-3, amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, mode='max', verbose=True)\n",
        "\n",
        "    best_val_micro_f1, best_val_loss, patience_cnt = [0, 999999, 0]\n",
        "    \n",
        "    train_acc_list = []\n",
        "    train_f1_list = []\n",
        "    train_roc_auc_list = []\n",
        "    \n",
        "    \n",
        "    val_acc_list = []\n",
        "    val_f1_list = []\n",
        "    val_roc_auc_list = []\n",
        "\n",
        "\n",
        "    for epoch in range(config['number_of_epochs']):\n",
        "        \n",
        "        #Training\n",
        "        model.train()\n",
        "        batch1 = Data(x=data.x, edge_index=data.edge_index)\n",
        "        batch1.true_edge_index =data.true_edge_index.clone().detach()\n",
        "        batch1.x.requires_grad=True\n",
        "        batch1.train_edges_pos = torch.tensor(train_edges)\n",
        "        predictions, target, micro_f1, epoch_loss, store = train_or_test(model, batch1, loss_func, optimizer, 'Train')\n",
        "        \n",
        "        train_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "        train_acc = (predictions == target).sum()/(predictions.shape[0])\n",
        "        train_acc_list.append(train_acc)\n",
        "        train_f1_list.append(train_micro_f1)\n",
        "        roc_auc_value = roc_auc_score(target, store, average='micro')\n",
        "        macro_roc = roc_auc_score(target, store, average='macro')\n",
        "        train_roc_auc_list.append(roc_auc_value)\n",
        "        print(f\"Training Epoch Loss after {epoch} is {epoch_loss}\")\n",
        "\n",
        "        #Validation\n",
        "        model.eval()\n",
        "        batch2 = Data(x=data.x, edge_index=data.edge_index)\n",
        "        batch2.true_edge_index = data.true_edge_index.clone().detach()\n",
        "        batch2.train_edges_pos = torch.tensor(val_edges)\n",
        "        predictions, target, micro_f1, val_loss, store = train_or_test(model, batch2, loss_func, optimizer, 'Train')\n",
        "        \n",
        "        val_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "        val_acc = (predictions == target).sum()/(predictions.shape[0])\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_f1_list.append(val_micro_f1)\n",
        "        roc_auc_value = roc_auc_score(target, store, average='micro')\n",
        "        val_roc_auc_list.append(roc_auc_value)\n",
        "        macro_roc = roc_auc_score(target, store, average='macro')\n",
        "\n",
        "        if val_micro_f1 > best_val_micro_f1 or  best_val_loss - val_loss > 1e-5:\n",
        "            best_val_micro_f1 = max(val_micro_f1, best_val_micro_f1)\n",
        "            best_val_loss = min(val_loss, best_val_loss)\n",
        "            patience_cnt = 0\n",
        "            torch.save(model.state_dict(), config['path']+f'model_lp_{fold}')\n",
        "        else:\n",
        "            patience_cnt = patience_cnt+1\n",
        "        \n",
        "        temp = {}\n",
        "        temp[\"Validation Epoch Loss\"] = val_loss\n",
        "        temp[\"Epoch\"] = epoch\n",
        "        temp[\"Micro F1\"] = val_micro_f1\n",
        "        temp[\"Patience Count\"] = patience_cnt\n",
        "        temp[\"Best Val F1\"] = best_val_micro_f1\n",
        "        temp[\"Best Val Loss\"] = best_val_loss\n",
        "        temp[\"Micro ROC value\"] = roc_auc_value\n",
        "        temp[\"Macro ROC value\"] = macro_roc\n",
        "\n",
        "\n",
        "        print(temp)\n",
        "\n",
        "        if patience_cnt >= config['patience_period']:\n",
        "            break\n",
        "\n",
        "        scheduler.step(val_acc_list[-1])\n",
        "\n",
        "    train_acc_fin.append(train_acc_list)  \n",
        "    train_f1_fin.append(train_f1_list)\n",
        "    train_roc_auc_fin.append(train_roc_auc_list)\n",
        "    \n",
        "    val_acc_fin.append(val_acc_list)  \n",
        "    val_f1_fin.append(val_f1_list)\n",
        "    val_roc_auc_fin.append(val_roc_auc_list)\n",
        "\n",
        "    #Testing\n",
        "    model.load_state_dict(torch.load(config['path']+f'model_lp_{fold}'))\n",
        "    model.eval()\n",
        "    batch3 = Data(x=data.x, edge_index=data.edge_index)\n",
        "    batch3.true_edge_index = data.true_edge_index.clone().detach()\n",
        "    batch3.train_edges_pos = torch.tensor(test_edges)\n",
        "    predictions, target, micro_f1, test_loss, store = train_or_test(model, batch3, loss_func, optimizer, 'Train')\n",
        "    \n",
        "    test_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "    test_micro_precision = precision_score(target, predictions, average='micro')\n",
        "    test_micro_recall = recall_score(target, predictions, average='micro')\n",
        "    test_micro_roc_auc_score = roc_auc_score(target, store, average='micro')\n",
        "    test_macro_roc_auc_score = roc_auc_score(target, store, average='macro')\n",
        "\n",
        "    avg_test_loss = avg_test_loss + test_loss\n",
        "    avg_test_micro_f1 = avg_test_micro_f1 + test_micro_f1\n",
        "    avg_test_micro_recall = avg_test_micro_recall + test_micro_recall\n",
        "    avg_test_micro_roc_auc_score = avg_test_micro_roc_auc_score + test_micro_roc_auc_score\n",
        "\n",
        "    temp = {}\n",
        "    print(\"Testing stats.\\n\\n\\n\")\n",
        "    temp[\"Micro F1\"] = test_micro_f1 \n",
        "    temp[\"Micro Recall\"] = test_micro_recall\n",
        "    temp[\"Micro Precision\"] = test_micro_precision\n",
        "    temp[\"Micro ROC_AUC_Score\"] = test_micro_roc_auc_score\n",
        "    temp[\"Macro ROC_AUC_Score\"] = test_macro_roc_auc_score\n",
        "    print(temp)\n",
        "    print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"Average Statistics\")\n",
        "average = {}\n",
        "\n",
        "average[\"avg_test_micro_f1\"] = avg_test_micro_f1\n",
        "average[\"avg_test_loss\"] = avg_test_loss\n",
        "average[\"avg_test_micro_recall\"] = avg_test_micro_recall\n",
        "average[\"avg_test_micro_precision\"] = avg_test_micro_precision\n",
        "average[\"avg_test_micro_roc_auc_score\"] = avg_test_micro_roc_auc_score\n",
        "\n",
        "\n",
        "end = time.time()\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 137009) (2, 34253) (2, 42816)\n",
            "Training Epoch Loss after 0 is 0.7100927829742432\n",
            "{'Validation Epoch Loss': 0.6925360560417175, 'Epoch': 0, 'Micro F1': 0.5581116982454092, 'Patience Count': 0, 'Best Val F1': 0.5581116982454092, 'Best Val Loss': 0.6925360560417175, 'Micro ROC value': 0.5753192789900743, 'Macro ROC value': 0.5753192789900743}\n",
            "Training Epoch Loss after 1 is 0.6951198577880859\n",
            "{'Validation Epoch Loss': 0.6878843903541565, 'Epoch': 1, 'Micro F1': 0.5445879862783738, 'Patience Count': 0, 'Best Val F1': 0.5581116982454092, 'Best Val Loss': 0.6878843903541565, 'Micro ROC value': 0.6868757844536482, 'Macro ROC value': 0.6868757844536482}\n",
            "Training Epoch Loss after 2 is 0.677245557308197\n",
            "{'Validation Epoch Loss': 0.6687700152397156, 'Epoch': 2, 'Micro F1': 0.6395060286690217, 'Patience Count': 0, 'Best Val F1': 0.6395060286690217, 'Best Val Loss': 0.6687700152397156, 'Micro ROC value': 0.6880343999049581, 'Macro ROC value': 0.6880343999049581}\n",
            "Training Epoch Loss after 3 is 0.6626657247543335\n",
            "{'Validation Epoch Loss': 0.6551219820976257, 'Epoch': 3, 'Micro F1': 0.6520108021312313, 'Patience Count': 0, 'Best Val F1': 0.6520108021312313, 'Best Val Loss': 0.6551219820976257, 'Micro ROC value': 0.7099968678364553, 'Macro ROC value': 0.7099968678364553}\n",
            "Training Epoch Loss after 4 is 0.6443468928337097\n",
            "{'Validation Epoch Loss': 0.6317418217658997, 'Epoch': 4, 'Micro F1': 0.695068901086068, 'Patience Count': 0, 'Best Val F1': 0.695068901086068, 'Best Val Loss': 0.6317418217658997, 'Micro ROC value': 0.7537644925170071, 'Macro ROC value': 0.7537644925170071}\n",
            "Training Epoch Loss after 5 is 0.6186283826828003\n",
            "{'Validation Epoch Loss': 0.6074417233467102, 'Epoch': 5, 'Micro F1': 0.708728084901391, 'Patience Count': 0, 'Best Val F1': 0.708728084901391, 'Best Val Loss': 0.6074417233467102, 'Micro ROC value': 0.7865599070548672, 'Macro ROC value': 0.7865599070548672}\n",
            "Training Epoch Loss after 6 is 0.5948183536529541\n",
            "{'Validation Epoch Loss': 0.5808347463607788, 'Epoch': 6, 'Micro F1': 0.7245350772195136, 'Patience Count': 0, 'Best Val F1': 0.7245350772195136, 'Best Val Loss': 0.5808347463607788, 'Micro ROC value': 0.8036658246598454, 'Macro ROC value': 0.8036658246598454}\n",
            "Training Epoch Loss after 7 is 0.5652737617492676\n",
            "{'Validation Epoch Loss': 0.5529757738113403, 'Epoch': 7, 'Micro F1': 0.7466061366887572, 'Patience Count': 0, 'Best Val F1': 0.7466061366887572, 'Best Val Loss': 0.5529757738113403, 'Micro ROC value': 0.8117873053674984, 'Macro ROC value': 0.8117873053674984}\n",
            "Training Epoch Loss after 8 is 0.5396814346313477\n",
            "{'Validation Epoch Loss': 0.5276491641998291, 'Epoch': 8, 'Micro F1': 0.7461098297959303, 'Patience Count': 0, 'Best Val F1': 0.7466061366887572, 'Best Val Loss': 0.5276491641998291, 'Micro ROC value': 0.8232738394727679, 'Macro ROC value': 0.8232738394727679}\n",
            "Training Epoch Loss after 9 is 0.5144611597061157\n",
            "{'Validation Epoch Loss': 0.505168616771698, 'Epoch': 9, 'Micro F1': 0.7542953069119043, 'Patience Count': 0, 'Best Val F1': 0.7542953069119043, 'Best Val Loss': 0.505168616771698, 'Micro ROC value': 0.8333179065979756, 'Macro ROC value': 0.8333179065979756}\n",
            "Training Epoch Loss after 10 is 0.4963623881340027\n",
            "{'Validation Epoch Loss': 0.491860032081604, 'Epoch': 10, 'Micro F1': 0.7587656555630163, 'Patience Count': 0, 'Best Val F1': 0.7587656555630163, 'Best Val Loss': 0.491860032081604, 'Micro ROC value': 0.841034278980328, 'Macro ROC value': 0.841034278980328}\n",
            "Training Epoch Loss after 11 is 0.48767194151878357\n",
            "{'Validation Epoch Loss': 0.4868170917034149, 'Epoch': 11, 'Micro F1': 0.7644001985227571, 'Patience Count': 0, 'Best Val F1': 0.7644001985227571, 'Best Val Loss': 0.4868170917034149, 'Micro ROC value': 0.8442234348009057, 'Macro ROC value': 0.8442234348009057}\n",
            "Training Epoch Loss after 12 is 0.4856347143650055\n",
            "{'Validation Epoch Loss': 0.48606523871421814, 'Epoch': 12, 'Micro F1': 0.7662764218147845, 'Patience Count': 0, 'Best Val F1': 0.7662764218147845, 'Best Val Loss': 0.48606523871421814, 'Micro ROC value': 0.8471895734343828, 'Macro ROC value': 0.8471895734343828}\n",
            "Training Epoch Loss after 13 is 0.4856936037540436\n",
            "{'Validation Epoch Loss': 0.4839443564414978, 'Epoch': 13, 'Micro F1': 0.7665795658584296, 'Patience Count': 0, 'Best Val F1': 0.7665795658584296, 'Best Val Loss': 0.4839443564414978, 'Micro ROC value': 0.8496577072155862, 'Macro ROC value': 0.8496577072155862}\n",
            "Training Epoch Loss after 14 is 0.480283260345459\n",
            "{'Validation Epoch Loss': 0.480211466550827, 'Epoch': 14, 'Micro F1': 0.7685312235424634, 'Patience Count': 0, 'Best Val F1': 0.7685312235424634, 'Best Val Loss': 0.480211466550827, 'Micro ROC value': 0.8522238485409859, 'Macro ROC value': 0.8522238485409859}\n",
            "Training Epoch Loss after 15 is 0.47723332047462463\n",
            "{'Validation Epoch Loss': 0.4745115041732788, 'Epoch': 15, 'Micro F1': 0.7783519451134954, 'Patience Count': 0, 'Best Val F1': 0.7783519451134954, 'Best Val Loss': 0.4745115041732788, 'Micro ROC value': 0.8545088068536616, 'Macro ROC value': 0.8545088068536616}\n",
            "Training Epoch Loss after 16 is 0.4722762107849121\n",
            "{'Validation Epoch Loss': 0.47058364748954773, 'Epoch': 16, 'Micro F1': 0.7823630736891276, 'Patience Count': 0, 'Best Val F1': 0.7823630736891276, 'Best Val Loss': 0.47058364748954773, 'Micro ROC value': 0.8561206145516072, 'Macro ROC value': 0.8561206145516072}\n",
            "Training Epoch Loss after 17 is 0.47165223956108093\n",
            "{'Validation Epoch Loss': 0.4708871841430664, 'Epoch': 17, 'Micro F1': 0.779085043645812, 'Patience Count': 1, 'Best Val F1': 0.7823630736891276, 'Best Val Loss': 0.47058364748954773, 'Micro ROC value': 0.8536780077671069, 'Macro ROC value': 0.8536780077671069}\n",
            "Training Epoch Loss after 18 is 0.46972769498825073\n",
            "{'Validation Epoch Loss': 0.46905210614204407, 'Epoch': 18, 'Micro F1': 0.7808367150322599, 'Patience Count': 0, 'Best Val F1': 0.7823630736891276, 'Best Val Loss': 0.46905210614204407, 'Micro ROC value': 0.8545343568640675, 'Macro ROC value': 0.8545343568640675}\n",
            "Training Epoch Loss after 19 is 0.4673127830028534\n",
            "{'Validation Epoch Loss': 0.4694993197917938, 'Epoch': 19, 'Micro F1': 0.7824246405371871, 'Patience Count': 0, 'Best Val F1': 0.7824246405371871, 'Best Val Loss': 0.46905210614204407, 'Micro ROC value': 0.8555387009338656, 'Macro ROC value': 0.8555387009338656}\n",
            "Training Epoch Loss after 20 is 0.4676372706890106\n",
            "{'Validation Epoch Loss': 0.4681059718132019, 'Epoch': 20, 'Micro F1': 0.7817855370332526, 'Patience Count': 0, 'Best Val F1': 0.7824246405371871, 'Best Val Loss': 0.4681059718132019, 'Micro ROC value': 0.8566186917996841, 'Macro ROC value': 0.8566186917996841}\n",
            "Training Epoch Loss after 21 is 0.46784672141075134\n",
            "{'Validation Epoch Loss': 0.4661676585674286, 'Epoch': 21, 'Micro F1': 0.7842086824511721, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8581395378351273, 'Macro ROC value': 0.8581395378351273}\n",
            "Training Epoch Loss after 22 is 0.4677687883377075\n",
            "{'Validation Epoch Loss': 0.4666541516780853, 'Epoch': 22, 'Micro F1': 0.7795165245825061, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8574519789069499, 'Macro ROC value': 0.8574519789069499}\n",
            "Training Epoch Loss after 23 is 0.4664019048213959\n",
            "{'Validation Epoch Loss': 0.46624287962913513, 'Epoch': 23, 'Micro F1': 0.7830846933115349, 'Patience Count': 2, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8577641734711272, 'Macro ROC value': 0.8577641734711272}\n",
            "Training Epoch Loss after 24 is 0.46641892194747925\n",
            "{'Validation Epoch Loss': 0.46626153588294983, 'Epoch': 24, 'Micro F1': 0.7813996262991942, 'Patience Count': 3, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8581333928505763, 'Macro ROC value': 0.8581333928505763}\n",
            "Training Epoch Loss after 25 is 0.4647737741470337\n",
            "{'Validation Epoch Loss': 0.46735844016075134, 'Epoch': 25, 'Micro F1': 0.780179838262342, 'Patience Count': 4, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8568675893216142, 'Macro ROC value': 0.8568675893216142}\n",
            "Training Epoch Loss after 26 is 0.4637860953807831\n",
            "{'Validation Epoch Loss': 0.4677042067050934, 'Epoch': 26, 'Micro F1': 0.7795959478001927, 'Patience Count': 5, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4661676585674286, 'Micro ROC value': 0.8572127227411687, 'Macro ROC value': 0.8572127227411687}\n",
            "Training Epoch Loss after 27 is 0.46449294686317444\n",
            "{'Validation Epoch Loss': 0.4658576250076294, 'Epoch': 27, 'Micro F1': 0.7804539814612074, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4658576250076294, 'Micro ROC value': 0.8586954733852714, 'Macro ROC value': 0.8586954733852714}\n",
            "Training Epoch Loss after 28 is 0.4636594355106354\n",
            "{'Validation Epoch Loss': 0.4644915461540222, 'Epoch': 28, 'Micro F1': 0.7820920795258809, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4644915461540222, 'Micro ROC value': 0.8596271182401258, 'Macro ROC value': 0.8596271182401258}\n",
            "Training Epoch Loss after 29 is 0.46501484513282776\n",
            "{'Validation Epoch Loss': 0.4643000066280365, 'Epoch': 29, 'Micro F1': 0.7803258108778792, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4643000066280365, 'Micro ROC value': 0.8595770533789437, 'Macro ROC value': 0.8595770533789437}\n",
            "Training Epoch Loss after 30 is 0.4637405574321747\n",
            "{'Validation Epoch Loss': 0.46445024013519287, 'Epoch': 30, 'Micro F1': 0.7817385592292534, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.4643000066280365, 'Micro ROC value': 0.8596918553884499, 'Macro ROC value': 0.8596918553884499}\n",
            "Training Epoch Loss after 31 is 0.46347251534461975\n",
            "{'Validation Epoch Loss': 0.46416640281677246, 'Epoch': 31, 'Micro F1': 0.7816103698946077, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46416640281677246, 'Micro ROC value': 0.8598338037528475, 'Macro ROC value': 0.8598338037528475}\n",
            "Training Epoch Loss after 32 is 0.4642355740070343\n",
            "{'Validation Epoch Loss': 0.465131938457489, 'Epoch': 32, 'Micro F1': 0.7808855086638541, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46416640281677246, 'Micro ROC value': 0.8590890115437702, 'Macro ROC value': 0.8590890115437702}\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 33 is 0.4645107686519623\n",
            "{'Validation Epoch Loss': 0.46217525005340576, 'Epoch': 33, 'Micro F1': 0.7811578547864422, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8608911018215617, 'Macro ROC value': 0.8608911018215617}\n",
            "Training Epoch Loss after 34 is 0.46413135528564453\n",
            "{'Validation Epoch Loss': 0.4647213816642761, 'Epoch': 34, 'Micro F1': 0.7808855086638541, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.859315968061618, 'Macro ROC value': 0.859315968061618}\n",
            "Training Epoch Loss after 35 is 0.4646248519420624\n",
            "{'Validation Epoch Loss': 0.4652840197086334, 'Epoch': 35, 'Micro F1': 0.7813914109713018, 'Patience Count': 2, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.859030473658811, 'Macro ROC value': 0.859030473658811}\n",
            "Training Epoch Loss after 36 is 0.46431779861450195\n",
            "{'Validation Epoch Loss': 0.46261826157569885, 'Epoch': 36, 'Micro F1': 0.7819835338082448, 'Patience Count': 3, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8604546105914945, 'Macro ROC value': 0.8604546105914945}\n",
            "Training Epoch Loss after 37 is 0.46365493535995483\n",
            "{'Validation Epoch Loss': 0.46430012583732605, 'Epoch': 37, 'Micro F1': 0.7814840225975505, 'Patience Count': 4, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8597827398996007, 'Macro ROC value': 0.8597827398996007}\n",
            "Training Epoch Loss after 38 is 0.4648366868495941\n",
            "{'Validation Epoch Loss': 0.46501460671424866, 'Epoch': 38, 'Micro F1': 0.7803599842342671, 'Patience Count': 5, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8586449543233572, 'Macro ROC value': 0.8586449543233572}\n",
            "Training Epoch Loss after 39 is 0.46391814947128296\n",
            "{'Validation Epoch Loss': 0.4646054804325104, 'Epoch': 39, 'Micro F1': 0.7805739643242927, 'Patience Count': 6, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8592510038343677, 'Macro ROC value': 0.8592510038343677}\n",
            "Training Epoch Loss after 40 is 0.46468445658683777\n",
            "{'Validation Epoch Loss': 0.46550822257995605, 'Epoch': 40, 'Micro F1': 0.7797322744989269, 'Patience Count': 7, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8587896053007025, 'Macro ROC value': 0.8587896053007025}\n",
            "Training Epoch Loss after 41 is 0.4634888768196106\n",
            "{'Validation Epoch Loss': 0.4638484716415405, 'Epoch': 41, 'Micro F1': 0.7811870493095496, 'Patience Count': 8, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8597057971943731, 'Macro ROC value': 0.8597057971943731}\n",
            "Training Epoch Loss after 42 is 0.46358522772789\n",
            "{'Validation Epoch Loss': 0.46479010581970215, 'Epoch': 42, 'Micro F1': 0.7810264794324586, 'Patience Count': 9, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8593062026461509, 'Macro ROC value': 0.8593062026461509}\n",
            "Training Epoch Loss after 43 is 0.4633930027484894\n",
            "{'Validation Epoch Loss': 0.4631573557853699, 'Epoch': 43, 'Micro F1': 0.7822107383726022, 'Patience Count': 10, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46217525005340576, 'Micro ROC value': 0.8604606468343109, 'Macro ROC value': 0.8604606468343109}\n",
            "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 44 is 0.46381819248199463\n",
            "{'Validation Epoch Loss': 0.46207672357559204, 'Epoch': 44, 'Micro F1': 0.7820482877412197, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46207672357559204, 'Micro ROC value': 0.8609305795876345, 'Macro ROC value': 0.8609305795876345}\n",
            "Training Epoch Loss after 45 is 0.4622194468975067\n",
            "{'Validation Epoch Loss': 0.4631108343601227, 'Epoch': 45, 'Micro F1': 0.780734534201384, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46207672357559204, 'Micro ROC value': 0.860243305670836, 'Macro ROC value': 0.860243305670836}\n",
            "Training Epoch Loss after 46 is 0.4629773795604706\n",
            "{'Validation Epoch Loss': 0.46094778180122375, 'Epoch': 46, 'Micro F1': 0.7831576796193034, 'Patience Count': 0, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8614608518657736, 'Macro ROC value': 0.8614608518657736}\n",
            "Training Epoch Loss after 47 is 0.4642132520675659\n",
            "{'Validation Epoch Loss': 0.46286362409591675, 'Epoch': 47, 'Micro F1': 0.782556017808919, 'Patience Count': 1, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8603852888119595, 'Macro ROC value': 0.8603852888119595}\n",
            "Training Epoch Loss after 48 is 0.46475809812545776\n",
            "{'Validation Epoch Loss': 0.46354687213897705, 'Epoch': 48, 'Micro F1': 0.7807053396782764, 'Patience Count': 2, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8599273752975907, 'Macro ROC value': 0.8599273752975907}\n",
            "Training Epoch Loss after 49 is 0.4627779722213745\n",
            "{'Validation Epoch Loss': 0.463602751493454, 'Epoch': 49, 'Micro F1': 0.7810314876720728, 'Patience Count': 3, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8596943674388582, 'Macro ROC value': 0.8596943674388582}\n",
            "Training Epoch Loss after 50 is 0.4629729688167572\n",
            "{'Validation Epoch Loss': 0.4668058454990387, 'Epoch': 50, 'Micro F1': 0.7806583461061236, 'Patience Count': 4, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8579430768611469, 'Macro ROC value': 0.8579430768611469}\n",
            "Training Epoch Loss after 51 is 0.46296435594558716\n",
            "{'Validation Epoch Loss': 0.4648461639881134, 'Epoch': 51, 'Micro F1': 0.7823694274954018, 'Patience Count': 5, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8590734689502645, 'Macro ROC value': 0.8590734689502645}\n",
            "Training Epoch Loss after 52 is 0.46455124020576477\n",
            "{'Validation Epoch Loss': 0.4626980721950531, 'Epoch': 52, 'Micro F1': 0.7829679152191049, 'Patience Count': 6, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.860684022110757, 'Macro ROC value': 0.860684022110757}\n",
            "Training Epoch Loss after 53 is 0.4633042514324188\n",
            "{'Validation Epoch Loss': 0.4659731686115265, 'Epoch': 53, 'Micro F1': 0.7812746328788719, 'Patience Count': 7, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8584577439032517, 'Macro ROC value': 0.8584577439032517}\n",
            "Training Epoch Loss after 54 is 0.4646126627922058\n",
            "{'Validation Epoch Loss': 0.4635835886001587, 'Epoch': 54, 'Micro F1': 0.7808221177707062, 'Patience Count': 8, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8598546655677203, 'Macro ROC value': 0.8598546655677203}\n",
            "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 55 is 0.4624403417110443\n",
            "{'Validation Epoch Loss': 0.4634561240673065, 'Epoch': 55, 'Micro F1': 0.7822964411876332, 'Patience Count': 9, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8599295495663684, 'Macro ROC value': 0.8599295495663684}\n",
            "Training Epoch Loss after 56 is 0.4638957679271698\n",
            "{'Validation Epoch Loss': 0.46449699997901917, 'Epoch': 56, 'Micro F1': 0.7809680903862435, 'Patience Count': 10, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8593066467049644, 'Macro ROC value': 0.8593066467049644}\n",
            "Training Epoch Loss after 57 is 0.4639027714729309\n",
            "{'Validation Epoch Loss': 0.4649031460285187, 'Epoch': 57, 'Micro F1': 0.7811222702323952, 'Patience Count': 11, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8593725196966777, 'Macro ROC value': 0.8593725196966777}\n",
            "Training Epoch Loss after 58 is 0.46379417181015015\n",
            "{'Validation Epoch Loss': 0.46288976073265076, 'Epoch': 58, 'Micro F1': 0.7821066767874348, 'Patience Count': 12, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8603338220738959, 'Macro ROC value': 0.8603338220738959}\n",
            "Training Epoch Loss after 59 is 0.46269911527633667\n",
            "{'Validation Epoch Loss': 0.4636361300945282, 'Epoch': 59, 'Micro F1': 0.7820190932181121, 'Patience Count': 13, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8598316405642318, 'Macro ROC value': 0.8598316405642318}\n",
            "Training Epoch Loss after 60 is 0.46382832527160645\n",
            "{'Validation Epoch Loss': 0.4672618508338928, 'Epoch': 60, 'Micro F1': 0.7788628567257865, 'Patience Count': 14, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8574004134773633, 'Macro ROC value': 0.8574004134773633}\n",
            "Training Epoch Loss after 61 is 0.463469922542572\n",
            "{'Validation Epoch Loss': 0.4637095630168915, 'Epoch': 61, 'Micro F1': 0.7819023151256824, 'Patience Count': 15, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8598602047965668, 'Macro ROC value': 0.8598602047965668}\n",
            "Training Epoch Loss after 62 is 0.4638049006462097\n",
            "{'Validation Epoch Loss': 0.4642242193222046, 'Epoch': 62, 'Micro F1': 0.7816833562023764, 'Patience Count': 16, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8593872224977712, 'Macro ROC value': 0.8593872224977712}\n",
            "Training Epoch Loss after 63 is 0.46306097507476807\n",
            "{'Validation Epoch Loss': 0.4618796110153198, 'Epoch': 63, 'Micro F1': 0.7836539865121304, 'Patience Count': 17, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8612961648560555, 'Macro ROC value': 0.8612961648560555}\n",
            "Training Epoch Loss after 64 is 0.464406818151474\n",
            "{'Validation Epoch Loss': 0.46714529395103455, 'Epoch': 64, 'Micro F1': 0.7790818188453397, 'Patience Count': 18, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8573896206579995, 'Macro ROC value': 0.8573896206579995}\n",
            "Training Epoch Loss after 65 is 0.46489259600639343\n",
            "{'Validation Epoch Loss': 0.46474337577819824, 'Epoch': 65, 'Micro F1': 0.7808367150322599, 'Patience Count': 19, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8590565665887853, 'Macro ROC value': 0.8590565665887853}\n",
            "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Training Epoch Loss after 66 is 0.4634576141834259\n",
            "{'Validation Epoch Loss': 0.4636975824832916, 'Epoch': 66, 'Micro F1': 0.7823484760014013, 'Patience Count': 20, 'Best Val F1': 0.7842086824511721, 'Best Val Loss': 0.46094778180122375, 'Micro ROC value': 0.8600381507321522, 'Macro ROC value': 0.8600381507321522}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.7753474249678851, 'Micro Recall': 0.7753474249678851, 'Micro Precision': 0.7753474249678851, 'Micro ROC_AUC_Score': 0.8523210713405531, 'Macro ROC_AUC_Score': 0.8523210713405531}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(2, 137009) (2, 34253) (2, 42816)\n",
            "Training Epoch Loss after 0 is 0.6948277354240417\n",
            "{'Validation Epoch Loss': 0.6891561150550842, 'Epoch': 0, 'Micro F1': 0.5900359092634222, 'Patience Count': 0, 'Best Val F1': 0.5900359092634222, 'Best Val Loss': 0.6891561150550842, 'Micro ROC value': 0.5433431991752193, 'Macro ROC value': 0.5433431991752193}\n",
            "Training Epoch Loss after 1 is 0.6842807531356812\n",
            "{'Validation Epoch Loss': 0.679374098777771, 'Epoch': 1, 'Micro F1': 0.6108402429055237, 'Patience Count': 0, 'Best Val F1': 0.6108402429055237, 'Best Val Loss': 0.679374098777771, 'Micro ROC value': 0.5946239895398251, 'Macro ROC value': 0.5946239895398251}\n",
            "Training Epoch Loss after 2 is 0.6750638484954834\n",
            "{'Validation Epoch Loss': 0.670954167842865, 'Epoch': 2, 'Micro F1': 0.621420021603947, 'Patience Count': 0, 'Best Val F1': 0.621420021603947, 'Best Val Loss': 0.670954167842865, 'Micro ROC value': 0.5966263761820509, 'Macro ROC value': 0.5966263761820509}\n",
            "Training Epoch Loss after 3 is 0.667078971862793\n",
            "{'Validation Epoch Loss': 0.662233829498291, 'Epoch': 3, 'Micro F1': 0.6438268180889265, 'Patience Count': 0, 'Best Val F1': 0.6438268180889265, 'Best Val Loss': 0.662233829498291, 'Micro ROC value': 0.6084024660387719, 'Macro ROC value': 0.6084024660387719}\n",
            "Training Epoch Loss after 4 is 0.6586613059043884\n",
            "{'Validation Epoch Loss': 0.6538766026496887, 'Epoch': 4, 'Micro F1': 0.6511546433889003, 'Patience Count': 0, 'Best Val F1': 0.6511546433889003, 'Best Val Loss': 0.6538766026496887, 'Micro ROC value': 0.6137996378285296, 'Macro ROC value': 0.6137996378285296}\n",
            "Training Epoch Loss after 5 is 0.6508315801620483\n",
            "{'Validation Epoch Loss': 0.6470968127250671, 'Epoch': 5, 'Micro F1': 0.6513881995737599, 'Patience Count': 0, 'Best Val F1': 0.6513881995737599, 'Best Val Loss': 0.6470968127250671, 'Micro ROC value': 0.6194072730402044, 'Macro ROC value': 0.6194072730402044}\n",
            "Training Epoch Loss after 6 is 0.6436642408370972\n",
            "{'Validation Epoch Loss': 0.6377092003822327, 'Epoch': 6, 'Micro F1': 0.6624821183545967, 'Patience Count': 0, 'Best Val F1': 0.6624821183545967, 'Best Val Loss': 0.6377092003822327, 'Micro ROC value': 0.6323274437801534, 'Macro ROC value': 0.6323274437801534}\n",
            "Training Epoch Loss after 7 is 0.6355822086334229\n",
            "{'Validation Epoch Loss': 0.6312271952629089, 'Epoch': 7, 'Micro F1': 0.666233614573906, 'Patience Count': 0, 'Best Val F1': 0.666233614573906, 'Best Val Loss': 0.6312271952629089, 'Micro ROC value': 0.6361572835657194, 'Macro ROC value': 0.6361572835657194}\n",
            "Training Epoch Loss after 8 is 0.6301921606063843\n",
            "{'Validation Epoch Loss': 0.6247815489768982, 'Epoch': 8, 'Micro F1': 0.6694305358889392, 'Patience Count': 0, 'Best Val F1': 0.6694305358889392, 'Best Val Loss': 0.6247815489768982, 'Micro ROC value': 0.6454753394715707, 'Macro ROC value': 0.6454753394715707}\n",
            "Training Epoch Loss after 9 is 0.6236069202423096\n",
            "{'Validation Epoch Loss': 0.6203933954238892, 'Epoch': 9, 'Micro F1': 0.669717672476716, 'Patience Count': 0, 'Best Val F1': 0.669717672476716, 'Best Val Loss': 0.6203933954238892, 'Micro ROC value': 0.6565252286143455, 'Macro ROC value': 0.6565252286143455}\n",
            "Training Epoch Loss after 10 is 0.6185453534126282\n",
            "{'Validation Epoch Loss': 0.6153966188430786, 'Epoch': 10, 'Micro F1': 0.6717659767027706, 'Patience Count': 0, 'Best Val F1': 0.6717659767027706, 'Best Val Loss': 0.6153966188430786, 'Micro ROC value': 0.6755057339162479, 'Macro ROC value': 0.6755057339162479}\n",
            "Training Epoch Loss after 11 is 0.615679919719696\n",
            "{'Validation Epoch Loss': 0.6104462146759033, 'Epoch': 11, 'Micro F1': 0.6742865484271221, 'Patience Count': 0, 'Best Val F1': 0.6742865484271221, 'Best Val Loss': 0.6104462146759033, 'Micro ROC value': 0.700818194409333, 'Macro ROC value': 0.700818194409333}\n",
            "Training Epoch Loss after 12 is 0.6128143072128296\n",
            "{'Validation Epoch Loss': 0.6094397306442261, 'Epoch': 12, 'Micro F1': 0.6725688260882259, 'Patience Count': 0, 'Best Val F1': 0.6742865484271221, 'Best Val Loss': 0.6094397306442261, 'Micro ROC value': 0.7142204654622949, 'Macro ROC value': 0.7142204654622949}\n",
            "Training Epoch Loss after 13 is 0.6095028519630432\n",
            "{'Validation Epoch Loss': 0.6066072583198547, 'Epoch': 13, 'Micro F1': 0.6725542288266722, 'Patience Count': 0, 'Best Val F1': 0.6742865484271221, 'Best Val Loss': 0.6066072583198547, 'Micro ROC value': 0.7244901509966936, 'Macro ROC value': 0.7244901509966936}\n",
            "Training Epoch Loss after 14 is 0.6090679168701172\n",
            "{'Validation Epoch Loss': 0.6028016209602356, 'Epoch': 14, 'Micro F1': 0.6745445521429405, 'Patience Count': 0, 'Best Val F1': 0.6745445521429405, 'Best Val Loss': 0.6028016209602356, 'Micro ROC value': 0.732795971019091, 'Macro ROC value': 0.732795971019091}\n",
            "Training Epoch Loss after 15 is 0.6059058904647827\n",
            "{'Validation Epoch Loss': 0.6036940217018127, 'Epoch': 15, 'Micro F1': 0.6768750182465769, 'Patience Count': 0, 'Best Val F1': 0.6768750182465769, 'Best Val Loss': 0.6028016209602356, 'Micro ROC value': 0.7366070397986961, 'Macro ROC value': 0.7366070397986961}\n",
            "Training Epoch Loss after 16 is 0.6045610308647156\n",
            "{'Validation Epoch Loss': 0.6014295220375061, 'Epoch': 16, 'Micro F1': 0.678855557988468, 'Patience Count': 0, 'Best Val F1': 0.678855557988468, 'Best Val Loss': 0.6014295220375061, 'Micro ROC value': 0.7413856966283878, 'Macro ROC value': 0.7413856966283878}\n",
            "Training Epoch Loss after 17 is 0.6022630929946899\n",
            "{'Validation Epoch Loss': 0.5997447371482849, 'Epoch': 17, 'Micro F1': 0.6731675984993358, 'Patience Count': 0, 'Best Val F1': 0.678855557988468, 'Best Val Loss': 0.5997447371482849, 'Micro ROC value': 0.7476914390364018, 'Macro ROC value': 0.7476914390364018}\n",
            "Training Epoch Loss after 18 is 0.5998590588569641\n",
            "{'Validation Epoch Loss': 0.5979869365692139, 'Epoch': 18, 'Micro F1': 0.6732257028581438, 'Patience Count': 0, 'Best Val F1': 0.678855557988468, 'Best Val Loss': 0.5979869365692139, 'Micro ROC value': 0.7512359514099731, 'Macro ROC value': 0.7512359514099731}\n",
            "Training Epoch Loss after 19 is 0.5975757241249084\n",
            "{'Validation Epoch Loss': 0.5932223200798035, 'Epoch': 19, 'Micro F1': 0.6836435296693671, 'Patience Count': 0, 'Best Val F1': 0.6836435296693671, 'Best Val Loss': 0.5932223200798035, 'Micro ROC value': 0.7594235760294643, 'Macro ROC value': 0.7594235760294643}\n",
            "Training Epoch Loss after 20 is 0.5953320860862732\n",
            "{'Validation Epoch Loss': 0.5925946235656738, 'Epoch': 20, 'Micro F1': 0.6949229242088053, 'Patience Count': 0, 'Best Val F1': 0.6949229242088053, 'Best Val Loss': 0.5925946235656738, 'Micro ROC value': 0.7600093843544699, 'Macro ROC value': 0.7600093843544699}\n",
            "Training Epoch Loss after 21 is 0.5913582444190979\n",
            "{'Validation Epoch Loss': 0.586862325668335, 'Epoch': 21, 'Micro F1': 0.6887526457922779, 'Patience Count': 0, 'Best Val F1': 0.6949229242088053, 'Best Val Loss': 0.586862325668335, 'Micro ROC value': 0.766098987864444, 'Macro ROC value': 0.766098987864444}\n",
            "Training Epoch Loss after 22 is 0.5866780281066895\n",
            "{'Validation Epoch Loss': 0.5849385261535645, 'Epoch': 22, 'Micro F1': 0.6935114225239034, 'Patience Count': 0, 'Best Val F1': 0.6949229242088053, 'Best Val Loss': 0.5849385261535645, 'Micro ROC value': 0.7673561005177897, 'Macro ROC value': 0.7673561005177897}\n",
            "Training Epoch Loss after 23 is 0.5800495147705078\n",
            "{'Validation Epoch Loss': 0.5791024565696716, 'Epoch': 23, 'Micro F1': 0.7136309228388754, 'Patience Count': 0, 'Best Val F1': 0.7136309228388754, 'Best Val Loss': 0.5791024565696716, 'Micro ROC value': 0.7703840056718021, 'Macro ROC value': 0.7703840056718021}\n",
            "Training Epoch Loss after 24 is 0.5726311206817627\n",
            "{'Validation Epoch Loss': 0.5643886923789978, 'Epoch': 24, 'Micro F1': 0.7291186173473856, 'Patience Count': 0, 'Best Val F1': 0.7291186173473856, 'Best Val Loss': 0.5643886923789978, 'Micro ROC value': 0.784034786121915, 'Macro ROC value': 0.784034786121915}\n",
            "Training Epoch Loss after 25 is 0.5595999360084534\n",
            "{'Validation Epoch Loss': 0.5518302321434021, 'Epoch': 25, 'Micro F1': 0.7354976206463666, 'Patience Count': 0, 'Best Val F1': 0.7354976206463666, 'Best Val Loss': 0.5518302321434021, 'Micro ROC value': 0.7944003734444275, 'Macro ROC value': 0.7944003734444275}\n",
            "Training Epoch Loss after 26 is 0.55051589012146\n",
            "{'Validation Epoch Loss': 0.5439183712005615, 'Epoch': 26, 'Micro F1': 0.7369135550170788, 'Patience Count': 0, 'Best Val F1': 0.7369135550170788, 'Best Val Loss': 0.5439183712005615, 'Micro ROC value': 0.8006335860982295, 'Macro ROC value': 0.8006335860982295}\n",
            "Training Epoch Loss after 27 is 0.5377014875411987\n",
            "{'Validation Epoch Loss': 0.5341796278953552, 'Epoch': 27, 'Micro F1': 0.7427670569001255, 'Patience Count': 0, 'Best Val F1': 0.7427670569001255, 'Best Val Loss': 0.5341796278953552, 'Micro ROC value': 0.8092866691296619, 'Macro ROC value': 0.8092866691296619}\n",
            "Training Epoch Loss after 28 is 0.5293378829956055\n",
            "{'Validation Epoch Loss': 0.5250410437583923, 'Epoch': 28, 'Micro F1': 0.7477336758974059, 'Patience Count': 0, 'Best Val F1': 0.7477336758974059, 'Best Val Loss': 0.5250410437583923, 'Micro ROC value': 0.8175279578047507, 'Macro ROC value': 0.8175279578047507}\n",
            "Training Epoch Loss after 29 is 0.5223232507705688\n",
            "{'Validation Epoch Loss': 0.5189182758331299, 'Epoch': 29, 'Micro F1': 0.7520618932924604, 'Patience Count': 0, 'Best Val F1': 0.7520618932924604, 'Best Val Loss': 0.5189182758331299, 'Micro ROC value': 0.8231796925045174, 'Macro ROC value': 0.8231796925045174}\n",
            "Training Epoch Loss after 30 is 0.5174499750137329\n",
            "{'Validation Epoch Loss': 0.5088854432106018, 'Epoch': 30, 'Micro F1': 0.7611964264860446, 'Patience Count': 0, 'Best Val F1': 0.7611964264860446, 'Best Val Loss': 0.5088854432106018, 'Micro ROC value': 0.8314647133804659, 'Macro ROC value': 0.8314647133804659}\n",
            "Training Epoch Loss after 31 is 0.5084682106971741\n",
            "{'Validation Epoch Loss': 0.5047523379325867, 'Epoch': 31, 'Micro F1': 0.7658419093496825, 'Patience Count': 0, 'Best Val F1': 0.7658419093496825, 'Best Val Loss': 0.5047523379325867, 'Micro ROC value': 0.8360898166997507, 'Macro ROC value': 0.8360898166997507}\n",
            "Training Epoch Loss after 32 is 0.5036038756370544\n",
            "{'Validation Epoch Loss': 0.505139172077179, 'Epoch': 32, 'Micro F1': 0.7684514772859979, 'Patience Count': 0, 'Best Val F1': 0.7684514772859979, 'Best Val Loss': 0.5047523379325867, 'Micro ROC value': 0.8377110239877079, 'Macro ROC value': 0.8377110239877079}\n",
            "Training Epoch Loss after 33 is 0.5031630396842957\n",
            "{'Validation Epoch Loss': 0.5058227777481079, 'Epoch': 33, 'Micro F1': 0.7691881003123814, 'Patience Count': 0, 'Best Val F1': 0.7691881003123814, 'Best Val Loss': 0.5047523379325867, 'Micro ROC value': 0.8375378404270459, 'Macro ROC value': 0.8375378404270459}\n",
            "Training Epoch Loss after 34 is 0.49951690435409546\n",
            "{'Validation Epoch Loss': 0.49619245529174805, 'Epoch': 34, 'Micro F1': 0.7731227373584024, 'Patience Count': 0, 'Best Val F1': 0.7731227373584024, 'Best Val Loss': 0.49619245529174805, 'Micro ROC value': 0.8438647855444924, 'Macro ROC value': 0.8438647855444924}\n",
            "Training Epoch Loss after 35 is 0.4959813356399536\n",
            "{'Validation Epoch Loss': 0.49354517459869385, 'Epoch': 35, 'Micro F1': 0.7722827197617727, 'Patience Count': 0, 'Best Val F1': 0.7731227373584024, 'Best Val Loss': 0.49354517459869385, 'Micro ROC value': 0.8447271760564982, 'Macro ROC value': 0.8447271760564982}\n",
            "Training Epoch Loss after 36 is 0.4928300678730011\n",
            "{'Validation Epoch Loss': 0.49238115549087524, 'Epoch': 36, 'Micro F1': 0.7739760021020057, 'Patience Count': 0, 'Best Val F1': 0.7739760021020057, 'Best Val Loss': 0.49238115549087524, 'Micro ROC value': 0.8454239610141794, 'Macro ROC value': 0.8454239610141794}\n",
            "Training Epoch Loss after 37 is 0.4921186566352844\n",
            "{'Validation Epoch Loss': 0.48970404267311096, 'Epoch': 37, 'Micro F1': 0.7751810113278057, 'Patience Count': 0, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48970404267311096, 'Micro ROC value': 0.8473767837080306, 'Macro ROC value': 0.8473767837080306}\n",
            "Training Epoch Loss after 38 is 0.4920264184474945\n",
            "{'Validation Epoch Loss': 0.49107077717781067, 'Epoch': 38, 'Micro F1': 0.7712542333294407, 'Patience Count': 1, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48970404267311096, 'Micro ROC value': 0.8465338021882882, 'Macro ROC value': 0.8465338021882882}\n",
            "Training Epoch Loss after 39 is 0.4902668297290802\n",
            "{'Validation Epoch Loss': 0.48668038845062256, 'Epoch': 39, 'Micro F1': 0.7738380240569894, 'Patience Count': 0, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48668038845062256, 'Micro ROC value': 0.8499467967299336, 'Macro ROC value': 0.8499467967299336}\n",
            "Training Epoch Loss after 40 is 0.4881852865219116\n",
            "{'Validation Epoch Loss': 0.4898567795753479, 'Epoch': 40, 'Micro F1': 0.7739905993635594, 'Patience Count': 1, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48668038845062256, 'Micro ROC value': 0.8482882226101846, 'Macro ROC value': 0.8482882226101846}\n",
            "Training Epoch Loss after 41 is 0.489300400018692\n",
            "{'Validation Epoch Loss': 0.49064165353775024, 'Epoch': 41, 'Micro F1': 0.7726197775247439, 'Patience Count': 2, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48668038845062256, 'Micro ROC value': 0.847549031644929, 'Macro ROC value': 0.847549031644929}\n",
            "Training Epoch Loss after 42 is 0.48841482400894165\n",
            "{'Validation Epoch Loss': 0.48754826188087463, 'Epoch': 42, 'Micro F1': 0.7712475548159179, 'Patience Count': 3, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48668038845062256, 'Micro ROC value': 0.8498266208543461, 'Macro ROC value': 0.8498266208543461}\n",
            "Training Epoch Loss after 43 is 0.48766618967056274\n",
            "{'Validation Epoch Loss': 0.48608529567718506, 'Epoch': 43, 'Micro F1': 0.7743230421137142, 'Patience Count': 0, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48608529567718506, 'Micro ROC value': 0.850677687115576, 'Macro ROC value': 0.850677687115576}\n",
            "Training Epoch Loss after 44 is 0.4863468110561371\n",
            "{'Validation Epoch Loss': 0.489085853099823, 'Epoch': 44, 'Micro F1': 0.7743974540517656, 'Patience Count': 1, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48608529567718506, 'Micro ROC value': 0.8484203076945886, 'Macro ROC value': 0.8484203076945886}\n",
            "Training Epoch Loss after 45 is 0.48616844415664673\n",
            "{'Validation Epoch Loss': 0.4850570857524872, 'Epoch': 45, 'Micro F1': 0.7748226432721221, 'Patience Count': 0, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.4850570857524872, 'Micro ROC value': 0.850886031871683, 'Macro ROC value': 0.850886031871683}\n",
            "Training Epoch Loss after 46 is 0.4855239987373352\n",
            "{'Validation Epoch Loss': 0.48444685339927673, 'Epoch': 46, 'Micro F1': 0.7723994978542026, 'Patience Count': 0, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48444685339927673, 'Micro ROC value': 0.8514625732030847, 'Macro ROC value': 0.8514625732030847}\n",
            "Training Epoch Loss after 47 is 0.48486024141311646\n",
            "{'Validation Epoch Loss': 0.48843705654144287, 'Epoch': 47, 'Micro F1': 0.7722143555756683, 'Patience Count': 1, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48444685339927673, 'Micro ROC value': 0.8480128366400215, 'Macro ROC value': 0.8480128366400215}\n",
            "Training Epoch Loss after 48 is 0.48445260524749756\n",
            "{'Validation Epoch Loss': 0.48496413230895996, 'Epoch': 48, 'Micro F1': 0.7746766706565849, 'Patience Count': 2, 'Best Val F1': 0.7751810113278057, 'Best Val Loss': 0.48444685339927673, 'Micro ROC value': 0.850284035145801, 'Macro ROC value': 0.850284035145801}\n",
            "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 49 is 0.48444005846977234\n",
            "{'Validation Epoch Loss': 0.4822545051574707, 'Epoch': 49, 'Micro F1': 0.7763488263459067, 'Patience Count': 0, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8520207918124221, 'Macro ROC value': 0.8520207918124221}\n",
            "Training Epoch Loss after 50 is 0.48362287878990173\n",
            "{'Validation Epoch Loss': 0.48329249024391174, 'Epoch': 50, 'Micro F1': 0.7760163491715933, 'Patience Count': 1, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8512892945606656, 'Macro ROC value': 0.8512892945606656}\n",
            "Training Epoch Loss after 51 is 0.48395735025405884\n",
            "{'Validation Epoch Loss': 0.4834934175014496, 'Epoch': 51, 'Micro F1': 0.7747788514874611, 'Patience Count': 2, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8511954343246736, 'Macro ROC value': 0.8511954343246736}\n",
            "Training Epoch Loss after 52 is 0.4827962815761566\n",
            "{'Validation Epoch Loss': 0.48313310742378235, 'Epoch': 52, 'Micro F1': 0.774121974717543, 'Patience Count': 3, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8515494024690483, 'Macro ROC value': 0.8515494024690483}\n",
            "Training Epoch Loss after 53 is 0.48392680287361145\n",
            "{'Validation Epoch Loss': 0.4828258752822876, 'Epoch': 53, 'Micro F1': 0.7763407584737103, 'Patience Count': 4, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8517164964309532, 'Macro ROC value': 0.8517164964309532}\n",
            "Training Epoch Loss after 54 is 0.482849657535553\n",
            "{'Validation Epoch Loss': 0.48343291878700256, 'Epoch': 54, 'Micro F1': 0.775975826934867, 'Patience Count': 5, 'Best Val F1': 0.7763488263459067, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8512494629008502, 'Macro ROC value': 0.8512494629008502}\n",
            "Training Epoch Loss after 55 is 0.48326215147972107\n",
            "{'Validation Epoch Loss': 0.4829363226890564, 'Epoch': 55, 'Micro F1': 0.776472133827694, 'Patience Count': 0, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8515445327377029, 'Macro ROC value': 0.8515445327377029}\n",
            "Training Epoch Loss after 56 is 0.4833882749080658\n",
            "{'Validation Epoch Loss': 0.4839516282081604, 'Epoch': 56, 'Micro F1': 0.7745744898257088, 'Patience Count': 1, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8508790168504459, 'Macro ROC value': 0.8508790168504459}\n",
            "Training Epoch Loss after 57 is 0.4828319847583771\n",
            "{'Validation Epoch Loss': 0.4839523434638977, 'Epoch': 57, 'Micro F1': 0.7754616451353916, 'Patience Count': 2, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8509490955185235, 'Macro ROC value': 0.8509490955185235}\n",
            "Training Epoch Loss after 58 is 0.4829964339733124\n",
            "{'Validation Epoch Loss': 0.4852588176727295, 'Epoch': 58, 'Micro F1': 0.7749069410991899, 'Patience Count': 3, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4822545051574707, 'Micro ROC value': 0.8498847948251499, 'Macro ROC value': 0.8498847948251499}\n",
            "Training Epoch Loss after 59 is 0.4827337861061096\n",
            "{'Validation Epoch Loss': 0.4820176064968109, 'Epoch': 59, 'Micro F1': 0.7759579592730459, 'Patience Count': 0, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4820176064968109, 'Micro ROC value': 0.8524368506151301, 'Macro ROC value': 0.8524368506151301}\n",
            "Training Epoch Loss after 60 is 0.48286357522010803\n",
            "{'Validation Epoch Loss': 0.48268061876296997, 'Epoch': 60, 'Micro F1': 0.7758817003386663, 'Patience Count': 1, 'Best Val F1': 0.776472133827694, 'Best Val Loss': 0.4820176064968109, 'Micro ROC value': 0.8517883479703452, 'Macro ROC value': 0.8517883479703452}\n",
            "Training Epoch Loss after 61 is 0.4821851849555969\n",
            "{'Validation Epoch Loss': 0.48153117299079895, 'Epoch': 61, 'Micro F1': 0.7772165941669342, 'Patience Count': 0, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48153117299079895, 'Micro ROC value': 0.8526278794157423, 'Macro ROC value': 0.8526278794157423}\n",
            "Training Epoch Loss after 62 is 0.4827786982059479\n",
            "{'Validation Epoch Loss': 0.4842665493488312, 'Epoch': 62, 'Micro F1': 0.7751485336408626, 'Patience Count': 1, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48153117299079895, 'Micro ROC value': 0.850624385609785, 'Macro ROC value': 0.850624385609785}\n",
            "Training Epoch Loss after 63 is 0.48143458366394043\n",
            "{'Validation Epoch Loss': 0.482389897108078, 'Epoch': 63, 'Micro F1': 0.7763261612121566, 'Patience Count': 2, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48153117299079895, 'Micro ROC value': 0.8520174357707216, 'Macro ROC value': 0.8520174357707216}\n",
            "Training Epoch Loss after 64 is 0.48296892642974854\n",
            "{'Validation Epoch Loss': 0.4840131103992462, 'Epoch': 64, 'Micro F1': 0.7764266215091752, 'Patience Count': 3, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48153117299079895, 'Micro ROC value': 0.8508056382432513, 'Macro ROC value': 0.8508056382432513}\n",
            "Training Epoch Loss after 65 is 0.48201999068260193\n",
            "{'Validation Epoch Loss': 0.48110538721084595, 'Epoch': 65, 'Micro F1': 0.7765272607838843, 'Patience Count': 0, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8529443053290396, 'Macro ROC value': 0.8529443053290396}\n",
            "Training Epoch Loss after 66 is 0.483508825302124\n",
            "{'Validation Epoch Loss': 0.4821716845035553, 'Epoch': 66, 'Micro F1': 0.7747609663528209, 'Patience Count': 1, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8521410080362536, 'Macro ROC value': 0.8521410080362536}\n",
            "Training Epoch Loss after 67 is 0.4826244115829468\n",
            "{'Validation Epoch Loss': 0.4819038510322571, 'Epoch': 67, 'Micro F1': 0.7764575365661401, 'Patience Count': 2, 'Best Val F1': 0.7772165941669342, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8522142288292802, 'Macro ROC value': 0.8522142288292802}\n",
            "Training Epoch Loss after 68 is 0.4827640652656555\n",
            "{'Validation Epoch Loss': 0.48139819502830505, 'Epoch': 68, 'Micro F1': 0.777450150351794, 'Patience Count': 0, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8526577195713856, 'Macro ROC value': 0.8526577195713856}\n",
            "Training Epoch Loss after 69 is 0.4823080599308014\n",
            "{'Validation Epoch Loss': 0.48245716094970703, 'Epoch': 69, 'Micro F1': 0.7755459535209621, 'Patience Count': 1, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.852029918563646, 'Macro ROC value': 0.852029918563646}\n",
            "Training Epoch Loss after 70 is 0.4820096492767334\n",
            "{'Validation Epoch Loss': 0.4848226308822632, 'Epoch': 70, 'Micro F1': 0.7738314209804094, 'Patience Count': 2, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8502175946681597, 'Macro ROC value': 0.8502175946681597}\n",
            "Training Epoch Loss after 71 is 0.4814361035823822\n",
            "{'Validation Epoch Loss': 0.482521653175354, 'Epoch': 71, 'Micro F1': 0.7757130762268998, 'Patience Count': 3, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8518089595333029, 'Macro ROC value': 0.8518089595333029}\n",
            "Training Epoch Loss after 72 is 0.4810524582862854\n",
            "{'Validation Epoch Loss': 0.4814605116844177, 'Epoch': 72, 'Micro F1': 0.7773009269396395, 'Patience Count': 4, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.48110538721084595, 'Micro ROC value': 0.8525857459218895, 'Macro ROC value': 0.8525857459218895}\n",
            "Training Epoch Loss after 73 is 0.4830130338668823\n",
            "{'Validation Epoch Loss': 0.4795415997505188, 'Epoch': 73, 'Micro F1': 0.7787201121069688, 'Patience Count': 0, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8539155455656849, 'Macro ROC value': 0.8539155455656849}\n",
            "Training Epoch Loss after 74 is 0.4823983907699585\n",
            "{'Validation Epoch Loss': 0.48112091422080994, 'Epoch': 74, 'Micro F1': 0.777888068198406, 'Patience Count': 1, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8527923188264481, 'Macro ROC value': 0.8527923188264481}\n",
            "Training Epoch Loss after 75 is 0.4814300537109375\n",
            "{'Validation Epoch Loss': 0.4837440252304077, 'Epoch': 75, 'Micro F1': 0.774063585671328, 'Patience Count': 2, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8510245927109396, 'Macro ROC value': 0.8510245927109396}\n",
            "Training Epoch Loss after 76 is 0.48266980051994324\n",
            "{'Validation Epoch Loss': 0.48043277859687805, 'Epoch': 76, 'Micro F1': 0.7781411033093442, 'Patience Count': 3, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8533286431728182, 'Macro ROC value': 0.8533286431728182}\n",
            "Training Epoch Loss after 77 is 0.48232898116111755\n",
            "{'Validation Epoch Loss': 0.4827014207839966, 'Epoch': 77, 'Micro F1': 0.7767754178527115, 'Patience Count': 4, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8516415802819775, 'Macro ROC value': 0.8516415802819775}\n",
            "Training Epoch Loss after 78 is 0.48125484585762024\n",
            "{'Validation Epoch Loss': 0.4836411774158478, 'Epoch': 78, 'Micro F1': 0.7758638307811335, 'Patience Count': 5, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8510878574011631, 'Macro ROC value': 0.8510878574011631}\n",
            "Training Epoch Loss after 79 is 0.4825092852115631\n",
            "{'Validation Epoch Loss': 0.4810754060745239, 'Epoch': 79, 'Micro F1': 0.7766440405809794, 'Patience Count': 6, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8527798444967347, 'Macro ROC value': 0.8527798444967347}\n",
            "Training Epoch Loss after 80 is 0.4817967116832733\n",
            "{'Validation Epoch Loss': 0.4813041090965271, 'Epoch': 80, 'Micro F1': 0.7772165941669342, 'Patience Count': 7, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8526548532186221, 'Macro ROC value': 0.8526548532186221}\n",
            "Training Epoch Loss after 81 is 0.4820755124092102\n",
            "{'Validation Epoch Loss': 0.4844758212566376, 'Epoch': 81, 'Micro F1': 0.7751160550056932, 'Patience Count': 8, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8504421147777412, 'Macro ROC value': 0.8504421147777412}\n",
            "Training Epoch Loss after 82 is 0.4804328978061676\n",
            "{'Validation Epoch Loss': 0.4801141023635864, 'Epoch': 82, 'Micro F1': 0.7776691092751, 'Patience Count': 9, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8534106076525607, 'Macro ROC value': 0.8534106076525607}\n",
            "Training Epoch Loss after 83 is 0.4808655381202698\n",
            "{'Validation Epoch Loss': 0.4800594449043274, 'Epoch': 83, 'Micro F1': 0.77734471936355, 'Patience Count': 10, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8533895567525762, 'Macro ROC value': 0.8533895567525762}\n",
            "Training Epoch Loss after 84 is 0.48171836137771606\n",
            "{'Validation Epoch Loss': 0.4826081693172455, 'Epoch': 84, 'Micro F1': 0.7748080460105684, 'Patience Count': 11, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8517349858126064, 'Macro ROC value': 0.8517349858126064}\n",
            "Epoch    85: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 85 is 0.4810323715209961\n",
            "{'Validation Epoch Loss': 0.4821259677410126, 'Epoch': 85, 'Micro F1': 0.7753091105499029, 'Patience Count': 12, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8521358934728078, 'Macro ROC value': 0.8521358934728078}\n",
            "Training Epoch Loss after 86 is 0.48217064142227173\n",
            "{'Validation Epoch Loss': 0.4841746389865875, 'Epoch': 86, 'Micro F1': 0.7738300294864683, 'Patience Count': 13, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8504649652473393, 'Macro ROC value': 0.8504649652473393}\n",
            "Training Epoch Loss after 87 is 0.4811546206474304\n",
            "{'Validation Epoch Loss': 0.4800432622432709, 'Epoch': 87, 'Micro F1': 0.7785887367529851, 'Patience Count': 14, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8533971465338062, 'Macro ROC value': 0.8533971465338062}\n",
            "Training Epoch Loss after 88 is 0.4811229109764099\n",
            "{'Validation Epoch Loss': 0.4809170365333557, 'Epoch': 88, 'Micro F1': 0.7765889119201239, 'Patience Count': 15, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8527792655428995, 'Macro ROC value': 0.8527792655428995}\n",
            "Training Epoch Loss after 89 is 0.48084911704063416\n",
            "{'Validation Epoch Loss': 0.4839390218257904, 'Epoch': 89, 'Micro F1': 0.7745452953026012, 'Patience Count': 16, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8507634541665918, 'Macro ROC value': 0.8507634541665918}\n",
            "Training Epoch Loss after 90 is 0.47985756397247314\n",
            "{'Validation Epoch Loss': 0.48297303915023804, 'Epoch': 90, 'Micro F1': 0.7758233095877612, 'Patience Count': 17, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8513359986481346, 'Macro ROC value': 0.8513359986481346}\n",
            "Training Epoch Loss after 91 is 0.48146530985832214\n",
            "{'Validation Epoch Loss': 0.4800785481929779, 'Epoch': 91, 'Micro F1': 0.7769002817394858, 'Patience Count': 18, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8534222868432217, 'Macro ROC value': 0.8534222868432217}\n",
            "Training Epoch Loss after 92 is 0.4812491834163666\n",
            "{'Validation Epoch Loss': 0.4822688698768616, 'Epoch': 92, 'Micro F1': 0.7764396759360631, 'Patience Count': 19, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8518804299558527, 'Macro ROC value': 0.8518804299558527}\n",
            "Training Epoch Loss after 93 is 0.480789452791214\n",
            "{'Validation Epoch Loss': 0.48024889826774597, 'Epoch': 93, 'Micro F1': 0.7769100516743058, 'Patience Count': 20, 'Best Val F1': 0.7787201121069688, 'Best Val Loss': 0.4795415997505188, 'Micro ROC value': 0.8532829833596869, 'Macro ROC value': 0.8532829833596869}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.772229358869555, 'Micro Recall': 0.772229358869555, 'Micro Precision': 0.772229358869555, 'Micro ROC_AUC_Score': 0.8475593517660045, 'Macro ROC_AUC_Score': 0.8475593517660045}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(2, 137009) (2, 34253) (2, 42816)\n",
            "Training Epoch Loss after 0 is 0.8639843463897705\n",
            "{'Validation Epoch Loss': 0.7369498014450073, 'Epoch': 0, 'Micro F1': 0.4798411817942954, 'Patience Count': 0, 'Best Val F1': 0.4798411817942954, 'Best Val Loss': 0.7369498014450073, 'Micro ROC value': 0.43852709232098397, 'Macro ROC value': 0.43852709232098397}\n",
            "Training Epoch Loss after 1 is 0.6964418292045593\n",
            "{'Validation Epoch Loss': 0.7057456970214844, 'Epoch': 1, 'Micro F1': 0.5007590576007941, 'Patience Count': 0, 'Best Val F1': 0.5007590576007941, 'Best Val Loss': 0.7057456970214844, 'Micro ROC value': 0.6011108588063445, 'Macro ROC value': 0.6011108588063445}\n",
            "Training Epoch Loss after 2 is 0.7228977084159851\n",
            "{'Validation Epoch Loss': 0.7270811796188354, 'Epoch': 2, 'Micro F1': 0.5000437917846612, 'Patience Count': 1, 'Best Val F1': 0.5007590576007941, 'Best Val Loss': 0.7057456970214844, 'Micro ROC value': 0.6131260837096599, 'Macro ROC value': 0.6131260837096599}\n",
            "Training Epoch Loss after 3 is 0.7203643918037415\n",
            "{'Validation Epoch Loss': 0.708191990852356, 'Epoch': 3, 'Micro F1': 0.5022042508466659, 'Patience Count': 0, 'Best Val F1': 0.5022042508466659, 'Best Val Loss': 0.7057456970214844, 'Micro ROC value': 0.6108223747687694, 'Macro ROC value': 0.6108223747687694}\n",
            "Training Epoch Loss after 4 is 0.6963826417922974\n",
            "{'Validation Epoch Loss': 0.6879322528839111, 'Epoch': 4, 'Micro F1': 0.5474265027880769, 'Patience Count': 0, 'Best Val F1': 0.5474265027880769, 'Best Val Loss': 0.6879322528839111, 'Micro ROC value': 0.6190673434615057, 'Macro ROC value': 0.6190673434615057}\n",
            "Training Epoch Loss after 5 is 0.6836715340614319\n",
            "{'Validation Epoch Loss': 0.6823014616966248, 'Epoch': 5, 'Micro F1': 0.6029836802615829, 'Patience Count': 0, 'Best Val F1': 0.6029836802615829, 'Best Val Loss': 0.6823014616966248, 'Micro ROC value': 0.6247787635706344, 'Macro ROC value': 0.6247787635706344}\n",
            "Training Epoch Loss after 6 is 0.681793212890625\n",
            "{'Validation Epoch Loss': 0.68108731508255, 'Epoch': 6, 'Micro F1': 0.5114150585350188, 'Patience Count': 0, 'Best Val F1': 0.6029836802615829, 'Best Val Loss': 0.68108731508255, 'Micro ROC value': 0.6383141027925189, 'Macro ROC value': 0.6383141027925189}\n",
            "Training Epoch Loss after 7 is 0.6789761185646057\n",
            "{'Validation Epoch Loss': 0.6754142642021179, 'Epoch': 7, 'Micro F1': 0.5390476746562345, 'Patience Count': 0, 'Best Val F1': 0.6029836802615829, 'Best Val Loss': 0.6754142642021179, 'Micro ROC value': 0.6617638510929518, 'Macro ROC value': 0.6617638510929518}\n",
            "Training Epoch Loss after 8 is 0.6694324016571045\n",
            "{'Validation Epoch Loss': 0.662809431552887, 'Epoch': 8, 'Micro F1': 0.641564242548098, 'Patience Count': 0, 'Best Val F1': 0.641564242548098, 'Best Val Loss': 0.662809431552887, 'Micro ROC value': 0.6992587679939034, 'Macro ROC value': 0.6992587679939034}\n",
            "Training Epoch Loss after 9 is 0.6552956700325012\n",
            "{'Validation Epoch Loss': 0.6482815146446228, 'Epoch': 9, 'Micro F1': 0.6787638136669538, 'Patience Count': 0, 'Best Val F1': 0.6787638136669538, 'Best Val Loss': 0.6482815146446228, 'Micro ROC value': 0.7299197345653872, 'Macro ROC value': 0.7299197345653872}\n",
            "Training Epoch Loss after 10 is 0.6406775712966919\n",
            "{'Validation Epoch Loss': 0.6343321204185486, 'Epoch': 10, 'Micro F1': 0.6681410116049923, 'Patience Count': 0, 'Best Val F1': 0.6787638136669538, 'Best Val Loss': 0.6343321204185486, 'Micro ROC value': 0.7593630441877603, 'Macro ROC value': 0.7593630441877603}\n",
            "Training Epoch Loss after 11 is 0.6277073621749878\n",
            "{'Validation Epoch Loss': 0.6221932768821716, 'Epoch': 11, 'Micro F1': 0.6553047997197244, 'Patience Count': 0, 'Best Val F1': 0.6787638136669538, 'Best Val Loss': 0.6221932768821716, 'Micro ROC value': 0.777020500493683, 'Macro ROC value': 0.777020500493683}\n",
            "Training Epoch Loss after 12 is 0.6136673092842102\n",
            "{'Validation Epoch Loss': 0.6049504280090332, 'Epoch': 12, 'Micro F1': 0.6720725191953989, 'Patience Count': 0, 'Best Val F1': 0.6787638136669538, 'Best Val Loss': 0.6049504280090332, 'Micro ROC value': 0.7888437478908538, 'Macro ROC value': 0.7888437478908538}\n",
            "Training Epoch Loss after 13 is 0.5922617316246033\n",
            "{'Validation Epoch Loss': 0.5808821320533752, 'Epoch': 13, 'Micro F1': 0.7244792198881802, 'Patience Count': 0, 'Best Val F1': 0.7244792198881802, 'Best Val Loss': 0.5808821320533752, 'Micro ROC value': 0.7948754397558231, 'Macro ROC value': 0.7948754397558231}\n",
            "Training Epoch Loss after 14 is 0.5694368481636047\n",
            "{'Validation Epoch Loss': 0.5627310872077942, 'Epoch': 14, 'Micro F1': 0.7349721192304324, 'Patience Count': 0, 'Best Val F1': 0.7349721192304324, 'Best Val Loss': 0.5627310872077942, 'Micro ROC value': 0.7929202666089228, 'Macro ROC value': 0.7929202666089228}\n",
            "Training Epoch Loss after 15 is 0.5543937087059021\n",
            "{'Validation Epoch Loss': 0.5501109957695007, 'Epoch': 15, 'Micro F1': 0.738649966424338, 'Patience Count': 0, 'Best Val F1': 0.738649966424338, 'Best Val Loss': 0.5501109957695007, 'Micro ROC value': 0.7940423101785964, 'Macro ROC value': 0.7940423101785964}\n",
            "Training Epoch Loss after 16 is 0.5407845973968506\n",
            "{'Validation Epoch Loss': 0.5373565554618835, 'Epoch': 16, 'Micro F1': 0.750295598861397, 'Patience Count': 0, 'Best Val F1': 0.750295598861397, 'Best Val Loss': 0.5373565554618835, 'Micro ROC value': 0.8015190039417857, 'Macro ROC value': 0.8015190039417857}\n",
            "Training Epoch Loss after 17 is 0.529399037361145\n",
            "{'Validation Epoch Loss': 0.5258178114891052, 'Epoch': 17, 'Micro F1': 0.7516314106775281, 'Patience Count': 0, 'Best Val F1': 0.7516314106775281, 'Best Val Loss': 0.5258178114891052, 'Micro ROC value': 0.8126846623277963, 'Macro ROC value': 0.8126846623277963}\n",
            "Training Epoch Loss after 18 is 0.5245052576065063\n",
            "{'Validation Epoch Loss': 0.5215359330177307, 'Epoch': 18, 'Micro F1': 0.7542405044813593, 'Patience Count': 0, 'Best Val F1': 0.7542405044813593, 'Best Val Loss': 0.5215359330177307, 'Micro ROC value': 0.8186016912867178, 'Macro ROC value': 0.8186016912867178}\n",
            "Training Epoch Loss after 19 is 0.5197142958641052\n",
            "{'Validation Epoch Loss': 0.5183286666870117, 'Epoch': 19, 'Micro F1': 0.7582109596239746, 'Patience Count': 0, 'Best Val F1': 0.7582109596239746, 'Best Val Loss': 0.5183286666870117, 'Micro ROC value': 0.8197739179130725, 'Macro ROC value': 0.8197739179130725}\n",
            "Training Epoch Loss after 20 is 0.5135390758514404\n",
            "{'Validation Epoch Loss': 0.5117989182472229, 'Epoch': 20, 'Micro F1': 0.7715899801471446, 'Patience Count': 0, 'Best Val F1': 0.7715899801471446, 'Best Val Loss': 0.5117989182472229, 'Micro ROC value': 0.8230923138227753, 'Macro ROC value': 0.8230923138227753}\n",
            "Training Epoch Loss after 21 is 0.5108435750007629\n",
            "{'Validation Epoch Loss': 0.5073108673095703, 'Epoch': 21, 'Micro F1': 0.7770998160745044, 'Patience Count': 0, 'Best Val F1': 0.7770998160745044, 'Best Val Loss': 0.5073108673095703, 'Micro ROC value': 0.826215856960266, 'Macro ROC value': 0.826215856960266}\n",
            "Training Epoch Loss after 22 is 0.5052078366279602\n",
            "{'Validation Epoch Loss': 0.5044111013412476, 'Epoch': 22, 'Micro F1': 0.7749248241029982, 'Patience Count': 0, 'Best Val F1': 0.7770998160745044, 'Best Val Loss': 0.5044111013412476, 'Micro ROC value': 0.8301415525086562, 'Macro ROC value': 0.8301415525086562}\n",
            "Training Epoch Loss after 23 is 0.4998174011707306\n",
            "{'Validation Epoch Loss': 0.5004681944847107, 'Epoch': 23, 'Micro F1': 0.7742971418561879, 'Patience Count': 0, 'Best Val F1': 0.7770998160745044, 'Best Val Loss': 0.5004681944847107, 'Micro ROC value': 0.8347525910424785, 'Macro ROC value': 0.8347525910424785}\n",
            "Training Epoch Loss after 24 is 0.49480530619621277\n",
            "{'Validation Epoch Loss': 0.4909556806087494, 'Epoch': 24, 'Micro F1': 0.7821212740489883, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4909556806087494, 'Micro ROC value': 0.8406966677125175, 'Macro ROC value': 0.8406966677125175}\n",
            "Training Epoch Loss after 25 is 0.4891408681869507\n",
            "{'Validation Epoch Loss': 0.49122920632362366, 'Epoch': 25, 'Micro F1': 0.7785011531836628, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4909556806087494, 'Micro ROC value': 0.840322000120264, 'Macro ROC value': 0.840322000120264}\n",
            "Training Epoch Loss after 26 is 0.48643752932548523\n",
            "{'Validation Epoch Loss': 0.48604685068130493, 'Epoch': 26, 'Micro F1': 0.7762353112911466, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48604685068130493, 'Micro ROC value': 0.8446228233992272, 'Macro ROC value': 0.8446228233992272}\n",
            "Training Epoch Loss after 27 is 0.48333555459976196\n",
            "{'Validation Epoch Loss': 0.48639002442359924, 'Epoch': 27, 'Micro F1': 0.7742062623166193, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48604685068130493, 'Micro ROC value': 0.8448987151372075, 'Macro ROC value': 0.8448987151372075}\n",
            "Training Epoch Loss after 28 is 0.48260125517845154\n",
            "{'Validation Epoch Loss': 0.48500731587409973, 'Epoch': 28, 'Micro F1': 0.7752313665956265, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48500731587409973, 'Micro ROC value': 0.8454991684683357, 'Macro ROC value': 0.8454991684683357}\n",
            "Training Epoch Loss after 29 is 0.4822809100151062\n",
            "{'Validation Epoch Loss': 0.4836770296096802, 'Epoch': 29, 'Micro F1': 0.77553940030948, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4836770296096802, 'Micro ROC value': 0.8466246370097406, 'Macro ROC value': 0.8466246370097406}\n",
            "Training Epoch Loss after 30 is 0.4782457947731018\n",
            "{'Validation Epoch Loss': 0.48081862926483154, 'Epoch': 30, 'Micro F1': 0.7766910927509999, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48081862926483154, 'Micro ROC value': 0.8482994434053472, 'Macro ROC value': 0.8482994434053472}\n",
            "Training Epoch Loss after 31 is 0.4780336022377014\n",
            "{'Validation Epoch Loss': 0.4813149571418762, 'Epoch': 31, 'Micro F1': 0.7751080228891744, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48081862926483154, 'Micro ROC value': 0.847495075609489, 'Macro ROC value': 0.847495075609489}\n",
            "Training Epoch Loss after 32 is 0.4785167872905731\n",
            "{'Validation Epoch Loss': 0.48086532950401306, 'Epoch': 32, 'Micro F1': 0.7750416021954281, 'Patience Count': 2, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.48081862926483154, 'Micro ROC value': 0.8477884940779972, 'Macro ROC value': 0.8477884940779972}\n",
            "Training Epoch Loss after 33 is 0.47729408740997314\n",
            "{'Validation Epoch Loss': 0.480591744184494, 'Epoch': 33, 'Micro F1': 0.7769538434589672, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.480591744184494, 'Micro ROC value': 0.8477020683856384, 'Macro ROC value': 0.8477020683856384}\n",
            "Training Epoch Loss after 34 is 0.47668445110321045\n",
            "{'Validation Epoch Loss': 0.48150634765625, 'Epoch': 34, 'Micro F1': 0.774532130448746, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.480591744184494, 'Micro ROC value': 0.8471865469769018, 'Macro ROC value': 0.8471865469769018}\n",
            "Training Epoch Loss after 35 is 0.47734612226486206\n",
            "{'Validation Epoch Loss': 0.4792711138725281, 'Epoch': 35, 'Micro F1': 0.7776626182412706, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4792711138725281, 'Micro ROC value': 0.8487019594313618, 'Macro ROC value': 0.8487019594313618}\n",
            "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 36 is 0.4749950170516968\n",
            "{'Validation Epoch Loss': 0.479095995426178, 'Epoch': 36, 'Micro F1': 0.7771857345148246, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.479095995426178, 'Micro ROC value': 0.8491810215952658, 'Macro ROC value': 0.8491810215952658}\n",
            "Training Epoch Loss after 37 is 0.4757886230945587\n",
            "{'Validation Epoch Loss': 0.47723859548568726, 'Epoch': 37, 'Micro F1': 0.7797565176772837, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47723859548568726, 'Micro ROC value': 0.8500874700828905, 'Macro ROC value': 0.8500874700828905}\n",
            "Training Epoch Loss after 38 is 0.4750373065471649\n",
            "{'Validation Epoch Loss': 0.4785430431365967, 'Epoch': 38, 'Micro F1': 0.7787930984147373, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47723859548568726, 'Micro ROC value': 0.8490012574782476, 'Macro ROC value': 0.8490012574782476}\n",
            "Training Epoch Loss after 39 is 0.47487860918045044\n",
            "{'Validation Epoch Loss': 0.47920456528663635, 'Epoch': 39, 'Micro F1': 0.7764120231821433, 'Patience Count': 2, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47723859548568726, 'Micro ROC value': 0.8487694097631899, 'Macro ROC value': 0.8487694097631899}\n",
            "Training Epoch Loss after 40 is 0.47554996609687805\n",
            "{'Validation Epoch Loss': 0.4760308563709259, 'Epoch': 40, 'Micro F1': 0.7786763203223074, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8507992009010791, 'Macro ROC value': 0.8507992009010791}\n",
            "Training Epoch Loss after 41 is 0.47629714012145996\n",
            "{'Validation Epoch Loss': 0.4808191955089569, 'Epoch': 41, 'Micro F1': 0.7758590488424373, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8476930738507846, 'Macro ROC value': 0.8476930738507846}\n",
            "Training Epoch Loss after 42 is 0.4750158190727234\n",
            "{'Validation Epoch Loss': 0.47689977288246155, 'Epoch': 42, 'Micro F1': 0.7780486380754971, 'Patience Count': 2, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8502655956248781, 'Macro ROC value': 0.8502655956248781}\n",
            "Training Epoch Loss after 43 is 0.4757620096206665\n",
            "{'Validation Epoch Loss': 0.479233056306839, 'Epoch': 43, 'Micro F1': 0.7761801885966192, 'Patience Count': 3, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8486906443044422, 'Macro ROC value': 0.8486906443044422}\n",
            "Training Epoch Loss after 44 is 0.476401686668396\n",
            "{'Validation Epoch Loss': 0.4793265759944916, 'Epoch': 44, 'Micro F1': 0.7761590564054653, 'Patience Count': 4, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8485969188993085, 'Macro ROC value': 0.8485969188993085}\n",
            "Training Epoch Loss after 45 is 0.47648876905441284\n",
            "{'Validation Epoch Loss': 0.47805115580558777, 'Epoch': 45, 'Micro F1': 0.7779172627215133, 'Patience Count': 5, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8495286847116276, 'Macro ROC value': 0.8495286847116276}\n",
            "Training Epoch Loss after 46 is 0.4754710793495178\n",
            "{'Validation Epoch Loss': 0.47783997654914856, 'Epoch': 46, 'Micro F1': 0.7771225037953988, 'Patience Count': 6, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8501038301241081, 'Macro ROC value': 0.8501038301241081}\n",
            "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 47 is 0.4744186997413635\n",
            "{'Validation Epoch Loss': 0.47781193256378174, 'Epoch': 47, 'Micro F1': 0.7774355530902403, 'Patience Count': 7, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8495587012975481, 'Macro ROC value': 0.8495587012975481}\n",
            "Training Epoch Loss after 48 is 0.47480446100234985\n",
            "{'Validation Epoch Loss': 0.4767104685306549, 'Epoch': 48, 'Micro F1': 0.7782238052141419, 'Patience Count': 8, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8506796655528686, 'Macro ROC value': 0.8506796655528686}\n",
            "Training Epoch Loss after 49 is 0.4746684730052948\n",
            "{'Validation Epoch Loss': 0.47788217663764954, 'Epoch': 49, 'Micro F1': 0.7779610545061746, 'Patience Count': 9, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8495581140489445, 'Macro ROC value': 0.8495581140489445}\n",
            "Training Epoch Loss after 50 is 0.4740665853023529\n",
            "{'Validation Epoch Loss': 0.4769107699394226, 'Epoch': 50, 'Micro F1': 0.7788222929378448, 'Patience Count': 10, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8506438779070128, 'Macro ROC value': 0.8506438779070128}\n",
            "Training Epoch Loss after 51 is 0.4745362102985382\n",
            "{'Validation Epoch Loss': 0.477107435464859, 'Epoch': 51, 'Micro F1': 0.7786033340145389, 'Patience Count': 11, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8501009252354037, 'Macro ROC value': 0.8501009252354037}\n",
            "Training Epoch Loss after 52 is 0.4753307104110718\n",
            "{'Validation Epoch Loss': 0.47680866718292236, 'Epoch': 52, 'Micro F1': 0.778272484416741, 'Patience Count': 12, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8505756921286238, 'Macro ROC value': 0.8505756921286238}\n",
            "Training Epoch Loss after 53 is 0.4751412272453308\n",
            "{'Validation Epoch Loss': 0.4805227518081665, 'Epoch': 53, 'Micro F1': 0.7750675133201956, 'Patience Count': 13, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8480481109682629, 'Macro ROC value': 0.8480481109682629}\n",
            "Training Epoch Loss after 54 is 0.4753348231315613\n",
            "{'Validation Epoch Loss': 0.4764670431613922, 'Epoch': 54, 'Micro F1': 0.7782351653164002, 'Patience Count': 14, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8507031960986298, 'Macro ROC value': 0.8507031960986298}\n",
            "Training Epoch Loss after 55 is 0.47395506501197815\n",
            "{'Validation Epoch Loss': 0.47702911496162415, 'Epoch': 55, 'Micro F1': 0.7780778325986045, 'Patience Count': 15, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8502748701469112, 'Macro ROC value': 0.8502748701469112}\n",
            "Training Epoch Loss after 56 is 0.47422102093696594\n",
            "{'Validation Epoch Loss': 0.47795674204826355, 'Epoch': 56, 'Micro F1': 0.7778588736752985, 'Patience Count': 16, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8494200019562623, 'Macro ROC value': 0.8494200019562623}\n",
            "Training Epoch Loss after 57 is 0.4758298099040985\n",
            "{'Validation Epoch Loss': 0.47866687178611755, 'Epoch': 57, 'Micro F1': 0.7768484052258959, 'Patience Count': 17, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8486872725983858, 'Macro ROC value': 0.8486872725983858}\n",
            "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 58 is 0.47492873668670654\n",
            "{'Validation Epoch Loss': 0.47609972953796387, 'Epoch': 58, 'Micro F1': 0.7777274983213149, 'Patience Count': 18, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4760308563709259, 'Micro ROC value': 0.8508897352028628, 'Macro ROC value': 0.8508897352028628}\n",
            "Training Epoch Loss after 59 is 0.47447896003723145\n",
            "{'Validation Epoch Loss': 0.47587883472442627, 'Epoch': 59, 'Micro F1': 0.7797500875861263, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8510688326638338, 'Macro ROC value': 0.8510688326638338}\n",
            "Training Epoch Loss after 60 is 0.4749036133289337\n",
            "{'Validation Epoch Loss': 0.4802396595478058, 'Epoch': 60, 'Micro F1': 0.7743701281639565, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8482644718560635, 'Macro ROC value': 0.8482644718560635}\n",
            "Training Epoch Loss after 61 is 0.4736965596675873\n",
            "{'Validation Epoch Loss': 0.47759556770324707, 'Epoch': 61, 'Micro F1': 0.7761363968119581, 'Patience Count': 2, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8502287238277542, 'Macro ROC value': 0.8502287238277542}\n",
            "Training Epoch Loss after 62 is 0.4744524359703064\n",
            "{'Validation Epoch Loss': 0.47914034128189087, 'Epoch': 62, 'Micro F1': 0.7760130795282026, 'Patience Count': 3, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8486225752347596, 'Macro ROC value': 0.8486225752347596}\n",
            "Training Epoch Loss after 63 is 0.47497794032096863\n",
            "{'Validation Epoch Loss': 0.4784650206565857, 'Epoch': 63, 'Micro F1': 0.7769083663012218, 'Patience Count': 4, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.849454993884119, 'Macro ROC value': 0.849454993884119}\n",
            "Training Epoch Loss after 64 is 0.4743843972682953\n",
            "{'Validation Epoch Loss': 0.47644123435020447, 'Epoch': 64, 'Micro F1': 0.7777274983213149, 'Patience Count': 5, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8506818845684558, 'Macro ROC value': 0.8506818845684558}\n",
            "Training Epoch Loss after 65 is 0.4745129942893982\n",
            "{'Validation Epoch Loss': 0.47671380639076233, 'Epoch': 65, 'Micro F1': 0.7782351653164002, 'Patience Count': 6, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8505812873150915, 'Macro ROC value': 0.8505812873150915}\n",
            "Training Epoch Loss after 66 is 0.47467365860939026\n",
            "{'Validation Epoch Loss': 0.4763508439064026, 'Epoch': 66, 'Micro F1': 0.7775523311826701, 'Patience Count': 7, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8510435589657332, 'Macro ROC value': 0.8510435589657332}\n",
            "Training Epoch Loss after 67 is 0.4747209846973419\n",
            "{'Validation Epoch Loss': 0.47840073704719543, 'Epoch': 67, 'Micro F1': 0.7763699529968178, 'Patience Count': 8, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8492833298585234, 'Macro ROC value': 0.8492833298585234}\n",
            "Training Epoch Loss after 68 is 0.4752238690853119\n",
            "{'Validation Epoch Loss': 0.47597336769104004, 'Epoch': 68, 'Micro F1': 0.7778442764137448, 'Patience Count': 9, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8511674377375783, 'Macro ROC value': 0.8511674377375783}\n",
            "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Training Epoch Loss after 69 is 0.4739966094493866\n",
            "{'Validation Epoch Loss': 0.47808000445365906, 'Epoch': 69, 'Micro F1': 0.7767786763203223, 'Patience Count': 10, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8497986260187889, 'Macro ROC value': 0.8497986260187889}\n",
            "Training Epoch Loss after 70 is 0.47487756609916687\n",
            "{'Validation Epoch Loss': 0.47656533122062683, 'Epoch': 70, 'Micro F1': 0.77876390389163, 'Patience Count': 11, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.47587883472442627, 'Micro ROC value': 0.8510346317641735, 'Macro ROC value': 0.8510346317641735}\n",
            "Training Epoch Loss after 71 is 0.4739473760128021\n",
            "{'Validation Epoch Loss': 0.4751690924167633, 'Epoch': 71, 'Micro F1': 0.7792099731402544, 'Patience Count': 0, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8516303893285915, 'Macro ROC value': 0.8516303893285915}\n",
            "Training Epoch Loss after 72 is 0.4747777283191681\n",
            "{'Validation Epoch Loss': 0.47865262627601624, 'Epoch': 72, 'Micro F1': 0.776305033282728, 'Patience Count': 1, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8490474262500602, 'Macro ROC value': 0.8490474262500602}\n",
            "Training Epoch Loss after 73 is 0.4752432405948639\n",
            "{'Validation Epoch Loss': 0.47612857818603516, 'Epoch': 73, 'Micro F1': 0.7785011531836628, 'Patience Count': 2, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8509466748786125, 'Macro ROC value': 0.8509466748786125}\n",
            "Training Epoch Loss after 74 is 0.4746841490268707\n",
            "{'Validation Epoch Loss': 0.47954097390174866, 'Epoch': 74, 'Micro F1': 0.7758459607018773, 'Patience Count': 3, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8489202570273573, 'Macro ROC value': 0.8489202570273573}\n",
            "Training Epoch Loss after 75 is 0.47358980774879456\n",
            "{'Validation Epoch Loss': 0.48030561208724976, 'Epoch': 75, 'Micro F1': 0.7752897556418414, 'Patience Count': 4, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.848058409815553, 'Macro ROC value': 0.848058409815553}\n",
            "Training Epoch Loss after 76 is 0.4755641520023346\n",
            "{'Validation Epoch Loss': 0.4765760004520416, 'Epoch': 76, 'Micro F1': 0.7791743306764766, 'Patience Count': 5, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8505547305046617, 'Macro ROC value': 0.8505547305046617}\n",
            "Training Epoch Loss after 77 is 0.4745641052722931\n",
            "{'Validation Epoch Loss': 0.47717997431755066, 'Epoch': 77, 'Micro F1': 0.7767640790587685, 'Patience Count': 6, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8503169108397637, 'Macro ROC value': 0.8503169108397637}\n",
            "Training Epoch Loss after 78 is 0.4746454060077667\n",
            "{'Validation Epoch Loss': 0.47781041264533997, 'Epoch': 78, 'Micro F1': 0.7768662598896448, 'Patience Count': 7, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8500603922969486, 'Macro ROC value': 0.8500603922969486}\n",
            "Training Epoch Loss after 79 is 0.47542041540145874\n",
            "{'Validation Epoch Loss': 0.4794301688671112, 'Epoch': 79, 'Micro F1': 0.7758882433655446, 'Patience Count': 8, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8487109533896786, 'Macro ROC value': 0.8487109533896786}\n",
            "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Training Epoch Loss after 80 is 0.47465547919273376\n",
            "{'Validation Epoch Loss': 0.47633033990859985, 'Epoch': 80, 'Micro F1': 0.7793331970922255, 'Patience Count': 9, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8505553874690195, 'Macro ROC value': 0.8505553874690195}\n",
            "Training Epoch Loss after 81 is 0.4740118980407715\n",
            "{'Validation Epoch Loss': 0.47782644629478455, 'Epoch': 81, 'Micro F1': 0.7767364456512219, 'Patience Count': 10, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8497714782486477, 'Macro ROC value': 0.8497714782486477}\n",
            "Training Epoch Loss after 82 is 0.47483354806900024\n",
            "{'Validation Epoch Loss': 0.47520530223846436, 'Epoch': 82, 'Micro F1': 0.7786001021823223, 'Patience Count': 11, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8515113504797589, 'Macro ROC value': 0.8515113504797589}\n",
            "Training Epoch Loss after 83 is 0.4759022891521454\n",
            "{'Validation Epoch Loss': 0.47877398133277893, 'Epoch': 83, 'Micro F1': 0.7761801885966192, 'Patience Count': 12, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8491022672211972, 'Macro ROC value': 0.8491022672211972}\n",
            "Training Epoch Loss after 84 is 0.4742335379123688\n",
            "{'Validation Epoch Loss': 0.4783588945865631, 'Epoch': 84, 'Micro F1': 0.7752864754397489, 'Patience Count': 13, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8491529504713637, 'Macro ROC value': 0.8491529504713637}\n",
            "Training Epoch Loss after 85 is 0.4755300283432007\n",
            "{'Validation Epoch Loss': 0.47785383462905884, 'Epoch': 85, 'Micro F1': 0.7765288097983971, 'Patience Count': 14, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8499099184269836, 'Macro ROC value': 0.8499099184269836}\n",
            "Training Epoch Loss after 86 is 0.47526678442955017\n",
            "{'Validation Epoch Loss': 0.4795916676521301, 'Epoch': 86, 'Micro F1': 0.775377339211164, 'Patience Count': 15, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8486698583460652, 'Macro ROC value': 0.8486698583460652}\n",
            "Training Epoch Loss after 87 is 0.4747636616230011\n",
            "{'Validation Epoch Loss': 0.47809386253356934, 'Epoch': 87, 'Micro F1': 0.7761039340194147, 'Patience Count': 16, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8495300287797038, 'Macro ROC value': 0.8495300287797038}\n",
            "Training Epoch Loss after 88 is 0.4744212329387665\n",
            "{'Validation Epoch Loss': 0.4788259267807007, 'Epoch': 88, 'Micro F1': 0.7762758379072754, 'Patience Count': 17, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8491920052407318, 'Macro ROC value': 0.8491920052407318}\n",
            "Training Epoch Loss after 89 is 0.47461020946502686\n",
            "{'Validation Epoch Loss': 0.4791722297668457, 'Epoch': 89, 'Micro F1': 0.7768662598896448, 'Patience Count': 18, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8484619940745355, 'Macro ROC value': 0.8484619940745355}\n",
            "Training Epoch Loss after 90 is 0.47406044602394104\n",
            "{'Validation Epoch Loss': 0.47912442684173584, 'Epoch': 90, 'Micro F1': 0.7756254926575774, 'Patience Count': 19, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8492232498090724, 'Macro ROC value': 0.8492232498090724}\n",
            "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Training Epoch Loss after 91 is 0.47524428367614746\n",
            "{'Validation Epoch Loss': 0.47825032472610474, 'Epoch': 91, 'Micro F1': 0.7774290552376504, 'Patience Count': 20, 'Best Val F1': 0.7821212740489883, 'Best Val Loss': 0.4751690924167633, 'Micro ROC value': 0.8495605073572895, 'Macro ROC value': 0.8495605073572895}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.7774897234678625, 'Micro Recall': 0.7774897234678625, 'Micro Precision': 0.7774897234678625, 'Micro ROC_AUC_Score': 0.852136008262875, 'Macro ROC_AUC_Score': 0.852136008262875}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(2, 137010) (2, 34253) (2, 42815)\n",
            "Training Epoch Loss after 0 is 0.6791119575500488\n",
            "{'Validation Epoch Loss': 0.6744776368141174, 'Epoch': 0, 'Micro F1': 0.6196864416676399, 'Patience Count': 0, 'Best Val F1': 0.6196864416676399, 'Best Val Loss': 0.6744776368141174, 'Micro ROC value': 0.6185705867964384, 'Macro ROC value': 0.6185705867964384}\n",
            "Training Epoch Loss after 1 is 0.6714112162590027\n",
            "{'Validation Epoch Loss': 0.663499116897583, 'Epoch': 1, 'Micro F1': 0.6382360669138469, 'Patience Count': 0, 'Best Val F1': 0.6382360669138469, 'Best Val Loss': 0.663499116897583, 'Micro ROC value': 0.6275930527821969, 'Macro ROC value': 0.6275930527821969}\n",
            "Training Epoch Loss after 2 is 0.6619578003883362\n",
            "{'Validation Epoch Loss': 0.6609166860580444, 'Epoch': 2, 'Micro F1': 0.6534941095750427, 'Patience Count': 0, 'Best Val F1': 0.6534941095750427, 'Best Val Loss': 0.6609166860580444, 'Micro ROC value': 0.6321824076258795, 'Macro ROC value': 0.6321824076258795}\n",
            "Training Epoch Loss after 3 is 0.6579251885414124\n",
            "{'Validation Epoch Loss': 0.6535720825195312, 'Epoch': 3, 'Micro F1': 0.6486190587410954, 'Patience Count': 0, 'Best Val F1': 0.6534941095750427, 'Best Val Loss': 0.6535720825195312, 'Micro ROC value': 0.6375449333956972, 'Macro ROC value': 0.6375449333956972}\n",
            "Training Epoch Loss after 4 is 0.6516178250312805\n",
            "{'Validation Epoch Loss': 0.6506355404853821, 'Epoch': 4, 'Micro F1': 0.6259159781624967, 'Patience Count': 0, 'Best Val F1': 0.6534941095750427, 'Best Val Loss': 0.6506355404853821, 'Micro ROC value': 0.6354601167685976, 'Macro ROC value': 0.6354601167685976}\n",
            "Training Epoch Loss after 5 is 0.6485437750816345\n",
            "{'Validation Epoch Loss': 0.6458774209022522, 'Epoch': 5, 'Micro F1': 0.6366303681429364, 'Patience Count': 0, 'Best Val F1': 0.6534941095750427, 'Best Val Loss': 0.6458774209022522, 'Micro ROC value': 0.6380984581162308, 'Macro ROC value': 0.6380984581162308}\n",
            "Training Epoch Loss after 6 is 0.6440098285675049\n",
            "{'Validation Epoch Loss': 0.6431870460510254, 'Epoch': 6, 'Micro F1': 0.6594896797360815, 'Patience Count': 0, 'Best Val F1': 0.6594896797360815, 'Best Val Loss': 0.6431870460510254, 'Micro ROC value': 0.6414448333432741, 'Macro ROC value': 0.6414448333432741}\n",
            "Training Epoch Loss after 7 is 0.6412352323532104\n",
            "{'Validation Epoch Loss': 0.638566255569458, 'Epoch': 7, 'Micro F1': 0.6595772633054039, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.638566255569458, 'Micro ROC value': 0.6447631241942438, 'Macro ROC value': 0.6447631241942438}\n",
            "Training Epoch Loss after 8 is 0.6378318071365356\n",
            "{'Validation Epoch Loss': 0.6351932883262634, 'Epoch': 8, 'Micro F1': 0.6493927361905875, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6351932883262634, 'Micro ROC value': 0.6463251987927241, 'Macro ROC value': 0.6463251987927241}\n",
            "Training Epoch Loss after 9 is 0.635569155216217\n",
            "{'Validation Epoch Loss': 0.6351941227912903, 'Epoch': 9, 'Micro F1': 0.6488569676797757, 'Patience Count': 1, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6351932883262634, 'Micro ROC value': 0.6393666985341793, 'Macro ROC value': 0.6393666985341793}\n",
            "Training Epoch Loss after 10 is 0.6327325105667114\n",
            "{'Validation Epoch Loss': 0.6315773129463196, 'Epoch': 10, 'Micro F1': 0.6595042769976353, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6315773129463196, 'Micro ROC value': 0.6495162564344665, 'Macro ROC value': 0.6495162564344665}\n",
            "Training Epoch Loss after 11 is 0.6297363042831421\n",
            "{'Validation Epoch Loss': 0.6270323395729065, 'Epoch': 11, 'Micro F1': 0.6538354864608422, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6270323395729065, 'Micro ROC value': 0.6571768039036885, 'Macro ROC value': 0.6571768039036885}\n",
            "Training Epoch Loss after 12 is 0.628372311592102\n",
            "{'Validation Epoch Loss': 0.6270420551300049, 'Epoch': 12, 'Micro F1': 0.6502204186494613, 'Patience Count': 1, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6270323395729065, 'Micro ROC value': 0.6604597475221878, 'Macro ROC value': 0.6604597475221878}\n",
            "Training Epoch Loss after 13 is 0.6247557401657104\n",
            "{'Validation Epoch Loss': 0.6239655017852783, 'Epoch': 13, 'Micro F1': 0.6552222465513466, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6239655017852783, 'Micro ROC value': 0.6754581578029537, 'Macro ROC value': 0.6754581578029537}\n",
            "Training Epoch Loss after 14 is 0.6234620213508606\n",
            "{'Validation Epoch Loss': 0.6191459894180298, 'Epoch': 14, 'Micro F1': 0.6568329781333022, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6191459894180298, 'Micro ROC value': 0.6901078686106066, 'Macro ROC value': 0.6901078686106066}\n",
            "Training Epoch Loss after 15 is 0.6203134059906006\n",
            "{'Validation Epoch Loss': 0.6179448962211609, 'Epoch': 15, 'Micro F1': 0.653738358683911, 'Patience Count': 0, 'Best Val F1': 0.6595772633054039, 'Best Val Loss': 0.6179448962211609, 'Micro ROC value': 0.7075604564617426, 'Macro ROC value': 0.7075604564617426}\n",
            "Training Epoch Loss after 16 is 0.6135667562484741\n",
            "{'Validation Epoch Loss': 0.6076200008392334, 'Epoch': 16, 'Micro F1': 0.6710069191019765, 'Patience Count': 0, 'Best Val F1': 0.6710069191019765, 'Best Val Loss': 0.6076200008392334, 'Micro ROC value': 0.7415916869169489, 'Macro ROC value': 0.7415916869169489}\n",
            "Training Epoch Loss after 17 is 0.6030212044715881\n",
            "{'Validation Epoch Loss': 0.5935943126678467, 'Epoch': 17, 'Micro F1': 0.6825732803923859, 'Patience Count': 0, 'Best Val F1': 0.6825732803923859, 'Best Val Loss': 0.5935943126678467, 'Micro ROC value': 0.7641961590568453, 'Macro ROC value': 0.7641961590568453}\n",
            "Training Epoch Loss after 18 is 0.5805034637451172\n",
            "{'Validation Epoch Loss': 0.5628514885902405, 'Epoch': 18, 'Micro F1': 0.7320740394721478, 'Patience Count': 0, 'Best Val F1': 0.7320740394721478, 'Best Val Loss': 0.5628514885902405, 'Micro ROC value': 0.793715790552973, 'Macro ROC value': 0.793715790552973}\n",
            "Training Epoch Loss after 19 is 0.5488201379776001\n",
            "{'Validation Epoch Loss': 0.534384548664093, 'Epoch': 19, 'Micro F1': 0.737453653694567, 'Patience Count': 0, 'Best Val F1': 0.737453653694567, 'Best Val Loss': 0.534384548664093, 'Micro ROC value': 0.80989634142492, 'Macro ROC value': 0.80989634142492}\n",
            "Training Epoch Loss after 20 is 0.5274654030799866\n",
            "{'Validation Epoch Loss': 0.5252228379249573, 'Epoch': 20, 'Micro F1': 0.7455697311184422, 'Patience Count': 0, 'Best Val F1': 0.7455697311184422, 'Best Val Loss': 0.5252228379249573, 'Micro ROC value': 0.8188179628444978, 'Macro ROC value': 0.8188179628444978}\n",
            "Training Epoch Loss after 21 is 0.524175226688385\n",
            "{'Validation Epoch Loss': 0.5236466526985168, 'Epoch': 21, 'Micro F1': 0.7484453916445274, 'Patience Count': 0, 'Best Val F1': 0.7484453916445274, 'Best Val Loss': 0.5236466526985168, 'Micro ROC value': 0.8266760672411719, 'Macro ROC value': 0.8266760672411719}\n",
            "Training Epoch Loss after 22 is 0.5217434167861938\n",
            "{'Validation Epoch Loss': 0.5141192078590393, 'Epoch': 22, 'Micro F1': 0.7562841210988819, 'Patience Count': 0, 'Best Val F1': 0.7562841210988819, 'Best Val Loss': 0.5141192078590393, 'Micro ROC value': 0.8341651902996701, 'Macro ROC value': 0.8341651902996701}\n",
            "Training Epoch Loss after 23 is 0.5115225315093994\n",
            "{'Validation Epoch Loss': 0.5101211071014404, 'Epoch': 23, 'Micro F1': 0.759353604951608, 'Patience Count': 0, 'Best Val F1': 0.759353604951608, 'Best Val Loss': 0.5101211071014404, 'Micro ROC value': 0.8335906855406772, 'Macro ROC value': 0.8335906855406772}\n",
            "Training Epoch Loss after 24 is 0.5035110116004944\n",
            "{'Validation Epoch Loss': 0.4988274872303009, 'Epoch': 24, 'Micro F1': 0.7596268939946865, 'Patience Count': 0, 'Best Val F1': 0.7596268939946865, 'Best Val Loss': 0.4988274872303009, 'Micro ROC value': 0.8390948943021934, 'Macro ROC value': 0.8390948943021934}\n",
            "Training Epoch Loss after 25 is 0.4985327124595642\n",
            "{'Validation Epoch Loss': 0.4972829222679138, 'Epoch': 25, 'Micro F1': 0.7620611634187286, 'Patience Count': 0, 'Best Val F1': 0.7620611634187286, 'Best Val Loss': 0.4972829222679138, 'Micro ROC value': 0.8403020207679739, 'Macro ROC value': 0.8403020207679739}\n",
            "Training Epoch Loss after 26 is 0.4987051486968994\n",
            "{'Validation Epoch Loss': 0.4976997971534729, 'Epoch': 26, 'Micro F1': 0.762487957259218, 'Patience Count': 0, 'Best Val F1': 0.762487957259218, 'Best Val Loss': 0.4972829222679138, 'Micro ROC value': 0.8425028731010086, 'Macro ROC value': 0.8425028731010086}\n",
            "Training Epoch Loss after 27 is 0.4949635863304138\n",
            "{'Validation Epoch Loss': 0.49010008573532104, 'Epoch': 27, 'Micro F1': 0.7714047970102625, 'Patience Count': 0, 'Best Val F1': 0.7714047970102625, 'Best Val Loss': 0.49010008573532104, 'Micro ROC value': 0.8488433030720268, 'Macro ROC value': 0.8488433030720268}\n",
            "Training Epoch Loss after 28 is 0.4897170960903168\n",
            "{'Validation Epoch Loss': 0.48483389616012573, 'Epoch': 28, 'Micro F1': 0.7718010101304995, 'Patience Count': 0, 'Best Val F1': 0.7718010101304995, 'Best Val Loss': 0.48483389616012573, 'Micro ROC value': 0.8505426163034502, 'Macro ROC value': 0.8505426163034502}\n",
            "Training Epoch Loss after 29 is 0.48436668515205383\n",
            "{'Validation Epoch Loss': 0.48248231410980225, 'Epoch': 29, 'Micro F1': 0.7732687142356651, 'Patience Count': 0, 'Best Val F1': 0.7732687142356651, 'Best Val Loss': 0.48248231410980225, 'Micro ROC value': 0.85010156154149, 'Macro ROC value': 0.85010156154149}\n",
            "Training Epoch Loss after 30 is 0.48075127601623535\n",
            "{'Validation Epoch Loss': 0.47942739725112915, 'Epoch': 30, 'Micro F1': 0.7740051966251131, 'Patience Count': 0, 'Best Val F1': 0.7740051966251131, 'Best Val Loss': 0.47942739725112915, 'Micro ROC value': 0.8516014587763298, 'Macro ROC value': 0.8516014587763298}\n",
            "Training Epoch Loss after 31 is 0.4801231324672699\n",
            "{'Validation Epoch Loss': 0.4743574857711792, 'Epoch': 31, 'Micro F1': 0.777450150351794, 'Patience Count': 0, 'Best Val F1': 0.777450150351794, 'Best Val Loss': 0.4743574857711792, 'Micro ROC value': 0.8551539757358201, 'Macro ROC value': 0.8551539757358201}\n",
            "Training Epoch Loss after 32 is 0.47408390045166016\n",
            "{'Validation Epoch Loss': 0.46985554695129395, 'Epoch': 32, 'Micro F1': 0.7808951040784748, 'Patience Count': 0, 'Best Val F1': 0.7808951040784748, 'Best Val Loss': 0.46985554695129395, 'Micro ROC value': 0.8587041253760119, 'Macro ROC value': 0.8587041253760119}\n",
            "Training Epoch Loss after 33 is 0.4705694913864136\n",
            "{'Validation Epoch Loss': 0.4687366783618927, 'Epoch': 33, 'Micro F1': 0.7820793553472891, 'Patience Count': 0, 'Best Val F1': 0.7820793553472891, 'Best Val Loss': 0.4687366783618927, 'Micro ROC value': 0.8592122581174965, 'Macro ROC value': 0.8592122581174965}\n",
            "Training Epoch Loss after 34 is 0.46803581714630127\n",
            "{'Validation Epoch Loss': 0.46333929896354675, 'Epoch': 34, 'Micro F1': 0.7854890510948905, 'Patience Count': 0, 'Best Val F1': 0.7854890510948905, 'Best Val Loss': 0.46333929896354675, 'Micro ROC value': 0.8635537891390987, 'Macro ROC value': 0.8635537891390987}\n",
            "Training Epoch Loss after 35 is 0.46482083201408386\n",
            "{'Validation Epoch Loss': 0.4616561233997345, 'Epoch': 35, 'Micro F1': 0.786745493029706, 'Patience Count': 0, 'Best Val F1': 0.786745493029706, 'Best Val Loss': 0.4616561233997345, 'Micro ROC value': 0.8646639966775727, 'Macro ROC value': 0.8646639966775727}\n",
            "Training Epoch Loss after 36 is 0.4629770517349243\n",
            "{'Validation Epoch Loss': 0.46097829937934875, 'Epoch': 36, 'Micro F1': 0.7859019647914052, 'Patience Count': 0, 'Best Val F1': 0.786745493029706, 'Best Val Loss': 0.46097829937934875, 'Micro ROC value': 0.8635014363542577, 'Macro ROC value': 0.8635014363542577}\n",
            "Training Epoch Loss after 37 is 0.4584875702857971\n",
            "{'Validation Epoch Loss': 0.4573499858379364, 'Epoch': 37, 'Micro F1': 0.7869237731001664, 'Patience Count': 0, 'Best Val F1': 0.7869237731001664, 'Best Val Loss': 0.4573499858379364, 'Micro ROC value': 0.8648645993210575, 'Macro ROC value': 0.8648645993210575}\n",
            "Training Epoch Loss after 38 is 0.4585324823856354\n",
            "{'Validation Epoch Loss': 0.457412451505661, 'Epoch': 38, 'Micro F1': 0.7863252853764633, 'Patience Count': 1, 'Best Val F1': 0.7869237731001664, 'Best Val Loss': 0.4573499858379364, 'Micro ROC value': 0.8650113445648375, 'Macro ROC value': 0.8650113445648375}\n",
            "Training Epoch Loss after 39 is 0.4568416178226471\n",
            "{'Validation Epoch Loss': 0.4552292823791504, 'Epoch': 39, 'Micro F1': 0.7862668963302485, 'Patience Count': 0, 'Best Val F1': 0.7869237731001664, 'Best Val Loss': 0.4552292823791504, 'Micro ROC value': 0.8657604764709816, 'Macro ROC value': 0.8657604764709816}\n",
            "Training Epoch Loss after 40 is 0.4549107253551483\n",
            "{'Validation Epoch Loss': 0.4521470069885254, 'Epoch': 40, 'Micro F1': 0.7877558170087292, 'Patience Count': 0, 'Best Val F1': 0.7877558170087292, 'Best Val Loss': 0.4521470069885254, 'Micro ROC value': 0.8680944334006808, 'Macro ROC value': 0.8680944334006808}\n",
            "Training Epoch Loss after 41 is 0.455794095993042\n",
            "{'Validation Epoch Loss': 0.45485028624534607, 'Epoch': 41, 'Micro F1': 0.7873033019005634, 'Patience Count': 1, 'Best Val F1': 0.7877558170087292, 'Best Val Loss': 0.4521470069885254, 'Micro ROC value': 0.866605269384789, 'Macro ROC value': 0.866605269384789}\n",
            "Training Epoch Loss after 42 is 0.45462265610694885\n",
            "{'Validation Epoch Loss': 0.45215851068496704, 'Epoch': 42, 'Micro F1': 0.7861376310180724, 'Patience Count': 2, 'Best Val F1': 0.7877558170087292, 'Best Val Loss': 0.4521470069885254, 'Micro ROC value': 0.8678233898034151, 'Macro ROC value': 0.8678233898034151}\n",
            "Training Epoch Loss after 43 is 0.4533541798591614\n",
            "{'Validation Epoch Loss': 0.45184704661369324, 'Epoch': 43, 'Micro F1': 0.788368901993986, 'Patience Count': 0, 'Best Val F1': 0.788368901993986, 'Best Val Loss': 0.45184704661369324, 'Micro ROC value': 0.8680539337027129, 'Macro ROC value': 0.8680539337027129}\n",
            "Training Epoch Loss after 44 is 0.4537297785282135\n",
            "{'Validation Epoch Loss': 0.4500879645347595, 'Epoch': 44, 'Micro F1': 0.7892885294718711, 'Patience Count': 0, 'Best Val F1': 0.7892885294718711, 'Best Val Loss': 0.4500879645347595, 'Micro ROC value': 0.8690533183198724, 'Macro ROC value': 0.8690533183198724}\n",
            "Training Epoch Loss after 45 is 0.453225702047348\n",
            "{'Validation Epoch Loss': 0.449575811624527, 'Epoch': 45, 'Micro F1': 0.7902665459959711, 'Patience Count': 0, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.449575811624527, 'Micro ROC value': 0.8693437732691134, 'Macro ROC value': 0.8693437732691134}\n",
            "Training Epoch Loss after 46 is 0.45160067081451416\n",
            "{'Validation Epoch Loss': 0.4496647119522095, 'Epoch': 46, 'Micro F1': 0.7902519487344174, 'Patience Count': 1, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.449575811624527, 'Micro ROC value': 0.8693829620986453, 'Macro ROC value': 0.8693829620986453}\n",
            "Training Epoch Loss after 47 is 0.45221418142318726\n",
            "{'Validation Epoch Loss': 0.4497342109680176, 'Epoch': 47, 'Micro F1': 0.7888944034099203, 'Patience Count': 2, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.449575811624527, 'Micro ROC value': 0.8693576439277141, 'Macro ROC value': 0.8693576439277141}\n",
            "Training Epoch Loss after 48 is 0.4513820707798004\n",
            "{'Validation Epoch Loss': 0.45060667395591736, 'Epoch': 48, 'Micro F1': 0.788012378839192, 'Patience Count': 3, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.449575811624527, 'Micro ROC value': 0.8687503326533544, 'Macro ROC value': 0.8687503326533544}\n",
            "Training Epoch Loss after 49 is 0.44997355341911316\n",
            "{'Validation Epoch Loss': 0.448121041059494, 'Epoch': 49, 'Micro F1': 0.7900767815957725, 'Patience Count': 0, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.448121041059494, 'Micro ROC value': 0.8702264522410583, 'Macro ROC value': 0.8702264522410583}\n",
            "Training Epoch Loss after 50 is 0.451498419046402\n",
            "{'Validation Epoch Loss': 0.44861340522766113, 'Epoch': 50, 'Micro F1': 0.7896327221768072, 'Patience Count': 1, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.448121041059494, 'Micro ROC value': 0.8701367068342512, 'Macro ROC value': 0.8701367068342512}\n",
            "Training Epoch Loss after 51 is 0.45005062222480774\n",
            "{'Validation Epoch Loss': 0.44939178228378296, 'Epoch': 51, 'Micro F1': 0.7893907103027472, 'Patience Count': 2, 'Best Val F1': 0.7902665459959711, 'Best Val Loss': 0.448121041059494, 'Micro ROC value': 0.8691359409595902, 'Macro ROC value': 0.8691359409595902}\n",
            "Training Epoch Loss after 52 is 0.45164191722869873\n",
            "{'Validation Epoch Loss': 0.44621366262435913, 'Epoch': 52, 'Micro F1': 0.7906898665810292, 'Patience Count': 0, 'Best Val F1': 0.7906898665810292, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8711360794462777, 'Macro ROC value': 0.8711360794462777}\n",
            "Training Epoch Loss after 53 is 0.4499273896217346\n",
            "{'Validation Epoch Loss': 0.44842416048049927, 'Epoch': 53, 'Micro F1': 0.7903249350421861, 'Patience Count': 1, 'Best Val F1': 0.7906898665810292, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.87041229000219, 'Macro ROC value': 0.87041229000219}\n",
            "Training Epoch Loss after 54 is 0.44830191135406494\n",
            "{'Validation Epoch Loss': 0.4480079114437103, 'Epoch': 54, 'Micro F1': 0.7895797141647566, 'Patience Count': 2, 'Best Val F1': 0.7906898665810292, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8703791875838673, 'Macro ROC value': 0.8703791875838673}\n",
            "Training Epoch Loss after 55 is 0.45036232471466064\n",
            "{'Validation Epoch Loss': 0.4494436979293823, 'Epoch': 55, 'Micro F1': 0.7892562586672506, 'Patience Count': 3, 'Best Val F1': 0.7906898665810292, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8695495797684839, 'Macro ROC value': 0.8695495797684839}\n",
            "Training Epoch Loss after 56 is 0.4500575065612793\n",
            "{'Validation Epoch Loss': 0.44725653529167175, 'Epoch': 56, 'Micro F1': 0.7908327859280343, 'Patience Count': 0, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8711781776418646, 'Macro ROC value': 0.8711781776418646}\n",
            "Training Epoch Loss after 57 is 0.45035043358802795\n",
            "{'Validation Epoch Loss': 0.4467148184776306, 'Epoch': 57, 'Micro F1': 0.7897702391031443, 'Patience Count': 1, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8713228722321704, 'Macro ROC value': 0.8713228722321704}\n",
            "Training Epoch Loss after 58 is 0.44801944494247437\n",
            "{'Validation Epoch Loss': 0.4467967450618744, 'Epoch': 58, 'Micro F1': 0.7894106827438214, 'Patience Count': 2, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44621366262435913, 'Micro ROC value': 0.8711999682056726, 'Macro ROC value': 0.8711999682056726}\n",
            "Training Epoch Loss after 59 is 0.44913244247436523\n",
            "{'Validation Epoch Loss': 0.44594407081604004, 'Epoch': 59, 'Micro F1': 0.7904125186115086, 'Patience Count': 0, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44594407081604004, 'Micro ROC value': 0.87169889884895, 'Macro ROC value': 0.87169889884895}\n",
            "Training Epoch Loss after 60 is 0.4496453106403351\n",
            "{'Validation Epoch Loss': 0.44683539867401123, 'Epoch': 60, 'Micro F1': 0.7907774501503518, 'Patience Count': 1, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44594407081604004, 'Micro ROC value': 0.8708818012270546, 'Macro ROC value': 0.8708818012270546}\n",
            "Training Epoch Loss after 61 is 0.45021694898605347\n",
            "{'Validation Epoch Loss': 0.446845144033432, 'Epoch': 61, 'Micro F1': 0.7902074422271208, 'Patience Count': 2, 'Best Val F1': 0.7908327859280343, 'Best Val Loss': 0.44594407081604004, 'Micro ROC value': 0.8711775722906618, 'Macro ROC value': 0.8711775722906618}\n",
            "Training Epoch Loss after 62 is 0.4491329789161682\n",
            "{'Validation Epoch Loss': 0.4446491599082947, 'Epoch': 62, 'Micro F1': 0.7917378293555215, 'Patience Count': 0, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8726535549834624, 'Macro ROC value': 0.8726535549834624}\n",
            "Training Epoch Loss after 63 is 0.4490593373775482\n",
            "{'Validation Epoch Loss': 0.4487314522266388, 'Epoch': 63, 'Micro F1': 0.789031137322453, 'Patience Count': 1, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8703004644912555, 'Macro ROC value': 0.8703004644912555}\n",
            "Training Epoch Loss after 64 is 0.44810986518859863\n",
            "{'Validation Epoch Loss': 0.44773799180984497, 'Epoch': 64, 'Micro F1': 0.7886170554403994, 'Patience Count': 2, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8709139149467767, 'Macro ROC value': 0.8709139149467767}\n",
            "Training Epoch Loss after 65 is 0.44816288352012634\n",
            "{'Validation Epoch Loss': 0.4454582929611206, 'Epoch': 65, 'Micro F1': 0.7896826555338219, 'Patience Count': 3, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8718725829504825, 'Macro ROC value': 0.8718725829504825}\n",
            "Training Epoch Loss after 66 is 0.4488067924976349\n",
            "{'Validation Epoch Loss': 0.44595983624458313, 'Epoch': 66, 'Micro F1': 0.7916180077075792, 'Patience Count': 4, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8715554165215156, 'Macro ROC value': 0.8715554165215156}\n",
            "Training Epoch Loss after 67 is 0.44855859875679016\n",
            "{'Validation Epoch Loss': 0.44726404547691345, 'Epoch': 67, 'Micro F1': 0.7898578226724666, 'Patience Count': 5, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.4446491599082947, 'Micro ROC value': 0.8709678864175013, 'Macro ROC value': 0.8709678864175013}\n",
            "Training Epoch Loss after 68 is 0.448536217212677\n",
            "{'Validation Epoch Loss': 0.44340550899505615, 'Epoch': 68, 'Micro F1': 0.7916970776282369, 'Patience Count': 0, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8734443223023223, 'Macro ROC value': 0.8734443223023223}\n",
            "Training Epoch Loss after 69 is 0.4475429356098175\n",
            "{'Validation Epoch Loss': 0.446999728679657, 'Epoch': 69, 'Micro F1': 0.7901059761188801, 'Patience Count': 1, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8712778701528545, 'Macro ROC value': 0.8712778701528545}\n",
            "Training Epoch Loss after 70 is 0.447589248418808\n",
            "{'Validation Epoch Loss': 0.4455633759498596, 'Epoch': 70, 'Micro F1': 0.7895889291136284, 'Patience Count': 2, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8718854337087117, 'Macro ROC value': 0.8718854337087117}\n",
            "Training Epoch Loss after 71 is 0.44782155752182007\n",
            "{'Validation Epoch Loss': 0.44699326157569885, 'Epoch': 71, 'Micro F1': 0.7894982701487526, 'Patience Count': 3, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8709807339588349, 'Macro ROC value': 0.8709807339588349}\n",
            "Training Epoch Loss after 72 is 0.4490039348602295\n",
            "{'Validation Epoch Loss': 0.4447120428085327, 'Epoch': 72, 'Micro F1': 0.7904909274046391, 'Patience Count': 4, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8726071275977532, 'Macro ROC value': 0.8726071275977532}\n",
            "Training Epoch Loss after 73 is 0.4497392475605011\n",
            "{'Validation Epoch Loss': 0.4472861588001251, 'Epoch': 73, 'Micro F1': 0.7888006539764101, 'Patience Count': 5, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.871019367027468, 'Macro ROC value': 0.871019367027468}\n",
            "Epoch    74: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 74 is 0.44798433780670166\n",
            "{'Validation Epoch Loss': 0.4465400278568268, 'Epoch': 74, 'Micro F1': 0.7904563103961697, 'Patience Count': 6, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8714667617771891, 'Macro ROC value': 0.8714667617771891}\n",
            "Training Epoch Loss after 75 is 0.4484107792377472\n",
            "{'Validation Epoch Loss': 0.4449159801006317, 'Epoch': 75, 'Micro F1': 0.7911478329416229, 'Patience Count': 7, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.872604730237279, 'Macro ROC value': 0.872604730237279}\n",
            "Training Epoch Loss after 76 is 0.4478786885738373\n",
            "{'Validation Epoch Loss': 0.4458368122577667, 'Epoch': 76, 'Micro F1': 0.7905438939654921, 'Patience Count': 8, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8720417970588338, 'Macro ROC value': 0.8720417970588338}\n",
            "Training Epoch Loss after 77 is 0.4475879669189453\n",
            "{'Validation Epoch Loss': 0.44644466042518616, 'Epoch': 77, 'Micro F1': 0.7896096692260532, 'Patience Count': 9, 'Best Val F1': 0.7917378293555215, 'Best Val Loss': 0.44340550899505615, 'Micro ROC value': 0.8716346501015012, 'Macro ROC value': 0.8716346501015012}\n",
            "Training Epoch Loss after 78 is 0.4472818970680237\n",
            "{'Validation Epoch Loss': 0.4430594742298126, 'Epoch': 78, 'Micro F1': 0.792733483198552, 'Patience Count': 0, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8738256601523003, 'Macro ROC value': 0.8738256601523003}\n",
            "Training Epoch Loss after 79 is 0.4472104609012604\n",
            "{'Validation Epoch Loss': 0.44848203659057617, 'Epoch': 79, 'Micro F1': 0.7887630280559367, 'Patience Count': 1, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8705695771681098, 'Macro ROC value': 0.8705695771681098}\n",
            "Training Epoch Loss after 80 is 0.4468308985233307\n",
            "{'Validation Epoch Loss': 0.4467417299747467, 'Epoch': 80, 'Micro F1': 0.7895074883951771, 'Patience Count': 2, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8713794535072847, 'Macro ROC value': 0.8713794535072847}\n",
            "Training Epoch Loss after 81 is 0.4474702477455139\n",
            "{'Validation Epoch Loss': 0.4471900761127472, 'Epoch': 81, 'Micro F1': 0.7899100782436063, 'Patience Count': 3, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8711390286874339, 'Macro ROC value': 0.8711390286874339}\n",
            "Training Epoch Loss after 82 is 0.44823360443115234\n",
            "{'Validation Epoch Loss': 0.444986492395401, 'Epoch': 82, 'Micro F1': 0.7903948616889278, 'Patience Count': 4, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8724835747907, 'Macro ROC value': 0.8724835747907}\n",
            "Training Epoch Loss after 83 is 0.4489383101463318\n",
            "{'Validation Epoch Loss': 0.4448307454586029, 'Epoch': 83, 'Micro F1': 0.7910256035967652, 'Patience Count': 5, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8723390224134202, 'Macro ROC value': 0.8723390224134202}\n",
            "Training Epoch Loss after 84 is 0.44797080755233765\n",
            "{'Validation Epoch Loss': 0.44576895236968994, 'Epoch': 84, 'Micro F1': 0.7895074883951771, 'Patience Count': 6, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8714879964821405, 'Macro ROC value': 0.8714879964821405}\n",
            "Training Epoch Loss after 85 is 0.44779670238494873\n",
            "{'Validation Epoch Loss': 0.447230726480484, 'Epoch': 85, 'Micro F1': 0.7897118500569293, 'Patience Count': 7, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.870741749679804, 'Macro ROC value': 0.870741749679804}\n",
            "Training Epoch Loss after 86 is 0.44928157329559326\n",
            "{'Validation Epoch Loss': 0.44680461287498474, 'Epoch': 86, 'Micro F1': 0.788850611625259, 'Patience Count': 8, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8713204081745316, 'Macro ROC value': 0.8713204081745316}\n",
            "Training Epoch Loss after 87 is 0.44772645831108093\n",
            "{'Validation Epoch Loss': 0.4462449550628662, 'Epoch': 87, 'Micro F1': 0.7901351706419876, 'Patience Count': 9, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.87178708799176, 'Macro ROC value': 0.87178708799176}\n",
            "Training Epoch Loss after 88 is 0.44753509759902954\n",
            "{'Validation Epoch Loss': 0.4453762471675873, 'Epoch': 88, 'Micro F1': 0.7913467433509473, 'Patience Count': 10, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8722256608464298, 'Macro ROC value': 0.8722256608464298}\n",
            "Training Epoch Loss after 89 is 0.4476037323474884\n",
            "{'Validation Epoch Loss': 0.44494572281837463, 'Epoch': 89, 'Micro F1': 0.7902604227490365, 'Patience Count': 11, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8722991578867043, 'Macro ROC value': 0.8722991578867043}\n",
            "Epoch    90: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 90 is 0.4465145170688629\n",
            "{'Validation Epoch Loss': 0.4454437792301178, 'Epoch': 90, 'Micro F1': 0.7907536896194327, 'Patience Count': 12, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8721102048496577, 'Macro ROC value': 0.8721102048496577}\n",
            "Training Epoch Loss after 91 is 0.4480096101760864\n",
            "{'Validation Epoch Loss': 0.4436569809913635, 'Epoch': 91, 'Micro F1': 0.7914136400794114, 'Patience Count': 13, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.873225766274468, 'Macro ROC value': 0.873225766274468}\n",
            "Training Epoch Loss after 92 is 0.44749364256858826\n",
            "{'Validation Epoch Loss': 0.44550758600234985, 'Epoch': 92, 'Micro F1': 0.7911332350408011, 'Patience Count': 14, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8720580297617918, 'Macro ROC value': 0.8720580297617918}\n",
            "Training Epoch Loss after 93 is 0.4485223889350891\n",
            "{'Validation Epoch Loss': 0.44466835260391235, 'Epoch': 93, 'Micro F1': 0.7907628528887981, 'Patience Count': 15, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8724116916580822, 'Macro ROC value': 0.8724116916580822}\n",
            "Training Epoch Loss after 94 is 0.4479031562805176\n",
            "{'Validation Epoch Loss': 0.4456225037574768, 'Epoch': 94, 'Micro F1': 0.7893177239949786, 'Patience Count': 16, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8723150845750197, 'Macro ROC value': 0.8723150845750197}\n",
            "Training Epoch Loss after 95 is 0.44909030199050903\n",
            "{'Validation Epoch Loss': 0.4446770250797272, 'Epoch': 95, 'Micro F1': 0.7902312273735841, 'Patience Count': 17, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8726708947472168, 'Macro ROC value': 0.8726708947472168}\n",
            "Training Epoch Loss after 96 is 0.44769150018692017\n",
            "{'Validation Epoch Loss': 0.4439237713813782, 'Epoch': 96, 'Micro F1': 0.7916532858435757, 'Patience Count': 18, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8732920974921085, 'Macro ROC value': 0.8732920974921085}\n",
            "Training Epoch Loss after 97 is 0.44837841391563416\n",
            "{'Validation Epoch Loss': 0.4465791583061218, 'Epoch': 97, 'Micro F1': 0.7897963652288156, 'Patience Count': 19, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8712528064185702, 'Macro ROC value': 0.8712528064185702}\n",
            "Training Epoch Loss after 98 is 0.44825810194015503\n",
            "{'Validation Epoch Loss': 0.4445624351501465, 'Epoch': 98, 'Micro F1': 0.7904240566382016, 'Patience Count': 20, 'Best Val F1': 0.792733483198552, 'Best Val Loss': 0.4430594742298126, 'Micro ROC value': 0.8724026054190688, 'Macro ROC value': 0.8724026054190688}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.7937989022538829, 'Micro Recall': 0.7937989022538829, 'Micro Precision': 0.7937989022538829, 'Micro ROC_AUC_Score': 0.875123507791732, 'Macro ROC_AUC_Score': 0.875123507791732}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(2, 137010) (2, 34253) (2, 42815)\n",
            "Training Epoch Loss after 0 is 0.7151182889938354\n",
            "{'Validation Epoch Loss': 0.6969175934791565, 'Epoch': 0, 'Micro F1': 0.5436750065687677, 'Patience Count': 0, 'Best Val F1': 0.5436750065687677, 'Best Val Loss': 0.6969175934791565, 'Micro ROC value': 0.5170011799068835, 'Macro ROC value': 0.5170011799068835}\n",
            "Training Epoch Loss after 1 is 0.7037500143051147\n",
            "{'Validation Epoch Loss': 0.6958910822868347, 'Epoch': 1, 'Micro F1': 0.47236738387878435, 'Patience Count': 0, 'Best Val F1': 0.5436750065687677, 'Best Val Loss': 0.6958910822868347, 'Micro ROC value': 0.5353014227629895, 'Macro ROC value': 0.5353014227629895}\n",
            "Training Epoch Loss after 2 is 0.6864711046218872\n",
            "{'Validation Epoch Loss': 0.6834640502929688, 'Epoch': 2, 'Micro F1': 0.5726443325304722, 'Patience Count': 0, 'Best Val F1': 0.5726443325304722, 'Best Val Loss': 0.6834640502929688, 'Micro ROC value': 0.6197153404270104, 'Macro ROC value': 0.6197153404270104}\n",
            "Training Epoch Loss after 3 is 0.6838085055351257\n",
            "{'Validation Epoch Loss': 0.6818938255310059, 'Epoch': 3, 'Micro F1': 0.581793232314385, 'Patience Count': 0, 'Best Val F1': 0.581793232314385, 'Best Val Loss': 0.6818938255310059, 'Micro ROC value': 0.6267612686735615, 'Macro ROC value': 0.6267612686735615}\n",
            "Training Epoch Loss after 4 is 0.6772945523262024\n",
            "{'Validation Epoch Loss': 0.6728122234344482, 'Epoch': 4, 'Micro F1': 0.6068462156047004, 'Patience Count': 0, 'Best Val F1': 0.6068462156047004, 'Best Val Loss': 0.6728122234344482, 'Micro ROC value': 0.6274550508245008, 'Macro ROC value': 0.6274550508245008}\n",
            "Training Epoch Loss after 5 is 0.6704829335212708\n",
            "{'Validation Epoch Loss': 0.6691422462463379, 'Epoch': 5, 'Micro F1': 0.6368586234581417, 'Patience Count': 0, 'Best Val F1': 0.6368586234581417, 'Best Val Loss': 0.6691422462463379, 'Micro ROC value': 0.6314898554623585, 'Macro ROC value': 0.6314898554623585}\n",
            "Training Epoch Loss after 6 is 0.6678967475891113\n",
            "{'Validation Epoch Loss': 0.6625814437866211, 'Epoch': 6, 'Micro F1': 0.6593291098589904, 'Patience Count': 0, 'Best Val F1': 0.6593291098589904, 'Best Val Loss': 0.6625814437866211, 'Micro ROC value': 0.6512626434358018, 'Macro ROC value': 0.6512626434358018}\n",
            "Training Epoch Loss after 7 is 0.6586317420005798\n",
            "{'Validation Epoch Loss': 0.6549569368362427, 'Epoch': 7, 'Micro F1': 0.6273356300362023, 'Patience Count': 0, 'Best Val F1': 0.6593291098589904, 'Best Val Loss': 0.6549569368362427, 'Micro ROC value': 0.6718506818187768, 'Macro ROC value': 0.6718506818187768}\n",
            "Training Epoch Loss after 8 is 0.6534444689750671\n",
            "{'Validation Epoch Loss': 0.6508039832115173, 'Epoch': 8, 'Micro F1': 0.6229673313286428, 'Patience Count': 0, 'Best Val F1': 0.6593291098589904, 'Best Val Loss': 0.6508039832115173, 'Micro ROC value': 0.6966413574990776, 'Macro ROC value': 0.6966413574990776}\n",
            "Training Epoch Loss after 9 is 0.6455610394477844\n",
            "{'Validation Epoch Loss': 0.6395214200019836, 'Epoch': 9, 'Micro F1': 0.6525121887133973, 'Patience Count': 0, 'Best Val F1': 0.6593291098589904, 'Best Val Loss': 0.6395214200019836, 'Micro ROC value': 0.7313629634642157, 'Macro ROC value': 0.7313629634642157}\n",
            "Training Epoch Loss after 10 is 0.6331607699394226\n",
            "{'Validation Epoch Loss': 0.6264961361885071, 'Epoch': 10, 'Micro F1': 0.6981928062594885, 'Patience Count': 0, 'Best Val F1': 0.6981928062594885, 'Best Val Loss': 0.6264961361885071, 'Micro ROC value': 0.7538441972047102, 'Macro ROC value': 0.7538441972047102}\n",
            "Training Epoch Loss after 11 is 0.615815281867981\n",
            "{'Validation Epoch Loss': 0.6007575988769531, 'Epoch': 11, 'Micro F1': 0.7157183312410592, 'Patience Count': 0, 'Best Val F1': 0.7157183312410592, 'Best Val Loss': 0.6007575988769531, 'Micro ROC value': 0.7773551592677918, 'Macro ROC value': 0.7773551592677918}\n",
            "Training Epoch Loss after 12 is 0.5865572094917297\n",
            "{'Validation Epoch Loss': 0.5708611607551575, 'Epoch': 12, 'Micro F1': 0.7270086418311339, 'Patience Count': 0, 'Best Val F1': 0.7270086418311339, 'Best Val Loss': 0.5708611607551575, 'Micro ROC value': 0.7949173862716852, 'Macro ROC value': 0.7949173862716852}\n",
            "Training Epoch Loss after 13 is 0.5519272685050964\n",
            "{'Validation Epoch Loss': 0.5361302495002747, 'Epoch': 13, 'Micro F1': 0.7339941027063323, 'Patience Count': 0, 'Best Val F1': 0.7339941027063323, 'Best Val Loss': 0.5361302495002747, 'Micro ROC value': 0.8067995992721216, 'Macro ROC value': 0.8067995992721216}\n",
            "Training Epoch Loss after 14 is 0.526114284992218\n",
            "{'Validation Epoch Loss': 0.5217983722686768, 'Epoch': 14, 'Micro F1': 0.7508795492036612, 'Patience Count': 0, 'Best Val F1': 0.7508795492036612, 'Best Val Loss': 0.5217983722686768, 'Micro ROC value': 0.816796302522173, 'Macro ROC value': 0.816796302522173}\n",
            "Training Epoch Loss after 15 is 0.5240616798400879\n",
            "{'Validation Epoch Loss': 0.5245516300201416, 'Epoch': 15, 'Micro F1': 0.7504634833510941, 'Patience Count': 1, 'Best Val F1': 0.7508795492036612, 'Best Val Loss': 0.5217983722686768, 'Micro ROC value': 0.8217282343642551, 'Macro ROC value': 0.8217282343642551}\n",
            "Training Epoch Loss after 16 is 0.5245330929756165\n",
            "{'Validation Epoch Loss': 0.5211471915245056, 'Epoch': 16, 'Micro F1': 0.7497445330063356, 'Patience Count': 0, 'Best Val F1': 0.7508795492036612, 'Best Val Loss': 0.5211471915245056, 'Micro ROC value': 0.8276073989032957, 'Macro ROC value': 0.8276073989032957}\n",
            "Training Epoch Loss after 17 is 0.5170735120773315\n",
            "{'Validation Epoch Loss': 0.5092070698738098, 'Epoch': 17, 'Micro F1': 0.7571015677458909, 'Patience Count': 0, 'Best Val F1': 0.7571015677458909, 'Best Val Loss': 0.5092070698738098, 'Micro ROC value': 0.8308757061661263, 'Macro ROC value': 0.8308757061661263}\n",
            "Training Epoch Loss after 18 is 0.5068108439445496\n",
            "{'Validation Epoch Loss': 0.5003340244293213, 'Epoch': 18, 'Micro F1': 0.7655285169992556, 'Patience Count': 0, 'Best Val F1': 0.7655285169992556, 'Best Val Loss': 0.5003340244293213, 'Micro ROC value': 0.8354185286343931, 'Macro ROC value': 0.8354185286343931}\n",
            "Training Epoch Loss after 19 is 0.49778881669044495\n",
            "{'Validation Epoch Loss': 0.49456802010536194, 'Epoch': 19, 'Micro F1': 0.7651120356178381, 'Patience Count': 0, 'Best Val F1': 0.7655285169992556, 'Best Val Loss': 0.49456802010536194, 'Micro ROC value': 0.8427043472315503, 'Macro ROC value': 0.8427043472315503}\n",
            "Training Epoch Loss after 20 is 0.4951658844947815\n",
            "{'Validation Epoch Loss': 0.49564310908317566, 'Epoch': 20, 'Micro F1': 0.7658307301550229, 'Patience Count': 0, 'Best Val F1': 0.7658307301550229, 'Best Val Loss': 0.49456802010536194, 'Micro ROC value': 0.8467945233986176, 'Macro ROC value': 0.8467945233986176}\n",
            "Training Epoch Loss after 21 is 0.49362823367118835\n",
            "{'Validation Epoch Loss': 0.4927659332752228, 'Epoch': 21, 'Micro F1': 0.772049163576913, 'Patience Count': 0, 'Best Val F1': 0.772049163576913, 'Best Val Loss': 0.4927659332752228, 'Micro ROC value': 0.8499631954083221, 'Macro ROC value': 0.8499631954083221}\n",
            "Training Epoch Loss after 22 is 0.4893161952495575\n",
            "{'Validation Epoch Loss': 0.4877394437789917, 'Epoch': 22, 'Micro F1': 0.771173327883689, 'Patience Count': 0, 'Best Val F1': 0.772049163576913, 'Best Val Loss': 0.4877394437789917, 'Micro ROC value': 0.8499964985408548, 'Macro ROC value': 0.8499964985408548}\n",
            "Training Epoch Loss after 23 is 0.4855492413043976\n",
            "{'Validation Epoch Loss': 0.48194679617881775, 'Epoch': 23, 'Micro F1': 0.7739905993635594, 'Patience Count': 0, 'Best Val F1': 0.7739905993635594, 'Best Val Loss': 0.48194679617881775, 'Micro ROC value': 0.8509653628508677, 'Macro ROC value': 0.8509653628508677}\n",
            "Training Epoch Loss after 24 is 0.48077118396759033\n",
            "{'Validation Epoch Loss': 0.4768674075603485, 'Epoch': 24, 'Micro F1': 0.780398797185648, 'Patience Count': 0, 'Best Val F1': 0.780398797185648, 'Best Val Loss': 0.4768674075603485, 'Micro ROC value': 0.8535530657258379, 'Macro ROC value': 0.8535530657258379}\n",
            "Training Epoch Loss after 25 is 0.4785420596599579\n",
            "{'Validation Epoch Loss': 0.477324903011322, 'Epoch': 25, 'Micro F1': 0.7758784286819556, 'Patience Count': 1, 'Best Val F1': 0.780398797185648, 'Best Val Loss': 0.4768674075603485, 'Micro ROC value': 0.8526417531545535, 'Macro ROC value': 0.8526417531545535}\n",
            "Training Epoch Loss after 26 is 0.47542041540145874\n",
            "{'Validation Epoch Loss': 0.4747799336910248, 'Epoch': 26, 'Micro F1': 0.7773998598621978, 'Patience Count': 0, 'Best Val F1': 0.780398797185648, 'Best Val Loss': 0.4747799336910248, 'Micro ROC value': 0.8556505086586284, 'Macro ROC value': 0.8556505086586284}\n",
            "Training Epoch Loss after 27 is 0.47280147671699524\n",
            "{'Validation Epoch Loss': 0.46970996260643005, 'Epoch': 27, 'Micro F1': 0.7820711199346023, 'Patience Count': 0, 'Best Val F1': 0.7820711199346023, 'Best Val Loss': 0.46970996260643005, 'Micro ROC value': 0.8594057616984858, 'Macro ROC value': 0.8594057616984858}\n",
            "Training Epoch Loss after 28 is 0.4699041247367859\n",
            "{'Validation Epoch Loss': 0.4677368402481079, 'Epoch': 28, 'Micro F1': 0.782161885993723, 'Patience Count': 0, 'Best Val F1': 0.782161885993723, 'Best Val Loss': 0.4677368402481079, 'Micro ROC value': 0.8602671942725795, 'Macro ROC value': 0.8602671942725795}\n",
            "Training Epoch Loss after 29 is 0.46749961376190186\n",
            "{'Validation Epoch Loss': 0.4644200801849365, 'Epoch': 29, 'Micro F1': 0.7876682334394067, 'Patience Count': 0, 'Best Val F1': 0.7876682334394067, 'Best Val Loss': 0.4644200801849365, 'Micro ROC value': 0.8635481047195246, 'Macro ROC value': 0.8635481047195246}\n",
            "Training Epoch Loss after 30 is 0.46630075573921204\n",
            "{'Validation Epoch Loss': 0.46328362822532654, 'Epoch': 30, 'Micro F1': 0.7859603538376202, 'Patience Count': 0, 'Best Val F1': 0.7876682334394067, 'Best Val Loss': 0.46328362822532654, 'Micro ROC value': 0.8635377937761533, 'Macro ROC value': 0.8635377937761533}\n",
            "Training Epoch Loss after 31 is 0.46159350872039795\n",
            "{'Validation Epoch Loss': 0.4573245644569397, 'Epoch': 31, 'Micro F1': 0.7885701773593169, 'Patience Count': 0, 'Best Val F1': 0.7885701773593169, 'Best Val Loss': 0.4573245644569397, 'Micro ROC value': 0.8659803835374815, 'Macro ROC value': 0.8659803835374815}\n",
            "Training Epoch Loss after 32 is 0.4603903889656067\n",
            "{'Validation Epoch Loss': 0.454802542924881, 'Epoch': 32, 'Micro F1': 0.7880039704551426, 'Patience Count': 0, 'Best Val F1': 0.7885701773593169, 'Best Val Loss': 0.454802542924881, 'Micro ROC value': 0.8659533586583967, 'Macro ROC value': 0.8659533586583967}\n",
            "Training Epoch Loss after 33 is 0.4588855803012848\n",
            "{'Validation Epoch Loss': 0.4591280519962311, 'Epoch': 33, 'Micro F1': 0.7868653840539515, 'Patience Count': 1, 'Best Val F1': 0.7885701773593169, 'Best Val Loss': 0.454802542924881, 'Micro ROC value': 0.8633373007956958, 'Macro ROC value': 0.8633373007956958}\n",
            "Training Epoch Loss after 34 is 0.4582950472831726\n",
            "{'Validation Epoch Loss': 0.45565465092658997, 'Epoch': 34, 'Micro F1': 0.7877266224856216, 'Patience Count': 2, 'Best Val F1': 0.7885701773593169, 'Best Val Loss': 0.454802542924881, 'Micro ROC value': 0.8652082646191029, 'Macro ROC value': 0.8652082646191029}\n",
            "Training Epoch Loss after 35 is 0.4567810297012329\n",
            "{'Validation Epoch Loss': 0.453543484210968, 'Epoch': 35, 'Micro F1': 0.7897556418415905, 'Patience Count': 0, 'Best Val F1': 0.7897556418415905, 'Best Val Loss': 0.453543484210968, 'Micro ROC value': 0.8669100714396109, 'Macro ROC value': 0.8669100714396109}\n",
            "Training Epoch Loss after 36 is 0.45563066005706787\n",
            "{'Validation Epoch Loss': 0.45292752981185913, 'Epoch': 36, 'Micro F1': 0.7894053075643009, 'Patience Count': 0, 'Best Val F1': 0.7897556418415905, 'Best Val Loss': 0.45292752981185913, 'Micro ROC value': 0.8675991799755958, 'Macro ROC value': 0.8675991799755958}\n",
            "Training Epoch Loss after 37 is 0.4559480547904968\n",
            "{'Validation Epoch Loss': 0.455285906791687, 'Epoch': 37, 'Micro F1': 0.786558841561323, 'Patience Count': 1, 'Best Val F1': 0.7897556418415905, 'Best Val Loss': 0.45292752981185913, 'Micro ROC value': 0.8660466608699633, 'Macro ROC value': 0.8660466608699633}\n",
            "Training Epoch Loss after 38 is 0.45371511578559875\n",
            "{'Validation Epoch Loss': 0.4519897401332855, 'Epoch': 38, 'Micro F1': 0.7890549732870115, 'Patience Count': 0, 'Best Val F1': 0.7897556418415905, 'Best Val Loss': 0.4519897401332855, 'Micro ROC value': 0.8678978065445574, 'Macro ROC value': 0.8678978065445574}\n",
            "Training Epoch Loss after 39 is 0.4547719359397888\n",
            "{'Validation Epoch Loss': 0.45111405849456787, 'Epoch': 39, 'Micro F1': 0.7897556418415905, 'Patience Count': 0, 'Best Val F1': 0.7897556418415905, 'Best Val Loss': 0.45111405849456787, 'Micro ROC value': 0.8688272467846688, 'Macro ROC value': 0.8688272467846688}\n",
            "Training Epoch Loss after 40 is 0.45384499430656433\n",
            "{'Validation Epoch Loss': 0.45021963119506836, 'Epoch': 40, 'Micro F1': 0.7908151348826346, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.45021963119506836, 'Micro ROC value': 0.8695943715380179, 'Macro ROC value': 0.8695943715380179}\n",
            "Training Epoch Loss after 41 is 0.4523811936378479\n",
            "{'Validation Epoch Loss': 0.44960302114486694, 'Epoch': 41, 'Micro F1': 0.7907336583656907, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44960302114486694, 'Micro ROC value': 0.8698768283726382, 'Macro ROC value': 0.8698768283726382}\n",
            "Training Epoch Loss after 42 is 0.45277297496795654\n",
            "{'Validation Epoch Loss': 0.4515484869480133, 'Epoch': 42, 'Micro F1': 0.7897702391031443, 'Patience Count': 1, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44960302114486694, 'Micro ROC value': 0.8683239721743747, 'Macro ROC value': 0.8683239721743747}\n",
            "Training Epoch Loss after 43 is 0.4522216022014618\n",
            "{'Validation Epoch Loss': 0.45223790407180786, 'Epoch': 43, 'Micro F1': 0.7892093892327455, 'Patience Count': 2, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44960302114486694, 'Micro ROC value': 0.8679905684378729, 'Macro ROC value': 0.8679905684378729}\n",
            "Training Epoch Loss after 44 is 0.45295894145965576\n",
            "{'Validation Epoch Loss': 0.450338751077652, 'Epoch': 44, 'Micro F1': 0.7883543047324322, 'Patience Count': 3, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44960302114486694, 'Micro ROC value': 0.8690189962385653, 'Macro ROC value': 0.8690189962385653}\n",
            "Training Epoch Loss after 45 is 0.4528924524784088\n",
            "{'Validation Epoch Loss': 0.44940122961997986, 'Epoch': 45, 'Micro F1': 0.790383324088401, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44940122961997986, 'Micro ROC value': 0.8697526964617, 'Macro ROC value': 0.8697526964617}\n",
            "Training Epoch Loss after 46 is 0.45128196477890015\n",
            "{'Validation Epoch Loss': 0.45009008049964905, 'Epoch': 46, 'Micro F1': 0.7902081569497563, 'Patience Count': 1, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44940122961997986, 'Micro ROC value': 0.8694585207087155, 'Macro ROC value': 0.8694585207087155}\n",
            "Training Epoch Loss after 47 is 0.4507090151309967\n",
            "{'Validation Epoch Loss': 0.4505625367164612, 'Epoch': 47, 'Micro F1': 0.7879163868858202, 'Patience Count': 2, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44940122961997986, 'Micro ROC value': 0.8685938333634392, 'Macro ROC value': 0.8685938333634392}\n",
            "Training Epoch Loss after 48 is 0.45151835680007935\n",
            "{'Validation Epoch Loss': 0.44895505905151367, 'Epoch': 48, 'Micro F1': 0.7901613020947376, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.44895505905151367, 'Micro ROC value': 0.8701095137941122, 'Macro ROC value': 0.8701095137941122}\n",
            "Training Epoch Loss after 49 is 0.4515630602836609\n",
            "{'Validation Epoch Loss': 0.4488345980644226, 'Epoch': 49, 'Micro F1': 0.7901789624266488, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.4488345980644226, 'Micro ROC value': 0.8700807059165285, 'Macro ROC value': 0.8700807059165285}\n",
            "Training Epoch Loss after 50 is 0.4523593783378601\n",
            "{'Validation Epoch Loss': 0.44907110929489136, 'Epoch': 50, 'Micro F1': 0.7901205733804338, 'Patience Count': 1, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.4488345980644226, 'Micro ROC value': 0.8698470278498831, 'Macro ROC value': 0.8698470278498831}\n",
            "Training Epoch Loss after 51 is 0.45153942704200745\n",
            "{'Validation Epoch Loss': 0.4483621120452881, 'Epoch': 51, 'Micro F1': 0.7903103377806323, 'Patience Count': 0, 'Best Val F1': 0.7908151348826346, 'Best Val Loss': 0.4483621120452881, 'Micro ROC value': 0.8702819843100316, 'Macro ROC value': 0.8702819843100316}\n",
            "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 52 is 0.45214417576789856\n",
            "{'Validation Epoch Loss': 0.44734346866607666, 'Epoch': 52, 'Micro F1': 0.7909288644292952, 'Patience Count': 0, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8708464123021031, 'Macro ROC value': 0.8708464123021031}\n",
            "Training Epoch Loss after 53 is 0.4516340494155884\n",
            "{'Validation Epoch Loss': 0.4505941569805145, 'Epoch': 53, 'Micro F1': 0.7880269765269181, 'Patience Count': 1, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8687687489584626, 'Macro ROC value': 0.8687687489584626}\n",
            "Training Epoch Loss after 54 is 0.4510079324245453\n",
            "{'Validation Epoch Loss': 0.4506663680076599, 'Epoch': 54, 'Micro F1': 0.7897203083031648, 'Patience Count': 2, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8686389129846059, 'Macro ROC value': 0.8686389129846059}\n",
            "Training Epoch Loss after 55 is 0.45065030455589294\n",
            "{'Validation Epoch Loss': 0.44887349009513855, 'Epoch': 55, 'Micro F1': 0.7907482556272445, 'Patience Count': 3, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8697845992321778, 'Macro ROC value': 0.8697845992321778}\n",
            "Training Epoch Loss after 56 is 0.4502111077308655\n",
            "{'Validation Epoch Loss': 0.45089617371559143, 'Epoch': 56, 'Micro F1': 0.7884180254879348, 'Patience Count': 4, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8685594378115103, 'Macro ROC value': 0.8685594378115103}\n",
            "Training Epoch Loss after 57 is 0.45080694556236267\n",
            "{'Validation Epoch Loss': 0.44975441694259644, 'Epoch': 57, 'Micro F1': 0.7897410445800368, 'Patience Count': 5, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8693113654989293, 'Macro ROC value': 0.8693113654989293}\n",
            "Training Epoch Loss after 58 is 0.4499156177043915\n",
            "{'Validation Epoch Loss': 0.4488409757614136, 'Epoch': 58, 'Micro F1': 0.7898839500766367, 'Patience Count': 6, 'Best Val F1': 0.7909288644292952, 'Best Val Loss': 0.44734346866607666, 'Micro ROC value': 0.8700751911369314, 'Macro ROC value': 0.8700751911369314}\n",
            "Training Epoch Loss after 59 is 0.4522669017314911\n",
            "{'Validation Epoch Loss': 0.44716644287109375, 'Epoch': 59, 'Micro F1': 0.7913552493285063, 'Patience Count': 0, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8708241231670553, 'Macro ROC value': 0.8708241231670553}\n",
            "Training Epoch Loss after 60 is 0.45142027735710144\n",
            "{'Validation Epoch Loss': 0.44888490438461304, 'Epoch': 60, 'Micro F1': 0.7892009459025486, 'Patience Count': 1, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8697474146335477, 'Macro ROC value': 0.8697474146335477}\n",
            "Training Epoch Loss after 61 is 0.45086878538131714\n",
            "{'Validation Epoch Loss': 0.44891881942749023, 'Epoch': 61, 'Micro F1': 0.7909088255043354, 'Patience Count': 2, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8699575946590052, 'Macro ROC value': 0.8699575946590052}\n",
            "Training Epoch Loss after 62 is 0.45102959871292114\n",
            "{'Validation Epoch Loss': 0.44949859380722046, 'Epoch': 62, 'Micro F1': 0.7888798061483665, 'Patience Count': 3, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8693569288310835, 'Macro ROC value': 0.8693569288310835}\n",
            "Training Epoch Loss after 63 is 0.44982871413230896\n",
            "{'Validation Epoch Loss': 0.44728365540504456, 'Epoch': 63, 'Micro F1': 0.7900913788573264, 'Patience Count': 4, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8707907576639635, 'Macro ROC value': 0.8707907576639635}\n",
            "Training Epoch Loss after 64 is 0.4505188465118408\n",
            "{'Validation Epoch Loss': 0.450203537940979, 'Epoch': 64, 'Micro F1': 0.7886515919010846, 'Patience Count': 5, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8689904516861542, 'Macro ROC value': 0.8689904516861542}\n",
            "Training Epoch Loss after 65 is 0.45140451192855835\n",
            "{'Validation Epoch Loss': 0.45011991262435913, 'Epoch': 65, 'Micro F1': 0.7894782938720696, 'Patience Count': 6, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8690795753214815, 'Macro ROC value': 0.8690795753214815}\n",
            "Training Epoch Loss after 66 is 0.4515414535999298\n",
            "{'Validation Epoch Loss': 0.44907549023628235, 'Epoch': 66, 'Micro F1': 0.7899891980264503, 'Patience Count': 7, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8699230407466092, 'Macro ROC value': 0.8699230407466092}\n",
            "Training Epoch Loss after 67 is 0.45106127858161926\n",
            "{'Validation Epoch Loss': 0.44894033670425415, 'Epoch': 67, 'Micro F1': 0.7895512801798381, 'Patience Count': 8, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8697833233088691, 'Macro ROC value': 0.8697833233088691}\n",
            "Training Epoch Loss after 68 is 0.44984644651412964\n",
            "{'Validation Epoch Loss': 0.44902920722961426, 'Epoch': 68, 'Micro F1': 0.7901789624266488, 'Patience Count': 9, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8697441349054972, 'Macro ROC value': 0.8697441349054972}\n",
            "Training Epoch Loss after 69 is 0.4496302604675293\n",
            "{'Validation Epoch Loss': 0.4526442289352417, 'Epoch': 69, 'Micro F1': 0.7878288033164978, 'Patience Count': 10, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8678402710970022, 'Macro ROC value': 0.8678402710970022}\n",
            "Training Epoch Loss after 70 is 0.44950923323631287\n",
            "{'Validation Epoch Loss': 0.44839516282081604, 'Epoch': 70, 'Micro F1': 0.7908358391965669, 'Patience Count': 11, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8704338298377655, 'Macro ROC value': 0.8704338298377655}\n",
            "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 71 is 0.4508620798587799\n",
            "{'Validation Epoch Loss': 0.4510160982608795, 'Epoch': 71, 'Micro F1': 0.7884971899861325, 'Patience Count': 12, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.868570751385711, 'Macro ROC value': 0.868570751385711}\n",
            "Training Epoch Loss after 72 is 0.4492707848548889\n",
            "{'Validation Epoch Loss': 0.4481474757194519, 'Epoch': 72, 'Micro F1': 0.7904709076577234, 'Patience Count': 13, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8704717048157409, 'Macro ROC value': 0.8704717048157409}\n",
            "Training Epoch Loss after 73 is 0.45095446705818176\n",
            "{'Validation Epoch Loss': 0.45115649700164795, 'Epoch': 73, 'Micro F1': 0.7886577622071381, 'Patience Count': 14, 'Best Val F1': 0.7913552493285063, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8684977539974565, 'Macro ROC value': 0.8684977539974565}\n",
            "Training Epoch Loss after 74 is 0.4505903124809265\n",
            "{'Validation Epoch Loss': 0.4477742314338684, 'Epoch': 74, 'Micro F1': 0.7916240913204682, 'Patience Count': 0, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8704252989650891, 'Macro ROC value': 0.8704252989650891}\n",
            "Training Epoch Loss after 75 is 0.44916144013404846\n",
            "{'Validation Epoch Loss': 0.45088255405426025, 'Epoch': 75, 'Micro F1': 0.7891425568563337, 'Patience Count': 1, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8686180013282881, 'Macro ROC value': 0.8686180013282881}\n",
            "Training Epoch Loss after 76 is 0.4491453170776367\n",
            "{'Validation Epoch Loss': 0.44879135489463806, 'Epoch': 76, 'Micro F1': 0.7899976643699638, 'Patience Count': 2, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8700241620371707, 'Macro ROC value': 0.8700241620371707}\n",
            "Training Epoch Loss after 77 is 0.45067745447158813\n",
            "{'Validation Epoch Loss': 0.44882023334503174, 'Epoch': 77, 'Micro F1': 0.7901497679035413, 'Patience Count': 3, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8700414578507442, 'Macro ROC value': 0.8700414578507442}\n",
            "Training Epoch Loss after 78 is 0.4495468735694885\n",
            "{'Validation Epoch Loss': 0.4500679671764374, 'Epoch': 78, 'Micro F1': 0.7893438435150719, 'Patience Count': 4, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8693058252749422, 'Macro ROC value': 0.8693058252749422}\n",
            "Training Epoch Loss after 79 is 0.45196104049682617\n",
            "{'Validation Epoch Loss': 0.4488218128681183, 'Epoch': 79, 'Micro F1': 0.7882782278665791, 'Patience Count': 5, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8698942399846873, 'Macro ROC value': 0.8698942399846873}\n",
            "Training Epoch Loss after 80 is 0.4501458406448364\n",
            "{'Validation Epoch Loss': 0.44969597458839417, 'Epoch': 80, 'Micro F1': 0.7888652088868128, 'Patience Count': 6, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8692748001109096, 'Macro ROC value': 0.8692748001109096}\n",
            "Training Epoch Loss after 81 is 0.450035035610199\n",
            "{'Validation Epoch Loss': 0.4513758718967438, 'Epoch': 81, 'Micro F1': 0.7877089263557404, 'Patience Count': 7, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.868340463091824, 'Macro ROC value': 0.868340463091824}\n",
            "Training Epoch Loss after 82 is 0.4506020247936249\n",
            "{'Validation Epoch Loss': 0.4473678469657898, 'Epoch': 82, 'Micro F1': 0.7904240566382016, 'Patience Count': 8, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8708949126929143, 'Macro ROC value': 0.8708949126929143}\n",
            "Training Epoch Loss after 83 is 0.4502693712711334\n",
            "{'Validation Epoch Loss': 0.4478972554206848, 'Epoch': 83, 'Micro F1': 0.7895074883951771, 'Patience Count': 9, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8705497087324061, 'Macro ROC value': 0.8705497087324061}\n",
            "Training Epoch Loss after 84 is 0.45066016912460327\n",
            "{'Validation Epoch Loss': 0.4492368996143341, 'Epoch': 84, 'Micro F1': 0.7893761130411936, 'Patience Count': 10, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8697153823104027, 'Macro ROC value': 0.8697153823104027}\n",
            "Training Epoch Loss after 85 is 0.4515093266963959\n",
            "{'Validation Epoch Loss': 0.44730499386787415, 'Epoch': 85, 'Micro F1': 0.7903072768411065, 'Patience Count': 11, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8708123494360147, 'Macro ROC value': 0.8708123494360147}\n",
            "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 86 is 0.4501645565032959\n",
            "{'Validation Epoch Loss': 0.4489368796348572, 'Epoch': 86, 'Micro F1': 0.7901758995693745, 'Patience Count': 12, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8699527445236583, 'Macro ROC value': 0.8699527445236583}\n",
            "Training Epoch Loss after 87 is 0.4498438835144043\n",
            "{'Validation Epoch Loss': 0.44924843311309814, 'Epoch': 87, 'Micro F1': 0.7899746007648965, 'Patience Count': 13, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8696188468222353, 'Macro ROC value': 0.8696188468222353}\n",
            "Training Epoch Loss after 88 is 0.4508972764015198\n",
            "{'Validation Epoch Loss': 0.44854050874710083, 'Epoch': 88, 'Micro F1': 0.7905730884885995, 'Patience Count': 14, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8701497340493838, 'Macro ROC value': 0.8701497340493838}\n",
            "Training Epoch Loss after 89 is 0.4514218270778656\n",
            "{'Validation Epoch Loss': 0.4500305950641632, 'Epoch': 89, 'Micro F1': 0.7889174354782202, 'Patience Count': 15, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8691388113382111, 'Macro ROC value': 0.8691388113382111}\n",
            "Training Epoch Loss after 90 is 0.4515385925769806\n",
            "{'Validation Epoch Loss': 0.44822660088539124, 'Epoch': 90, 'Micro F1': 0.7896473198645334, 'Patience Count': 16, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8701946615979771, 'Macro ROC value': 0.8701946615979771}\n",
            "Training Epoch Loss after 91 is 0.4505015015602112\n",
            "{'Validation Epoch Loss': 0.44962048530578613, 'Epoch': 91, 'Micro F1': 0.7893346179673586, 'Patience Count': 17, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8695015561846926, 'Macro ROC value': 0.8695015561846926}\n",
            "Training Epoch Loss after 92 is 0.45144814252853394\n",
            "{'Validation Epoch Loss': 0.45055150985717773, 'Epoch': 92, 'Micro F1': 0.788368901993986, 'Patience Count': 18, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8688466882079626, 'Macro ROC value': 0.8688466882079626}\n",
            "Training Epoch Loss after 93 is 0.45035097002983093\n",
            "{'Validation Epoch Loss': 0.44746163487434387, 'Epoch': 93, 'Micro F1': 0.7905292967039383, 'Patience Count': 19, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8708129831911235, 'Macro ROC value': 0.8708129831911235}\n",
            "Training Epoch Loss after 94 is 0.4490390121936798\n",
            "{'Validation Epoch Loss': 0.4483114778995514, 'Epoch': 94, 'Micro F1': 0.7903687268268474, 'Patience Count': 20, 'Best Val F1': 0.7916240913204682, 'Best Val Loss': 0.44716644287109375, 'Micro ROC value': 0.8702801616233278, 'Macro ROC value': 0.8702801616233278}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.7956323718323017, 'Micro Recall': 0.7956323718323017, 'Micro Precision': 0.7956323718323017, 'Micro ROC_AUC_Score': 0.8774217431991005, 'Macro ROC_AUC_Score': 0.8774217431991005}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Average Statistics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyK8dHezw_cj",
        "outputId": "e820422c-8e19-4598-838a-137557a15007"
      },
      "source": [
        "average[\"avg_test_micro_f1\"] = average[\"avg_test_micro_f1\"]/5\n",
        "average[\"avg_test_loss\"] = average[\"avg_test_loss\"]/5\n",
        "average[\"avg_test_micro_recall\"] = average[\"avg_test_micro_recall\"]/5\n",
        "average[\"avg_test_micro_precision\"] = average[\"avg_test_micro_precision\"]/5\n",
        "average[\"avg_test_micro_roc_auc_score\"] = average[\"avg_test_micro_roc_auc_score\"]/5\n",
        "\n",
        "\n",
        "print(average)\n",
        "time_taken = (end - begin)\n",
        "print(f\"Time taken is :{time_taken}\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'avg_test_micro_f1': 0.7828995562782974, 'avg_test_loss': 0.46308518052101133, 'avg_test_micro_recall': 0.7828995562782974, 'avg_test_micro_precision': 0.0, 'avg_test_micro_roc_auc_score': 0.8609123364720531}\n",
            "Time taken is :363.927077293396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "OxpcKAgdDCnW",
        "outputId": "ee9002c6-13f4-4766-d2f1-43f921276329"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "filepath = config['path']+\"/brightkight_1_64\" #\"numlayers_heads_emdedsize\"\n",
        "\n",
        "plt.title(\"Validation accuracy vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_acc_fin[i]))\n",
        "  y_axis= val_acc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_acc_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Validation f1 score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_f1_fin[i]))\n",
        "  y_axis= val_f1_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_f1_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.title(\"Validation roc_auc score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_roc_auc_fin[i]))\n",
        "  y_axis= val_roc_auc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_roc_auc_vs_epoch.png\" )\n",
        "plt.show()\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "xZlS2RXBDJnj",
        "outputId": "be8f0db4-33c9-4561-941f-c18203e273f7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Training accuracy vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_acc_fin[i]))\n",
        "  y_axis= train_acc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_acc_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Training f1 score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_f1_fin[i]))\n",
        "  y_axis= train_f1_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_f1_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.title(\"Training roc_auc score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_roc_auc_fin[i]))\n",
        "  y_axis= train_roc_auc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_roc_auc_vs_epoch.png\" )\n",
        "plt.show()\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXyddZn//9d19uxLkzRpkjYpbekGtLS0gOy7iIDiMEVUHHFwZkSHUWcGv84PGBwddMZtlBlFxQ2l7lhZyiICshSa0gLd9zZJ0ybNvp71+v1x30lP07RN26RJz7mej0ceOeder/tO+z6f87k3UVWMMcakLs9YF2CMMWZ0WdAbY0yKs6A3xpgUZ0FvjDEpzoLeGGNSnAW9McakOAt6c0JE5CkRuW2kpz2OOv5eRPaJSJeITBiNdZgjE5FLRKRurOswhxI7jz79iEhX0ttMIAzE3fefUNWfn/yqjp+I+IEO4FxVfcsd9kXgRmAW8B+qet/YVZgeROQS4BFVrRjrWszBfGNdgDn5VDW7/7WI7AQ+rqrPDZ5ORHyqGjuZtR2niUAIWJc0bCvwL8DfjUlFSU6h/WhSlHXdmAH9X71F5F9FZC/wIxEpEJHHRaRJRFrd1xVJ87wgIh93X39URF4Wkf92p90hIu8+zmmrReQlEekUkedE5EEReWSImmcAm9y3bSLyPICq/kRVnwI6h7Hdi0SkRkQ63O6fryeNu0BEXhWRNhGpFZGPusPzROSn7n7ZJSL/JiKepG17RUS+ISLNwH0iEnS3dbe7ju+KSMYQtQTddc1NGlYsIr0iUiIiRe7foE1EWkTkL/3rHWJZM0XkWXe6TSJyc9K4H7s1POvu4xdFZErS+PNFZKWItLu/z08aVygiPxKRPe7f7rFB6/2siDSKSIOI/M3R9r8ZfRb0ZrBSoBCYAtyB82/kR+77yUAv8J0jzL8YJ3iLgK8CPxQROY5pfwG8AUwA7gM+PNQCVHUzMMd9m6+qlx11Cw/1LeBbqpoLnAb8CsANvqeAbwPFwDxgjTvPt4E8YCpwMfARIDnUFgPbcb5tfAl4AJjhLmMaUA7cM8T2hIHfAbckDb4ZeFFVG4HPAnVuPROB/wcc0v8qIlnAszj7sQRYAvyviMxOmuxW4Is4+38N8HN33kLgCeB/cPb/14Enko59/Ayny2+Ou+xvJC2z1N0v5cDtwIMiUjC4PnOSqar9pPEPsBO4wn19CRABQkeYfh7QmvT+BZyuH4CPAluTxmXihFDpsUyL84ESAzKTxj+C0/87VE1V7ry+IcY9Atx3lH3wEvDvQNGg4Z8Hfj/E9F53P81OGvYJ4IWkbdudNE6AbuC0pGHnATsOU88VwLak968AH3Ff3w/8AZh2lG36a+Avg4Z9D7jXff1jYGnSuGyc4zSVOB+qbwya9zV3u8qABFAwxDovwWkI+JKGNeIcOxnzf+vp/GMtejNYk6r29b8RkUwR+Z7bPdGBE4r5IuI9zPx7+1+oao/7MvsYp50EtCQNA6g9xu04FrfjtLY3ut0U17nDK4FtQ0xfBPiBXUnDduG0Yvsl11uM80G2yu1yaQOWu8OH8mcgU0QWi0gVzofr791x/4Vz/OEZEdkuIncfZhlTgMX963PXeSvOB+khNapqF9CCs+8nDdq25O2rxPnbtB5mvc168PGIHg7/9zcniR2MNYMN7gb4LHA6sFhV94rIPGA1Tit1tDQAhSKSmRT2laO1MlXdAtzi9nW/H/iN201RCywaYpb9QBQnTNe7wyYD9cmLHTR9LzBHVZOnOVw9cRH5FU73zT7gcVXtdMd14vxNPuv24z8vIitV9U+DFlOL091z5RFWNbBPRSQbp8tuj/szZdC0k3E+nGpx/jb5qtp2tG0x44O16M3R5OCEVJvbd3vvaK9QVXcBNTgHMQMich7w3mNZhoj4RSSE82/cJyKhw30LEZEPiUixqiaA/vBK4PRZXyEiN4uIT0QmiMg8VY3j9ON/SURy3L78z+B0Ew21PQng+8A3RKTEXWe5iFx9hE34BU73y63u6/5arxORae6xjHac7pbEEPM/DswQkQ+7+8IvIueIyKykaa51DzYHcPrqV6hqLfCkO+8H3e3+a2A2zgdOA85xi/8V50C9X0QuOsJ2mHHAgt4czTeBDJxW6QqcVt3JcCtOP3Yz8B/AL3HO9x+u7+N8QN0CfMF9PeQBXeAaYJ041xd8C1iiqr2quhu4FqcF3YJzwPIsd55P4fS7bwdexgnjh49Qz7/idLmscLvAnsP5pjQkVX3dXf4knGDtN92dtwun3/x/VfXPQ8zfCVyFcxB2D0432VeAYNJkv8D54G4BFgAfcudtBq5zt7sZ5zTV61R1vzvfh3G+0WzE6YO/6wjbbcYBu2DKnBJE5JfARlUd9W8U6UBEfgzUqeq/jXUtZvRZi96MS243w2ki4hGRa4AbgMeONp8x5lB2MNaMV6U455NPwDlv/O9VdfXYlmTMqcm6bowxJsVZ140xxqS4cdd1U1RUpFVVVWNdhjHGnFJWrVq1X1WHvAhv3AV9VVUVNTU1Y12GMcacUkRk8NXMA6zrxhhjUpwFvTHGpDgLemOMSXEW9MYYk+Is6I0xJsVZ0BtjTIqzoDfGmBRnQW9G1wsvwHPPjXUVxqS1cXfBlEkBPT3w4ovw3e/CsmWQkwOtreAd9NyP++6DNWvgsdG5KWUsHGPrU1vZu2YvTeua2PuW8+TCae+exvyPzaf0rNKjLOFg/feFGvysc00ota/VEsgOMPHMiYeMH0o8Eacr0oVHPGQHshERVJVwPEzIFzrsfNF4lI5wB1mBLPZ27WVn206q86uZnDd5YL0NnQ2s2buG3e27WVS+iHml82joaqCxu5GEJpheOJ2cYM4R6wvHwuzv2U9BRgEhX4i+WB8ZvoyBdXRFusjwZdAT7WFb6zZKs0spzT50f6oqezr3sHH/xoH1LypfxLTCaQPLauxuZG3jWlp6W7ik6hKKMouOuv/69cX6aOhsYEvLFkK+EOdWnAvApv2beKfxHboj3dxyxi1k+bNY27iW7mg3xZnFFGUWkRvMHdbfaihdkS46wh2UZJWQ0AR9sT5yg7kAxBIxIvEImf7MYS1rR+sONjdv5tyKc8kL5R1XPUcz7m5qtnDhQrUrY0dJPO78jkahrQ0mTAC/Hzo64PXXYc4c8PmcFnhdHUQiMHMmVFXBa685P2+/DSUlcNllsHWr8z4rC4qLoboaNm6EP/0JwmEn4C+91An7Vavg7LMBSGiCl9Y+wc/uex9bc+OUXfV+PP4gfc19VE+pZlHFIspXlNP4diMbrt/AuvZ1NHbtI6ZxcjSHxV2LqWypxNvkZcfuHezYt4NWTyst2S10nt/J6eWns2DlAiK/jxBtjoJArDRGc1kzsb4YpZtL8ca8NCxqoPvCbrLLsileXYzH46H5umZKtISSF0poC7SxJ7KH4HNBfA0+2nPaaapsQuYKFZsqyNqYhRYrif0JAs0BZ9fmRvGGvPhiPmb+cSbvhN/hoVUP4ff6WVy+GJ/Hx96uvaxqWEVXpAuAwoxCZhfPZtP+TbT2tfKxeR/j0upLWVG3gnVN69jZtpOJWRPJ9Gfyau2rdEe7D/nTFoQKOLvsbLoiXbxe//pB44LeIOH4gWe2BLwBLq++nHMmnUNeKI+Xd7/M7vbd+L1+wrEwLb0t1HbUktCDH1xVnlPOBZMvYFXDKra2bD2khqkFU8nyZxHwBphWOI2+WB+v1L7C/p79h0ybHchmct5kmrqbaOppGhjuEQ+nFZxGyBci5AsR9AVJaIJ4Ik5c4wMftu3hdhq7G+kIdxy03Ex/JtF4lGgiOjCsJKuEsuwy3tr31kHT+jw+ijKLBn66Il3s69pHX6yPaCJKNB4lL5THnOI5VOZWkhvMpSPcwZaWLbxW9xqxROyg5S2ctJC5JXP546Y/0tzbTE4gh9LsUspyypg3cR65wVx+u+G3NHY3Mq1wGpn+TJp7m3l739sAeMXLDTNv4Lc3//aQ/TUcIrJKVRcOOc6CPgWFw9DbC/n5zvuWFvjXf4WHH4ZE0n/ewkInsJ99FtrbSQh0BEEUcsMHHgrb4ffwjfMT7JqUyccT85i0Yz/rWjfjz8wlULKQSm83LR17ecFTz67SXNqmFRMsqyZQVkFbXyO7XlzGOxVBzmpayOK1Z/DnhStYnbuGnF7honcqyepaQOX6qeTsz2HD3A2snbmWD/zmAwjC7srd9BY1M3XdLLxRP6IexK0s7onTk9mDzy8EIyE83c4X1IQnjiQ8bJm+hdcXv86uKbvIycnhjIlnMClnEvGOOIWPFVKyvARvxPmWkZAEokJfZh/+sB9v3Duwno6yDnpn9lLYVYh3vRdPt4dwZpid03eS0ZWBhATflT6IQnh1mI7eDsLeMH+6/E+EM8K8Z8Z7yA5kU7OnBo94KMwoZEHZAqYWTCWeiLO5eTPrmtYxY8IM/B4/P337p0TiETJ8GcwtmUtVfhWN3Y209bVxfuX5nD7h9IGW6ZT8KWxr2cbqvat5s+FNPOLhxpk3cuHkC5mUM4mXdr3EW/veYlrhNMpzylGUV3a/whNbnmBLyxYSmqAqv4qZRTOJJ+KEfCHyQnlMzZ9KWU4ZbX1thGNhAt4ANQ01vFr7KvNL53PB5AuIxqMEfUFOKziNXe27eKP+DaKJKD3RHrY0b8Hr8XLB5AtYWLaQ2cWzKc0uJa5xXq19lXWN69jdsZsJGROYWzKXM0rOINOfyfKty9nYvJFwLEw4HiYcC+MRD16PF694B7755IXymJg1kZKsEkqzS5lWOI3W3lae3/E8mf5Mzpx4JmdMPIOOcAdffOmLtPa28pGzPkJVfhX7e/azv2c/Td1Nzute532WP4vS7FIyfBn4vX78Hj/Nvc2sbVzL3q69tIfbyQvmUZ5bzqVVlzIlbwr7uvfh8zj/7v6w6Q9s3L+R9854L2eUnEFjdyN7u/eyu303qxtW0xfr4+Kqi5lROINtrdsGWv2XV1/OWaVn8eLOF/F7/dx3yX3H9d/egj5F1HfUU5ZThkc8tNW8zNZX/sjcxe8ltGCx0zLftcvpLvn+952ukksuIRzy8dbWV5i9uxf/R/6GL07awnLZip8gvu4u/C1t+PML6SsuZnXvFjxtIRa9sYjM7AymXTkV7y8gY00eO6fspHZGLe2BdrqynZboZc9fxsTGibx63qtsn7qd9/7xveR1HPzVM5IRIeHtJRDJwBNxWr1xX5TiSavRfbNpDWfj8UL15adRWJ3LyodWO4/VPh30/AZ8PypCRJh9eoJcbxfe1v1kF7TRWryPDn8jc9buZWaD03prJ5e3mEeYAEWlG+hdPBGtrGDqX32C0gvffcjX9GhvlD0r99C+u52qK6por2vn5ftfJlAcIO+OPKqLq8kN51I0s2hg3ng0TtO6JopmFuELDd3zmdAEezr30NTdRGFGIVPyBz9n+8j2dO5hT+cezpp4Fn6v/5jmPRa90V7aw+1DdrmYkRVLxOiKdJEfyh+1dVjQj3MJTdAR7iA7kI3P4yOhCZq6myjJKkFE2Pjsn/nq6q/xo94nuKTiYj6e+z4+t/Iz7M1KkNOZyaV1CT7RUcHrfVt5bAb0JCoori9ncmM+raEorbmdVNdVMamxnObMZgr9heTuyEUSQl9BH/4uP/5eP/H8ON4up4WrcUVUiPqjtLyrlerdVfRs7zmo7lB5iIz5GbQ+3gpA7vR8zvzg2XgSMcTnpb1d2PpWJ/LOO0zvWMXkuTlUbX6W3/ZcS22sjIlVIc6PvkCgahL7/v7fif/Nx2kKVbA/VMl1NwWZ8rP/oKP6LPxP/oGuUBETJoDH43xhqa2FadNwuqE2bIBt22DvXggGobISLrrIeW1MmjjhoHcf5fYtwAv8QFUfGDR+MvATIN+d5m5VfdId93ngdpyn1X9aVZ8+0rpSIehVlVdrXyU3mMvE7Ik89MaPeWttFN8bn2NKpY85Nz7BaWVFhHwhvvCnL/DMlmdIeBIUN5bz3nXvZg8N7Jq4nUAoxOx3pjJj9Vzi3jjbyvdT1BVgQksh+ybt4rRpFXS+7CHqi/Dm/NX4o35m7JpJTnMWADFPAo8KHhWi2XFapu5nKlPxxfJY31lJX9TLxFAHTd0Z1LdkUprdSTA3SH3FYkpKoDK6ib/UT+eltwrIyIDZ0yJcf1Ufc6Z04Y/2sC80mbfWB2j/y9t07Wrm8fYLiOEnEHC695NNZzPnsJI9lefimTSRouZNZJ4/n61Pb+XlfTOG3I9l3n3MPi+fnQ1Btm1zeppOPx1Wr3ayfPPmY/7DAArinmyWiIJn9FrMxpxMJxT0IuIFNgNX4jzSbSVwi6quT5rmIWC1qv6fiMwGnlTVKvf1o8AinKfZPwfMUNX44dZ3qgR9LBHjdxt+xyVVl1CSVTIwXFX5l2f+haW/XkpvRi+9Gb2c+faZBMNB3pySIL85k8vfKSOoHgoDcaS+DG8kSHtejLw2LwlRvAkPHnW6ChKS4O0K0H3TOCuyg33efDaXdrFgT4BMCdMx/Wy69ncyqXkDYQlQr5VsYBZbmMZZF+SSEUyw7rX9nDfnRS46czU1tVfw25fOpbqik8vP2UR+vIar5/2ZGRPXsaejmu1N02nqrqChKZPmFi/b+67lzHfNgEgr4frXCHavJjPYjapw8awXmVu5li3757O9+zLacm5CfdkEe95mVuGfmJS9Bf/kq9i9NZ93nn6R2dM3MGvqVt7ccxmPvXUbnR1xFpW/xPun/xR/Z4zGtsuI3HQnsb5uene/QU97C9v3FBHKDDC9uoec6Bvk+7axP76AQNki5iw+DYm2QOvb0LUVemoh2gUZE6HqQxAshs4t0L3T+d32FkRaIVgEsV6IdULpVTD97yH3dIh1QdOrgEJmufNBEG2Hvkbw58PkD0DHBtj1K8ishJzpzjrFCxMvdaZtehla3oS+vTBhMeTPBQR66qF7uzNtYAIUvwuinbDjJxDvdZaXMQk8Pmh+A3obwBuCgrOh4kbnwynS5nwwBQoge6qz7qaXYcotkHPamPw/MOPHiQb9ecB9qnq1+/7zAKr6n0nTfA/Yrqpfcaf/mqqeP3haEXnaXdZrh1vfqRD0u9p28bf/+7f4/uBj+6Xb+exffZantz3N2sa15PXmU/1wFbM2zjp4JlGCoTDhviCJnASTShvwJZTiikbi4mX/rmKKy5s47eKt9NRlkbe1lVfyLqctkMmE0wq47Kw3KEs8h6dwBnt65kPz6xQFthEMRmjtnUhT3xkUFvaQHWwnIkX4A36yPA0QbkLDzYjGht4YgJwZUDAPundB1zYIDzpLIm+uE3Du57PiRYjTl7mAYOnZSNubTriR9G/JE4TMCmd54Hyf68iDee+BPU84oehMCBsTUITzk8yX7YTvQB2zIXsatNRA754Dw70ZkDMNMieDPwfa1kL72gPjgxMgayoUnAWhEmf7PCHwBmHnI06oHgtfNsS6D97eAeJ8AISKoWUVxPsOjMqY5PwO74eE+5Une6oT8j21Th2JCOSf5QyPdTtBHus8QjEC5/8cqm45tm0wKedIQT+c8+jLgdqk93XA4kHT3Ac8IyKfArKAK5LmXTFo3vIhCrwDuANg8uTJwyhpdHXt6+K5f32OyvMrWXDHAjZuhB/8AP7f3H8gJ/4rlv3+bM576lw8CS/l7ZO5I+MOzs7K4x82nkfnL88k1htid2k5t172c6KdAUoW5TNpchnP/FsThdPaueLWZ/Fk5fJ/f/x3PLE3mFLRzHu/8y58m9+A1U/AxQJXRpgp21BPAIn3QkY5lH8Y2tdS2fNrOP08KLwexMeEnlomdGwAXxb4TyMz3AyJHsg4DYrOQ0LFMPFymLAIGp6Gzs3gz4PsaqfFmDHx4B0QDzuBE22HrT+Axhdg9t1QeiUUzkf8uZCIE/IknRffswfq/wgo5M2BwoXgy4D2DdC3D3omQuEkyMtzWt3Nr4M/FzKnQNkMmD0L/vhtiHY4rdb8MyGQB/EIaMwZ1t/NouqEZdd2COQ74Z9ciyq0vePMlzPNWc/hnPkf0LwCeuqc5Red73xI9dY7LWp/nvNB0bUDan/ntPQn/5Wzj7p3QdZkJ5D3veDUW3yBU1P/fuzb69QTKnb+PuCE//7XQcSZvr8rSdX5MPUk/beM98H+FU4tgQJnfF+js+2hiVB8vjPcmCMYTov+A8A1qvpx9/2HgcWqemfSNJ9xl/U1t0X/Q2Au8D/AClV9xJ3uh8BTqvqbw61vrFv0u1/ZzS9v/CU9+3sI5gY595d38f4PhvDFG9n+jam8+NQCan5zGaeds5lfvvUhFkbewPvXXczYtZsNK2ZTNGU/7/+731E2eQ8R7yTqZjzD1Plz4J574ItfhEzgic+CLIbLb4ErroDHH3fOXwe3HxmnFefLdkIg3geewIFASDUvvgiTJsH06WNdiTGnrCO16IeTHPVAZdL7CndYstuBXwG43TIhnC/iw5l3XFnx9RWIV3jfI+8j3BHmvveuoqQE3vzOv7BtVTU1v7kM5vdx66cfZek9/0kgJ478LsiGFbPxXnw2H3n7W5R9eiXM/xqB615zQr6rC77zHbjwQgh7YbkPPvsVKCuDX/3qQMiD08oTcVqh/cHuDaVuyANcfLGFvDGjaDjpsRKYLiLVIhIAlgDLBk2zG7gcQERm4QR9kzvdEhEJikg1MB14Y6SKHw0NbzZQdUkVZ956JoFZU1kYW8Gvv7+V9poaHvvu+2mbuIv/9783IYXvJqv6L1zxgaeIRf1c9d9X8m8vvJecXA9kToJZn3H6Xh9/HG67zTmv/atfhauugq9/3blS9MtfhtwjdCsYY8wIOGofvarGRORO4GmcUycfVtV1InI/UKOqy4DPAt8XkX/COUL1UXX6hNaJyK+A9UAM+OSRzrgZa70tvbTtbGPh3zvfft7OeRcz+RmPXfYzEvEPsLeylktDP8e///0gfw2NT3JObDMz6+4ip3yIe1T86Edw++3OLQI+/Wk491z4yEfgqadgwQK49daTvIXGmHQ0rJuauefEPzlo2D1Jr9cD7zrMvF8CvnQCNZ40Daudsy/Kzi6jsxN+u76AW2/5M7d2+XinVfnxBW/z7a9GnPvAqMI/AXSQ86nNUH6Os5Crr4YZM+Db34Yf/hBmz3ZO/A44V4Vyww2wZAl87nPO1T/GGDPK7O6VSRpWOUFfOr+UJ5ZDfMozfPbKF8n0wHW74Avn3I3vv/7rwA2//H7n3jGPPQbnnAP19fDMM85Nva69Fl59Fb7ylQMhD5CRAY8+OkZbaIxJR9akTNLwZgN5U/LInJDJ738Pl130E+YGYeefvZzTW8IdF97lnB1SVwc7dzp3a7zoogO32X3qqQMLu/lmp8X+oQ+NybYYY0w/C/okDW820FPdw5nfvJClv45yw+yXiKhw2dI4f7npCSZmT4SKCifod+xwbt/7vvfB+vVQUwNPPulcm/+pTzln2lxxhfPBYIwxY8iCHnjnF++w6y+7aNnSwuaCbbzT/jK57/8H3pPTzd46H5x5Dix0T0/tD/qdO52g/8hHoKjI6XN/9ll497udWwJPneocgDXGmDGW9n30LVtb+N2tvxt4/2ZoAwBT5v2Aaj+0vhiFf/7nAzNUVMAf/uD00VdVOVd63nPPgVC/9looLXXupmiMMeNA2rfo972zD4D5H59P9TXVbJ7yJgX17+HGbEgoFPx6D/zVXx2YoaLiwK0Zq6ud35/4hHPPXL8fLr/8JG+BMcYcWdoHfePaRgCu+eY1zPzBTMKhMBfvXcQ/BaAlYzpklR08Q0XFgddVVc7vQMC5wvWRRyA7++QUbowxw5T2XTdNa5somFpAICvAmnXrmeCBBy99kPwMD1z4o0NnGCroAebPd36MMWacSfugb1zbSMlc537yK7at594CDxNz9kPb7c49wwerdG/dEwrBxImHjjfGmHEmrbtuYuEYzZubKZ5bDMDbDes5xxeke3MWXP6PQ89UWuqcH19V5dx8zBhjxrm0btE3b24mEUtQsvyn8O4gu7rWcUZRBG9btnPrgqH4/U7YJ3fbGGPMOJbWQd9/ILbkzeX03LWGgvftJMsHzL38yK31Bx6A8kOen2KMMeNS2ge9xysUxZt5u76RM0PuiPf83ZFn/PCHR702Y4wZKWndR9+0tokJ5UG8xFldFeSsAMQTHigf/KREY4w5daV10O9ds5fiUi//xed49PIFnOkN0hye7jzRyRhjUkTaBn1HfQftu9vxZAb5F3mA56IbOcvvozfDzoU3xqSWtA36utfqAGiM5EPZm+Rmt1CV1Y236KwxrswYY0ZW2gZ97au1+EI+3m4pJzhtGfOCzvBJs88c28KMMWaEDSvoReQaEdkkIltF5O4hxn9DRNa4P5tFpC1pXDxp3OCHio+Z2ldrmXTOJFbunUzB6X/gu2UZ4M/DU2wHYo0xqeWop1eKiBd4ELgSqANWisgy9zmxAKjqPyVN/ykguaO7V1XnjVzJJy7WF6PhzQYW/eN5vPX6BH4wfy2zvArn/wqCE8a6PGOMGVHDadEvAraq6nZVjQBLgRuOMP0twLh+KOqeVXtIRBPESiupnP4St+Upm0puhPLrxro0Y4wZccMJ+nKgNul9nTvsECIyBagGnk8aHBKRGhFZISI3Hma+O9xpapqamoZZ+vGrfdXZnF3xChbOckr1TFky6us1xpixMNIHY5cAv1HVeNKwKaq6EPgg8E0ROW3wTKr6kKouVNWFxcXFI1zSoRpeeIn80l7e3JTBourXiShUVF496us1xpixMJygrwcqk95XuMOGsoRB3TaqWu/+3g68wMH992OiaUM7JWW72b5mC4vKNrMp7CUjlD/WZRljzKgYTtCvBKaLSLWIBHDC/JCzZ0RkJlAAvJY0rEBEgu7rIuBdwPrB855M8Wic/bVBiiuamFn4J84obKM2ljOWJRljzKg66lk3qhoTkTuBpwEv8LCqrhOR+4EaVe0P/SXAUlXVpNlnAd8TkQTOh8oDyWfrjIWWLc0kYh6Kyxv5zMzvUeBX2qN2J0pjTOoa1t0rVfVJ4MlBw+4Z9P6+IeZ7FTjjBOobcU1rtgKQO7GD6uKdAGj2uCrRGGNGVFpcGdvXB4sXw4svQu8vFWwAAB7MSURBVOOabSDKWz2XARBTyC+7cIwrNMaY0ZMWQV9fD2+8AU88AU3r9lFQ0sqanttRhXURmFZ17liXaIwxoyYtgr611fm9YQM0buymuLwRX8nZvN1RxG86oLp87tgWaIwxoygtgr6lxfm9cV2cll1KaEIfU6eF+NLOHJbu8uL3Bca2QGOMGUVpEfT9LfqOHc0k4kI008+MGbBFWpjekzG2xRljzChLi6Dvb9FPoBmAVm8B06fDLl831fHcMazMGGNGX1oEfX+LPs/j3D25hXK8GV20+mNUegvGsDJjjBl9aRP0wSCU5+wBIJJZTW27c2OzymDJWJZmjDGjblgXTJ3qWlqgqAhOC+3F3xdBi6ZR274bgMmZpWNcnTHGjK60adEXFipzitYQyIgQmDiP3fu3AVCZW3mUuY0x5tSWFkHf0gI3nP07AtEuWqMFVE/LoLZxC6JQXlg11uUZY8yoSougb2+L83fnfZ7O7gnUd09i7lyobdlJWSf4C4vGujxjjBlVaRH02eygPGcLkXgZ51+eycyZsLujlsntQIGddWOMSW3pEfSeOgD6OoSyqc4FUrU9DVR2AIWFY1iZMcaMvpQP+kgEirJqUYWe1hgZb76ChsPURvZTaS16Y0waSPmgb22FisI6ouEAiaiSseovNK98kV6NOF031qI3xqS4lA/6lhaoLKylrcs5Xz6DXmp3rwWgslMgxx4jaIxJbSkf9P0t+s7eKYAT9Lv3bQJgciIHPCm/C4wxaS7lU66lBSon1NIddp4Lm0kvta27AKj0WbeNMSb1DSvoReQaEdkkIltF5O4hxn9DRNa4P5tFpC1p3G0issX9uW0kiz+SFSvg298+0KIPRycCbou+ew+BhFCcYefQG2NS31GDXkS8wIPAu4HZwC0iMjt5GlX9J1Wdp6rzgG8Dv3PnLQTuBRYDi4B7ReSknObyk5/AXXfBru29FOfuJxqdADhBv0rrmd2ZgafAWvTGmNQ3nBb9ImCrqm5X1QiwFLjhCNPfAjzqvr4aeFZVW1S1FXgWuOZECh6uaBQSCXjluXoAYlHnvvNeTy+vZbVycUPAzrgxxqSF4QR9OVCb9L7OHXYIEZkCVAPPH8u8InKHiNSISE1TU9Nw6j6qWMz53dfirL6vJwu/H1aXx+j1KRdvi9k59MaYtDDSB2OXAL9R1fixzKSqD6nqQlVdWFxcPCKF9Ad9RaFzVWxvV4CMDHixyhl+0fpuC3pjTFoYTtDXA8n38q1whw1lCQe6bY513hEVjbor7A/6Di8ZIeXFKXDGPpjQrdZ1Y4xJC8MJ+pXAdBGpFpEATpgvGzyRiMwECoDXkgY/DVwlIgXuQdir3GGjLhaD3FznYqmOvkJ6WyKEQglemQwX73Qnsha9MSYNHPUJU6oaE5E7cQLaCzysqutE5H6gRlX7Q38JsFRVNWneFhH5Is6HBcD9qtoyspswtFgMpk6FWVPqaItW0tvSiyfUQ3cALt7lTmQtemNMGhjWowRV9UngyUHD7hn0/r7DzPsw8PBx1nfcolHw++HCBbWQWUFvSy/ekh4A5ja6E1mL3hiTBlL2ythYDHw+xde3HW9OFb0tvWgwDEAo4W62Bb0xJg2kdNAXZLVCtIOop5p4JD4Q9IEi5ypZ67oxxqSDlA36aBQmF24HoDcxGYCEvw+AQLFzJ0tr0Rtj0kHKBn0sBpX52wDoi5YBEA+6QT9xktOBn5k5ZvUZY8zJMqyDsaeiWAwm5Tkt+nDUuXlZvL9FP/sM2LgDRMasPmOMOVlStkUfjcKknO0Qmkhfl7OZ0YAb9P/fffD662NYnTHGnDwp3aIvy9kO2VPpa3QCPurvwZcATyAIgeAYV2iMMSdHyrboYzEozd7mBH2bE/Rhfy+BRMpusjHGDCllU080woSM2oOCPhLoJaDWL2+MSS8pG/TFWbvxSAKyT6OvrQ9/pp+IJ0pAU3aTjTFmSCmbeuXuGTdkT6WvvY9QfogIMQt6Y0zaSdnU6z+HnuyphNvCbtDHLeiNMWknZVOvsmA70UQQMsroa+uzoDfGpK2UTb2sQAe9sQIQD31tfQTzgoQlQRDvWJdmjDEnVcoGvc8TJY4fYFCL3oLeGJNeUjjoIyQGB70kCFiL3hiTZlIy6FXB64mS0ACqeuCsG0kQkJS9GNgYY4aUkkGfSIDfGyWBn1hvjEQ0YS16Y0zaGlbQi8g1IrJJRLaKyN2HmeZmEVkvIutE5BdJw+Missb9OeSh4qMhGj0Q9P1XxYbyQ0Q8ai16Y0zaOWrqiYgXeBC4EqgDVorIMlVdnzTNdODzwLtUtVVESpIW0auq80a47iOKxSDgi6ByaNAHLeiNMWlmOC36RcBWVd2uqhFgKXDDoGn+FnhQVVsBVLWRMRSL9bfoAwNBH8wLEvZYH70xJv0MJ+jLgdqk93XusGQzgBki8oqIrBCRa5LGhUSkxh1+4wnWOyz9XTdDtegD4j8ZJRhjzLgxUs1bHzAduASoAF4SkTNUtQ2Yoqr1IjIVeF5E3lHVbckzi8gdwB0AkydPPuFiYjHw+6IgWfS1Jwc9BDzWojfGpJfhtOjrgcqk9xXusGR1wDJVjarqDmAzTvCjqvXu7+3AC8D8wStQ1YdUdaGqLiwuLj7mjRgsFoOAd1AffV6QiBcCHmvRG2PSy3CCfiUwXUSqRSQALAEGnz3zGE5rHhEpwunK2S4iBSISTBr+LmA9o6y/RX9Q0Gf5LOiNMWnpqP0YqhoTkTuBpwEv8LCqrhOR+4EaVV3mjrtKRNYDceCfVbVZRM4HviciCZwPlQeSz9YZLf199Hicg7G+kA+fDyJeCHoCo716Y4wZV4bVYa2qTwJPDhp2T9JrBT7j/iRP8ypwxomXeWxiMQgmHYwN5YfQSISIz1r0xpj0k5JXxvafXonHT7gtTDAvSDTcA0DAay16Y0x6Scmgj0adC6YQ/4H73ER6AQj4LOiNMeklJYN+oEXvDRy4c2W4G4CA9dEbY9JM6ga9L4p4/IQ7wgRzgkQiztk3AV9wjKszxpiTKyWD/sBZN37ikTi+kI+I20cf9FrQG2PSS0oGff9NzcTrBL0n4CHcfzDWWvTGmDSTmkEfTeD1JAaC3hvwEom6B2P9FvTGmPSSkkEfj0YBEG/gQND3n3VjXTfGmDSTmkEfc4Lek9yi7z8Yay16Y0yaScmgT8QiAIO6bvqDPjSWpRljzEmXkkGvbotePD4S0cRBQR/0Z4xlacYYc9KlZND3d90ozsVR3oCX8MDBWGvRG2PSS0oGfSLuBD3q3LPNG/ASiYUBC3pjTPpJyaDv77o5KOj7++gD1nVjjEkvKRn0/QdjNTno484wC3pjTLpJyaDXRH+L3gv0d93YWTfGmPSUmkHv9tEf1KJ3W/nBYNaY1WWMMWMhJYM+ETu0RR/uPxgbzByrsowxZkykZNCTiLi/k7pu4m7QWx+9MSbNDCvoReQaEdkkIltF5O7DTHOziKwXkXUi8ouk4beJyBb357aRKvwQ8Tjs2gXt7eD20b+zdwMw6GCsteiNMWnmqEEvIl7gQeDdwGzgFhGZPWia6cDngXep6hzgLnd4IXAvsBhYBNwrIgUjugX99u+Hqir4+c8H+ui/X/NjoD/onWG+oLXojTHpZTgt+kXAVlXdrqoRYClww6Bp/hZ4UFVbAVS10R1+NfCsqra4454FrhmZ0gcJuWfT9PUNnHXT0+e04r0BL5FEhEAMxO8fldUbY8x4NZygLwdqk97XucOSzQBmiMgrIrJCRK45hnkRkTtEpEZEapqamoZffbL+oA+HEXUCPuIek+1v0QfjgAW9MSbNjNTBWB8wHbgEuAX4vojkD3dmVX1IVReq6sLi4uLjqyDgPvS7rw/cbppELOmsm0SEQBzw+Y5v+cYYc4oaTtDXA5VJ7yvcYcnqgGWqGlXVHcBmnOAfzrwjQ8Rp1ff1gbrn0ScFfSQRdYLe6x2V1RtjzHg1nKBfCUwXkWoRCQBLgGWDpnkMpzWPiBThdOVsB54GrhKRAvcg7FXusNExKOiTW/QRjTlBLzJqqzfGmPHoqP0YqhoTkTtxAtoLPKyq60TkfqBGVZdxINDXA3Hgn1W1GUBEvojzYQFwv6q2jMaGAANBL27QEx/Uok9YyBtj0s+wOqxV9UngyUHD7kl6rcBn3J/B8z4MPHxiZQ7TQNC7NzWLD2rRW9AbY9JQal0ZO7hFP6jrJmhBb4xJQ6l1CkoweFDQ6+A++kRqfa4ZY0ZWNBqlrq6Ovr6+sS7lsEKhEBUVFfiP4VTx1Ar6QS16Seq6CRMjoNaiN8YcXl1dHTk5OVRVVSHj8MQNVaW5uZm6ujqqq6uHPV9qNXHdoPcQIabg7Q96v5eIxgloam2uMWZk9fX1MWHChHEZ8gAiwoQJE475G0dqJd9A0EeJ9ge9F8QjRLCgN8Yc3XgN+X7HU19qJV8o5NwCITno3W4sJ+jtYiljTPpJvaB3u24iOEGvPgUgInGC1qI3xoxzy5cv5/TTT2fatGk88MADI7LM1Eq+/qCXvoEW/UDQkyCAteiNMeNXPB7nk5/8JE899RTr16/n0UcfZf369Se83JQ868Yj4YGgT3gTAIQlbkFvjBm+u+6CNWtGdpnz5sE3v3nY0W+88QbTpk1j6tSpACxZsoQ//OEPzJ49+7DzDEdqtug9B1r0CZ8T9BGxFr0xZnyrr6+nsvLAfSArKiqorz/x+0CmZIve6wkP9NHHvXHADXpJrc01xoyiI7S8TzWp16IPhw/quhkIeo9a0BtjxrXy8nJqaw88q6muro7y8kOe1XTMUivog0EAfJ4DQR/zxACnRR9MsS8wxpjUcs4557BlyxZ27NhBJBJh6dKlXH/99Se83NRKPvdxgl436H0JHzFvjHgiTtyDteiNMeOaz+fjO9/5DldffTXxeJyPfexjzJkz58SXOwK1jR8DQR8lCoQ0RNQTJRJ3bltsQW+MGe+uvfZarr322hFdZmp13bhB7/NEiCgENEBUkoLeYw8GN8akn5QMer83QlQhkAgQ8UToizk3ALKgN8ako5QMep/XudeNP+EnKlH2dO4BYCJZY1mdMcaMidQMek+MKM7B2Lg3zubmzQBUkDuGxRljzNgYVtCLyDUisklEtorI3UOM/6iINInIGvfn40nj4knDl41k8YcY1KLvP49+U/MmACo8+aO6emOMGY+OehqKiHiBB4ErgTpgpYgsU9XBd9r5pareOcQielV13omXOgxu0Ac8MSIKnrhnoEXvSUCpx1r0xpj0M5wW/SJgq6puV9UIsBS4YXTLOk6hEAkEvzdGVMET8wy06Eu7wecPjnWFxhhzRB/72McoKSlh7ty5I7bM4QR9OVCb9L7OHTbYTSLytoj8RkQqk4aHRKRGRFaIyI1DrUBE7nCnqWlqahp+9YOFQsTwOUEPEGOgRV/RIXAMD9M1xpix8NGPfpTly5eP6DJH6gqiPwKPqmpYRD4B/AS4zB03RVXrRWQq8LyIvKOq25JnVtWHgIcAFi5cqMddRX/Qe+JEFYg6Qd8R7qCiHSi1C6aMMcMzBncpBuCiiy5i586dI7re4bTo64HkFnqFO2yAqjarath9+wNgQdK4evf3duAFYP4J1HtkwSBR/ANBr1EduKlZeSfWojfGpKXhNHFXAtNFpBon4JcAH0yeQETKVLXBfXs9sMEdXgD0uC39IuBdwFdHqvhDuC36LG+cWNyHxg8EfUUH4LMWvTFmeFLoLsVHD3pVjYnIncDTgBd4WFXXicj9QI2qLgM+LSLXAzGgBfioO/ss4HsiksD59vDAEGfrjJz+rhtJEI9nABwc9NaiN8akoWE1cVX1SeDJQcPuSXr9eeDzQ8z3KnDGCdY4fMEgMfHi9SjxmHOGzUDXjQW9MSZNpdaVsV4v0WAmAIl4ALAWvTHm1HLLLbdw3nnnsWnTJioqKvjhD394wstMuU7reKbTZZOIOxdPqd85iae8E5g0aazKMsaYYXn00UdHfJmp1aIH4hluwMecFr0v4GOCN4dQDKiuHsPKjDFmbKRU0MfCMSIB5w6VGncfKxj0UZHIdiawoDfGpKGUCfqO+g6+FPoS2/ucU/77gz4QDFDR64cJEyDX7nVjjEk/KdNHn1WSBQI9OAdj+4P+toW3Mf3Zb0P18V9wa4wxp7KUadF7/V6yJ2bT4x6EJeH00Z9XfR7nvtMKU6eOYXXGGDN2UiboAXIrculzD8KScFr0Xi+wa5f1zxtj0lZKBX1OeQ69EedceXG7brwdrRCNWtAbY04JtbW1XHrppcyePZs5c+bwrW9964SXmTJ99OAEfV/Y2SSPOi17b9NeZ6R13RhjTgE+n4+vfe1rnH322XR2drJgwQKuvPJKZs+effzLHMH6xlxueS6xqJdoxIf0d93scx4Mbi16Y8yxuGv5XazZO7L3KZ5XOo9vXnPku6WVlZVRVlYGQE5ODrNmzaK+vv6Egj7lum4AOltyDwT93nrweGDy5LEszRhjjtnOnTtZvXo1ixcvPqHlpFyLHqCjJRdPwk8c8DbUQUUFBAJjW5wx5pRytJb3aOvq6uKmm27im9/8JrkneA1QSrboO1pz8DZ3AuD983PWP2+MOaVEo1Fuuukmbr31Vt7//vef8PJSKuj7W/SdLbkE9zYC4J06BT74wSPNZowx44aqcvvttzNr1iw+85nPjMgyU6rrJpgbRPxKR2sOvpY2ALx/egYKM8a4MmOMGZ5XXnmFn/3sZ5xxxhnMmzcPgC9/+ctce+21x73MlAp6AMlUOlty0b4OALwB7xhXZIwxw3fBBRegOrK3bEmprhsAyYzT2ZoDYSfgLeiNMeluWEEvIteIyCYR2Soidw8x/qMi0iQia9yfjyeNu01Etrg/t41k8UPWmhWlozUXAk5/vcefcp9lxhhzTI7adSMiXuBB4EqgDlgpIsuGeMj3L1X1zkHzFgL3AgsBBVa587aOSPWDJBLgy+6lc3sBhVNK8fg9iMhorMoYY04Zw2nuLgK2qup2VY0AS4Ebhrn8q4FnVbXFDfdngWuOr9Sja2qCzIIuNOEhGplo3TbGGMPwgr4cqE16X+cOG+wmEXlbRH4jIpXHMq+I3CEiNSJS09TUNMzSD9XQAIXV+xBvnI61AQt6Y4xh5A7G/hGoUtUzcVrtPzmWmVX1IVVdqKoLi4uLj7uIPXugtKqB87/8P0z6yCTmf2z+cS/LGGNSxXCCvh6oTHpf4Q4boKrNqhp23/4AWDDceUfSnj2QF+oiUtDO3P+Yy1X/fdVorcoYY0ZFX18fixYt4qyzzmLOnDnce++9J7zM4QT9SmC6iFSLSABYAixLnkBEypLeXg9scF8/DVwlIgUiUgBc5Q4bFXv2KHmZ7bQnIOQLjdZqjDFm1ASDQZ5//nneeust1qxZw/Lly1mxYsUJLfOoZ92oakxE7sQJaC/wsKquE5H7gRpVXQZ8WkSuB2JAC/BRd94WEfkizocFwP2q2nJCFR/G7vbdPBBbxCdyGunoFoqzjr8LyBhjWHUXtI7sbYopmAcLjnyzNBEhOzsbcO55E41GT/jswWFdGauqTwJPDhp2T9LrzwOfP8y8DwMPn0CNw1KRW0Fx63XkyU9YMv928nIrRnuVxhgzKuLxOAsWLGDr1q188pOftNsU9/OIh/KVDxI674eEsiuPPoMxxhzJUVreo8nr9bJmzRra2tp43/vex9q1a5k7d+5xLy+lLhvtbm13XvjzxrYQY4wZAfn5+Vx66aUsX778hJaTMkEfj0Nfp3MjMwIW9MaYU1NTUxNtbc7dd3t7e3n22WeZOXPmCS0zZbpumpogO9jfoj+xp7EYY8xYaWho4LbbbiMej5NIJLj55pu57rrrTmiZKRP0EybALx9ph11Y140x5pR15plnsnr16hFdZsp03fj9MG2y9dEbY8xgKRP0AETdPnrrujHGmAEpFvTWojfGmMFSK+gjbtDbWTfGGDMgtYI+2g7eDPD4x7oSY4wZN1Is6Dusf94YYwZJsaBvt/55Y8wpLx6PM3/+/BM+f76fBb0xxowz3/rWt5g1a9aILS9lLpgCnIOxdiDWGDMClt+1nL1r9o7oMkvnlXLNN4/82Oy6ujqeeOIJvvCFL/D1r399RNabWi36mPXRG2NObXfddRdf/epX8XhGLp5Tr0VvXTfGmBFwtJb3aHj88ccpKSlhwYIFvPDCCyO23NRq0VsfvTHmFPbKK6+wbNkyqqqqWLJkCc8//zwf+tCHTni5qRP0iTjEuizojTGnrP/8z/+krq6OnTt3snTpUi677DIeeeSRE15u6gR9rNP5bX30xhhzkGEFvYhcIyKbRGSriNx9hOluEhEVkYXu+yoR6RWRNe7Pd0eq8ENoAib/NeTNGbVVGGPMyXLJJZfw+OOPj8iyjnowVkS8wIPAlUAdsFJElqnq+kHT5QD/CLw+aBHbVHXeiFR7JMFCuGDpqK/GGGNONcNp0S8CtqrqdlWNAEuBG4aY7ovAV4C+EazPGGPMCRpO0JcDtUnv69xhA0TkbKBSVZ8YYv5qEVktIi+KyIVDrUBE7hCRGhGpaWpqGm7txhgz4lR1rEs4ouOp74QPxoqIB/g68NkhRjcAk1V1PvAZ4BcicsjRUlV9SFUXqurC4uLiEy3JGGOOSygUorm5edyGvarS3NxMKBQ6pvmGc8FUPVCZ9L7CHdYvB5gLvCAiAKXAMhG5XlVrgLBb4CoR2QbMAGqOqUpjjDkJKioqqKurYzz3LIRCISoqKo5pnuEE/UpguohU4wT8EuCD/SNVtR0o6n8vIi8An1PVGhEpBlpUNS4iU4HpwPZjqtAYY04Sv99PdXX1WJcx4o4a9KoaE5E7gacBL/Cwqq4TkfuBGlVddoTZLwLuF5EokAD+TlVbRqJwY4wxwyPjrS9q4cKFWlNjPTvGGHMsRGSVqi4calzqXBlrjDFmSOOuRS8iTcCuE1hEEbB/hMoZbadSrWD1jrZTqd5TqVZIj3qnqOqQpy2Ou6A/USJSc7ivL+PNqVQrWL2j7VSq91SqFaxe67oxxpgUZ0FvjDEpLhWD/qGxLuAYnEq1gtU72k6lek+lWiHN6025PnpjjDEHS8UWvTHGmCQW9MYYk+JSJuiH+xSssSIilSLyZxFZLyLrROQf3eH3iUh90lO4rh3rWvuJyE4Recetq8YdVigiz4rIFvd3wTio8/Sk/bdGRDpE5K7xtG9F5GERaRSRtUnDhtyX4vgf99/y2+5twMdDvf8lIhvdmn4vIvnu8JP3JLljq/ewf38R+by7fzeJyNXjoNZfJtW5U0TWuMNHZt+q6in/g3MPnm3AVCAAvAXMHuu6BtVYBpztvs4BNgOzgftwbgI35jUOUfNOoGjQsK8Cd7uv7wa+MtZ1DvFvYS8wZTztW5z7Pp0NrD3avgSuBZ4CBDgXeH2c1HsV4HNffyWp3qrk6cbR/h3y7+/+v3sLCALVbnZ4x7LWQeO/Btwzkvs2VVr0w30K1phR1QZVfdN93QlsYNADXE4RNwA/cV//BLhxDGsZyuU4j688kaurR5yqvgQMvqHf4fblDcBP1bECyBeRspNTqWOoelX1GVWNuW9X4NyyfFw4zP49nBuApaoaVtUdwFacDDkpjlSrOPd6vxl4dCTXmSpBf9SnYI0nIlIFzOfA83XvdL8OPzweukKSKPCMiKwSkTvcYRNVtcF9vReYODalHdYSDv5PMl73LRx+X54K/54/hvOto1+1HOVJcmNkqL//eN6/FwL7VHVL0rAT3repEvSnDBHJBn4L3KWqHcD/AacB83CeyPW1MSxvsAtU9Wzg3cAnReSi5JHqfLccN+fnikgAuB74tTtoPO/bg4y3fXkkIvIFIAb8/+2dsWoVQRSGv4OKRRBBSWEr6BNYpLBMYYIKahMRjOA7pMk72AmBIAghhY3ireMLCIZoIipqp8gNpLCxET0WczZs9O5NisvOOPwfXHY57IWff4azu2eGPesROlInuQz8N+Pf4jYHH1Qm4m0tif6wLlhFYGYnSEl+3d2fArj70N1/uftvYJUeXyEPw92/xnEXeEbSNmzKCHHczafwH+aATXcfQtneBl1eFjufzewecBW4EzcnogSyF+evSDXvi9lEBmPGv0h/zew4cBN40sQm5W0tiX6/C1Y81S0A4xqi9E7U3h4B79z9QSverr3eAHb+/m8OzGzKzE4156SFuB2Sr4tx2SLwPI/CkRx4GirV2xZdXg6Au7H7Zgb43irxZMPMrgBLwHV3/9GKT5vZsTgvppPcmPEfAAtmdtJS57wLwMu+9Y1gFnjv7l+awMS87WuluYeV7HnSTpbPwHJuPSP0XSa9mr8BtuI3D6wB2xEfAOdyaw2950k7E14DbxtPgbPAC+AjsAGcya01dE0Be8DpVqwYb0k3oG/AT1JN+H6Xl6TdNg9jLm8DlwrR+4lU227m70pceyvmyBawCVwrRG/n+APL4e8HYC631og/JnXha187EW/1CQQhhKicWko3QgghOlCiF0KIylGiF0KIylGiF0KIylGiF0KIylGiF0KIylGiF0KIyvkDvrqSsk4RZKUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}