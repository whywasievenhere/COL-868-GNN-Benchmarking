{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Link_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSk-NVszYi0D",
        "outputId": "9f311b16-1c53-47f5-d326-2fde0e25ab7e"
      },
      "source": [
        "!pip install --no-cache-dir torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install --no-cache-dir torch-geometric\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Collecting torch-scatter\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.8.0%2Bcu101/torch_scatter-2.0.6-cp37-cp37m-linux_x86_64.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 19.2MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.6\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Collecting torch-sparse\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.8.0%2Bcu101/torch_sparse-0.6.9-cp37-cp37m-linux_x86_64.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 29.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Collecting torch-cluster\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.8.0%2Bcu101/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 21.4MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
            "Collecting torch-spline-conv\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.8.0%2Bcu101/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (386kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 23.8MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/5c/3e95b76321fb14f24cc2ace392075717f645c4632e796ee0db1bc7d17231/torch_geometric-1.6.3.tar.gz (186kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.51.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/36/de17e79f29e06d9a92746d0dd9ec4636487ab03f6af10e78586aae533f7a/ase-3.21.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 62.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 59.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.3-cp37-none-any.whl size=322719 sha256=a2fd20567344ce698e8f00de3ef752ca28358a2332d86b9965eb21c13b35e3a1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gok3cnn7/wheels/6d/47/1e/0af8ce3e21783c3e584c22502011a3367c091694eebc50a971\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.21.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.3\n",
            "--2021-04-02 16:47:02--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.222.61.237, 3.212.138.198, 54.209.152.48, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.222.61.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14746350 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.06M  39.9MB/s    in 0.4s    \n",
            "\n",
            "2021-04-02 16:47:03 (39.9 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14746350/14746350]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLrLxCrVqNIR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRxlrXLtnRbi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import scipy.sparse as sp\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "import json\n",
        "\n",
        "from networkx.readwrite import json_graph\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pdb\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import os.path\n",
        "import torch_geometric as tg\n",
        "import torch_geometric.datasets\n",
        "import time\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzalEABl8qD5"
      },
      "source": [
        "class GAT1(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The most interesting and hardest implementation is implementation #3.\n",
        "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
        "\n",
        "    So I'll focus on imp #3 in this notebook.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,dropout=0.6, log_attention_weights=False):\n",
        "        super().__init__()\n",
        "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
        "\n",
        "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
        "        self.lin1 = nn.Linear(128, 256)\n",
        "        self.lin2 = nn.Linear(256, 1)\n",
        "        gat_layers = []  # collect GAT layers\n",
        "        for i in range(num_of_layers):\n",
        "            layer = GATLayer(\n",
        "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
        "                num_out_features=num_features_per_layer[i+1],\n",
        "                num_of_heads=num_heads_per_layer[i+1],\n",
        "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
        "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
        "                dropout_prob=dropout,\n",
        "                add_skip_connection=add_skip_connection,\n",
        "                bias=bias,\n",
        "                log_attention_weights=log_attention_weights\n",
        "            )\n",
        "            gat_layers.append(layer)\n",
        "\n",
        "        self.gat_net = nn.Sequential(\n",
        "            *gat_layers,\n",
        "        )\n",
        "\n",
        "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
        "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
        "    def forward(self, data):\n",
        "        return self.gat_net(data)\n",
        "\n",
        "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
        "        edge_list = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        \n",
        "        logits = torch.cat([z[edge_list[0]], z[edge_list[1]]], dim=-1)\n",
        "        logits = self.lin1(logits)\n",
        "        logits = torch.tanh(logits)\n",
        "        logits = self.lin2(logits)\n",
        "        return logits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xddbNsi685RS"
      },
      "source": [
        "class GATLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
        "\n",
        "    But, it's hopefully much more readable! (and of similar performance)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We'll use these constants in many functions so just extracting them here as member fields\n",
        "    src_nodes_dim = 0  # position of source nodes in edge index\n",
        "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
        "\n",
        "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
        "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
        "    head_dim = 1       # attention head dim\n",
        "\n",
        "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
        "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_of_heads = num_of_heads\n",
        "        self.num_out_features = num_out_features\n",
        "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
        "        self.add_skip_connection = add_skip_connection\n",
        "\n",
        "        #\n",
        "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
        "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
        "        #\n",
        "\n",
        "        # You can treat this one matrix as num_of_heads independent W matrices\n",
        "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "\n",
        "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
        "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
        "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
        "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
        "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
        "\n",
        "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
        "        if bias and concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
        "        elif bias and not concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        if add_skip_connection:\n",
        "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
        "        else:\n",
        "            self.register_parameter('skip_proj', None)\n",
        "\n",
        "        #\n",
        "        # End of trainable weights\n",
        "        #\n",
        "\n",
        "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
        "        self.activation = activation\n",
        "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
        "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
        "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
        "\n",
        "        self.init_params()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        #\n",
        "        # Step 1: Linear Projection + regularization\n",
        "        #\n",
        "\n",
        "        in_nodes_features, edge_index = data  # unpack data\n",
        "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
        "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
        "\n",
        "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
        "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
        "        in_nodes_features = self.dropout(in_nodes_features)\n",
        "\n",
        "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
        "        # We project the input node features into NH independent output features (one for each attention head)\n",
        "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
        "\n",
        "        #\n",
        "        # Step 2: Edge attention calculation\n",
        "        #\n",
        "\n",
        "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
        "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
        "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
        "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
        "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
        "\n",
        "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
        "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
        "        # by the edge index.\n",
        "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
        "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
        "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
        "\n",
        "        # shape = (E, NH, 1)\n",
        "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
        "        # Add stochasticity to neighborhood aggregation\n",
        "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
        "\n",
        "        #\n",
        "        # Step 3: Neighborhood aggregation\n",
        "        #\n",
        "\n",
        "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
        "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
        "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
        "\n",
        "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
        "        # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
        "\n",
        "        #\n",
        "        # Step 4: Residual/skip connections, concat and bias\n",
        "        #\n",
        "\n",
        "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
        "        return (out_nodes_features, edge_index)\n",
        "\n",
        "    #\n",
        "    # Helper functions (without comments there is very little code so don't be scared!)\n",
        "    #\n",
        "\n",
        "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
        "        \"\"\"\n",
        "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
        "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
        "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
        "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
        "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
        "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
        "\n",
        "        Note:\n",
        "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
        "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
        "        Check out this link for more details:\n",
        "\n",
        "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
        "\n",
        "        \"\"\"\n",
        "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
        "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
        "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
        "\n",
        "        # Calculate the denominator. shape = (E, NH)\n",
        "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
        "\n",
        "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
        "        # possibility of the computer rounding a very small number all the way to 0.\n",
        "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
        "\n",
        "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
        "        return attentions_per_edge.unsqueeze(-1)\n",
        "\n",
        "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
        "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
        "\n",
        "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
        "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes\n",
        "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
        "\n",
        "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
        "        # target index)\n",
        "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
        "\n",
        "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
        "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
        "        # shape = (N, NH) -> (E, NH)\n",
        "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
        "\n",
        "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
        "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
        "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
        "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
        "\n",
        "        # shape = (E) -> (E, NH, FOUT)\n",
        "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
        "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
        "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
        "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
        "\n",
        "        return out_nodes_features\n",
        "\n",
        "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
        "        \"\"\"\n",
        "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
        "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
        "\n",
        "        \"\"\"\n",
        "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
        "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
        "\n",
        "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
        "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
        "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
        "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
        "\n",
        "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
        "\n",
        "    def explicit_broadcast(self, this, other):\n",
        "        # Append singleton dimensions until this.dim() == other.dim()\n",
        "        for _ in range(this.dim(), other.dim()):\n",
        "            this = this.unsqueeze(-1)\n",
        "\n",
        "        # Explicitly expand so that shapes are the same\n",
        "        return this.expand_as(other)\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"\n",
        "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
        "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
        "\n",
        "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
        "        Feel free to experiment - there may be better initializations depending on your problem.\n",
        "\n",
        "        \"\"\"\n",
        "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
        "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
        "            self.attention_weights = attention_coefficients\n",
        "\n",
        "        if self.add_skip_connection:  # add skip or residual connection\n",
        "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
        "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
        "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
        "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
        "            else:\n",
        "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
        "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
        "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
        "\n",
        "        if self.concat:\n",
        "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
        "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
        "        else:\n",
        "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
        "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out_nodes_features += self.bias\n",
        "\n",
        "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG-D9JlKZLhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94dc237-1b58-4a03-a685-7d15dc06f6e2"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.datasets import PPI\n",
        "PPI_NUM_INPUT_FEATURES = 50\n",
        "PPI_NUM_CLASSES = 121\n",
        "train_dataset_1 = PPI(root='/tmp/PPI/train', split='train')\n",
        "val_dataset_1 = PPI(root='/tmp/PPI/val', split='val')\n",
        "test_dataset_1 = PPI(root='/tmp/PPI/test', split='test')\n",
        "def load_graph_data():\n",
        "    return (train_dataloader, val_dataloader, test_dataloader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting /tmp/PPI/train/ppi.zip\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting /tmp/PPI/val/ppi.zip\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting /tmp/PPI/test/ppi.zip\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqy-KJdvz-M_"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def split_edges(edges, remove_ratio):\n",
        "    e = edges.shape[1]\n",
        "    edges = edges[:, np.random.permutation(e)]\n",
        "    if True:\n",
        "        unique, counts = np.unique(edges, return_counts=True)\n",
        "        node_count = dict(zip(unique, counts))\n",
        "\n",
        "        index_train = []\n",
        "        index_val = []\n",
        "        for i in range(e):\n",
        "            node1 = edges[0,i]\n",
        "            node2 = edges[1,i]\n",
        "            if node_count[node1]>1 and node_count[node2]>1:\n",
        "                index_val.append(i)\n",
        "                node_count[node1] -= 1\n",
        "                node_count[node2] -= 1\n",
        "                if len(index_val) == int(e * remove_ratio):\n",
        "                    break\n",
        "            else:\n",
        "                index_train.append(i)\n",
        "        index_train = index_train + list(range(i + 1, e))\n",
        "\n",
        "        edges_train = edges[:, index_train]\n",
        "        edges_val = edges[:, index_val]\n",
        "        \n",
        "    return edges_train, edges_val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rts_k-W21Pp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786ee411-1939-409b-8c26-6eb409d6cc51"
      },
      "source": [
        "graphs = train_dataset_1 + val_dataset_1 + test_dataset_1\n",
        "dataset_1 = []\n",
        "for i in range(len(graphs)):\n",
        "    data = graphs[i]\n",
        "    data.true_edge_index = data.edge_index\n",
        "    true_edge_index = data.edge_index.cpu().detach().numpy()\n",
        "    data.edge_index, data.train_edges_pos = split_edges(true_edge_index, 0.5)\n",
        "    data.edge_index, data.train_edges_pos = torch.tensor(data.edge_index), torch.tensor(data.train_edges_pos)\n",
        "    dataset_1.append(data)\n",
        "print( dataset_1[0].x.shape, dataset_1[0].true_edge_index.shape, dataset_1[0].edge_index.shape, dataset_1[0].train_edges_pos.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1767, 50]) torch.Size([2, 32318]) torch.Size([2, 16159]) torch.Size([2, 16159])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMlem0X2YNPV"
      },
      "source": [
        "def get_link_labels(pos_edge_index, neg_edge_index):\n",
        "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
        "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
        "    link_labels[:pos_edge_index.size(1)] = 1.\n",
        "    return link_labels\n",
        "\n",
        "def train_or_test(model, batch, loss_func, optimizer, phase):\n",
        "    (node_features, edge_index, mask_link_positive) = batch.x, batch.edge_index, batch.train_edges_pos\n",
        "    mask_link_negative = negative_sampling(edge_index= batch.true_edge_index, num_nodes=batch.num_nodes, num_neg_samples=mask_link_positive.shape[1])\n",
        "    node_features, edge_index = node_features.to(device), edge_index.to(device)\n",
        "    mask_link_positive, mask_link_negative = mask_link_positive.to(device), mask_link_negative.to(device)\n",
        "    graph_data = (node_features, edge_index)\n",
        "    \n",
        "    if phase=='Train':\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    \n",
        "    embeddings = model(graph_data)[0]\n",
        "    unnormalized_scores = model.decode(embeddings, mask_link_positive, mask_link_negative)\n",
        "    unnormalized_scores = unnormalized_scores.squeeze(1)\n",
        "    batch_target = get_link_labels(mask_link_positive, mask_link_negative)\n",
        "    loss = loss_func(unnormalized_scores, batch_target)\n",
        "\n",
        "    if phase=='Train':\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    #Changing to Numpy to measure metrics\n",
        "    batch_pred = (unnormalized_scores > 0).float().cpu().numpy()\n",
        "    batch_target = batch_target.cpu().numpy()\n",
        "    unnormalized_scores =  unnormalized_scores.float()\n",
        "    unnormalized_scores = unnormalized_scores.cpu()\n",
        "    unnormalized_scores = unnormalized_scores.detach()\n",
        "    unnormalized_scores = unnormalized_scores.numpy() \n",
        "    #Metrics\n",
        "    micro_f1 = f1_score(batch_target, batch_pred, average='micro')\n",
        "    return [batch_pred, batch_target, micro_f1, loss.item(), unnormalized_scores]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnPh4b5RVSCK",
        "outputId": "6ab41f9a-5d03-438c-b818-5835581b01e3"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "config = {\n",
        "        'number_of_epochs': 1000,\n",
        "        'patience_period': 20,\n",
        "        'path': '/content/drive/My Drive/ppi_lp_models/',\n",
        "        'num_of_layers': 2,  # PPI has got 42% of nodes with all 0 features - that's why 3 layers are useful\n",
        "        'num_heads_per_layer': [4, 4],  # other values may give even better results from the reported ones\n",
        "        'num_features_per_layer': [50, 64, 64],  # 64 would also give ~0.975 uF1!\n",
        "        'add_skip_connection': True,  # skip connection is very important! (keep it otherwise micro-F1 is almost 0)\n",
        "        'bias': True,  # bias doesn't matter that much\n",
        "        'dropout': 0.0,  # dropout hurts the performance (best to keep it at 0)\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "dataset = np.empty(len(dataset_1), dtype=np.object)\n",
        "dataset[:] = dataset_1\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "val_acc_fin = []\n",
        "val_f1_fin = []\n",
        "val_roc_auc_fin = []\n",
        "\n",
        "train_acc_fin = []\n",
        "train_f1_fin = []\n",
        "train_roc_auc_fin = []\n",
        "\n",
        "begin = time.time()\n",
        "avg_test_loss, avg_test_micro_f1, avg_test_micro_recall, avg_test_micro_precision, avg_test_micro_roc_auc_score  = [0, 0, 0, 0, 0]\n",
        "fold = 0\n",
        "\n",
        "for dev_index, test_index in kf.split(dataset):\n",
        "    fold = fold+1\n",
        "    dev_dataset, test_dataset = dataset[dev_index], dataset[test_index]\n",
        "    train_dataset, val_dataset = train_test_split(dev_dataset, test_size=0.2)\n",
        "    train_dataset, test_dataset, val_dataset = train_dataset, test_dataset, val_dataset\n",
        "    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "    model = GAT1(\n",
        "          num_of_layers=config['num_of_layers'],\n",
        "          num_heads_per_layer=config['num_heads_per_layer'],\n",
        "          num_features_per_layer=config['num_features_per_layer'],\n",
        "          add_skip_connection=config['add_skip_connection'],\n",
        "          bias=config['bias'],\n",
        "          dropout=config['dropout'],\n",
        "          log_attention_weights=False  # no need to store attentions, used only in playground.py for visualizations\n",
        "          ).to(device)\n",
        "    \n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    #optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, eps=1e-3, amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, mode='max', verbose=True)\n",
        "\n",
        "    best_val_micro_f1, best_val_loss, patience_cnt = [0, 999999, 0]\n",
        "    \n",
        "    train_acc_list = []\n",
        "    train_f1_list = []\n",
        "    train_roc_auc_list = []\n",
        "    \n",
        "    \n",
        "    val_acc_list = []\n",
        "    val_f1_list = []\n",
        "    val_roc_auc_list = []\n",
        "\n",
        "\n",
        "    for epoch in range(config['number_of_epochs']):\n",
        "        epoch_loss = 0\n",
        "        predictions, target, store_temp = np.array([]), np.array([]),np.array([])\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            batch_pred, batch_target, micro_f1, loss, store = train_or_test(model, batch, loss_func, optimizer, 'Train')\n",
        "            epoch_loss = epoch_loss+loss\n",
        "            predictions = np.concatenate((predictions, batch_pred.flatten()), axis=0)\n",
        "            store_temp = np.concatenate((store_temp,store.flatten()), axis=0)\n",
        "            target = np.concatenate((target, batch_target.flatten()), axis=0)\n",
        "        #epoch_loss = epoch_loss/len(train_dataloader)\n",
        "        train_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "        train_acc = (predictions == target).sum()/(predictions.shape[0])\n",
        "        train_acc_list.append(train_acc)\n",
        "        train_f1_list.append(train_micro_f1)\n",
        "        roc_auc_value = roc_auc_score(target, store_temp, average='micro')\n",
        "        macro_roc = roc_auc_score(target, store_temp, average='macro')\n",
        "        train_roc_auc_list.append(roc_auc_value)\n",
        "        print(f\"Training Epoch Loss after {epoch} is {epoch_loss}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            predictions, target, store_temp = np.array([]), np.array([]),np.array([])\n",
        "            for batch_idx, batch in enumerate(val_dataloader):\n",
        "                batch_pred, batch_target, micro_f1, loss, store = train_or_test(model, batch, loss_func, None, 'Val')\n",
        "                val_loss = val_loss + loss\n",
        "                predictions = np.concatenate((predictions, batch_pred.flatten()), axis=0)\n",
        "                store_temp = np.concatenate((store_temp,store.flatten()), axis=0)\n",
        "                target = np.concatenate((target, batch_target.flatten()), axis=0)\n",
        "            val_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "            val_acc = (predictions == target).sum()/(predictions.shape[0])\n",
        "            val_acc_list.append(val_acc)\n",
        "            val_f1_list.append(val_micro_f1)\n",
        "            roc_auc_value = roc_auc_score(target, store_temp, average='micro')\n",
        "            val_roc_auc_list.append(roc_auc_value)\n",
        "            macro_roc = roc_auc_score(target, store_temp, average='macro')\n",
        "\n",
        "            if val_micro_f1 > best_val_micro_f1 or  best_val_loss - val_loss > 1e-5:\n",
        "                best_val_micro_f1 = max(val_micro_f1, best_val_micro_f1)\n",
        "                best_val_loss = min(val_loss, best_val_loss)\n",
        "                patience_cnt = 0\n",
        "                torch.save(model.state_dict(), config['path']+f'model_lp_{fold}')\n",
        "            else:\n",
        "                patience_cnt = patience_cnt+1\n",
        "            \n",
        "            temp = {}\n",
        "            temp[\"Validation Epoch Loss\"] = val_loss\n",
        "            temp[\"Epoch\"] = epoch\n",
        "            temp[\"Micro F1\"] = val_micro_f1\n",
        "            temp[\"Patience Count\"] = patience_cnt\n",
        "            temp[\"Best Val F1\"] = best_val_micro_f1\n",
        "            temp[\"Best Val Loss\"] = best_val_loss\n",
        "            temp[\"Micro ROC value\"] = roc_auc_value\n",
        "            temp[\"Macro ROC value\"] = macro_roc\n",
        "\n",
        "\n",
        "            print(temp)\n",
        "\n",
        "            if patience_cnt >= config['patience_period']:\n",
        "                break\n",
        "\n",
        "        scheduler.step(val_acc_list[-1])\n",
        "\n",
        "    \n",
        "    train_acc_fin.append(train_acc_list)  \n",
        "    train_f1_fin.append(train_f1_list)\n",
        "    train_roc_auc_fin.append(train_roc_auc_list)\n",
        "    \n",
        "    val_acc_fin.append(val_acc_list)  \n",
        "    val_f1_fin.append(val_f1_list)\n",
        "    val_roc_auc_fin.append(val_roc_auc_list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.load_state_dict(torch.load(config['path']+f'model_lp_{fold}'))\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        predictions, target, store_temp = np.array([]), np.array([]),np.array([])\n",
        "        for batch_idx, batch in enumerate(test_dataloader):\n",
        "            batch_pred, batch_target, micro_f1, loss, store = train_or_test(model, batch, loss_func, None, 'Test')\n",
        "            test_loss = test_loss + loss\n",
        "            predictions = np.concatenate((predictions, batch_pred.flatten()), axis=0)\n",
        "            target = np.concatenate((target, batch_target.flatten()), axis=0)\n",
        "            store_temp = np.concatenate((store_temp,store.flatten()), axis=0)\n",
        "        \n",
        "        test_micro_f1 = f1_score(target, predictions, average='micro')\n",
        "        test_micro_precision = precision_score(target, predictions, average='micro')\n",
        "        test_micro_recall = recall_score(target, predictions, average='micro')\n",
        "        test_micro_roc_auc_score = roc_auc_score(target, store_temp, average='micro')\n",
        "        test_macro_roc_auc_score = roc_auc_score(target, store_temp, average='macro')\n",
        "\n",
        "        avg_test_loss = avg_test_loss + test_loss\n",
        "        avg_test_micro_f1 = avg_test_micro_f1 + test_micro_f1\n",
        "        avg_test_micro_recall = avg_test_micro_recall + test_micro_recall\n",
        "        avg_test_micro_roc_auc_score = avg_test_micro_roc_auc_score + test_micro_roc_auc_score\n",
        "\n",
        "        temp = {}\n",
        "        print(\"Testing stats.\\n\\n\\n\")\n",
        "        temp[\"Micro F1\"] = test_micro_f1 \n",
        "        temp[\"Micro Recall\"] = test_micro_recall\n",
        "        temp[\"Micro Precision\"] = test_micro_precision\n",
        "        temp[\"Micro ROC_AUC_Score\"] = test_micro_roc_auc_score\n",
        "        temp[\"Macro ROC_AUC_Score\"] = test_macro_roc_auc_score\n",
        "        print(temp)\n",
        "        print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"Average Statistics\")\n",
        "average = {}\n",
        "\n",
        "average[\"avg_test_micro_f1\"] = avg_test_micro_f1\n",
        "average[\"avg_test_loss\"] = avg_test_loss\n",
        "average[\"avg_test_micro_recall\"] = avg_test_micro_recall\n",
        "average[\"avg_test_micro_precision\"] = avg_test_micro_precision\n",
        "average[\"avg_test_micro_roc_auc_score\"] = avg_test_micro_roc_auc_score\n",
        "\n",
        "\n",
        "end = time.time()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Epoch Loss after 0 is 9.310331284999847\n",
            "{'Validation Epoch Loss': 2.168711841106415, 'Epoch': 0, 'Micro F1': 0.7259163421520577, 'Patience Count': 0, 'Best Val F1': 0.7259163421520577, 'Best Val Loss': 2.168711841106415, 'Micro ROC value': 0.8245851516802329, 'Macro ROC value': 0.8245851516802329}\n",
            "Training Epoch Loss after 1 is 7.753049671649933\n",
            "{'Validation Epoch Loss': 1.9836122393608093, 'Epoch': 1, 'Micro F1': 0.7638797713207691, 'Patience Count': 0, 'Best Val F1': 0.7638797713207691, 'Best Val Loss': 1.9836122393608093, 'Micro ROC value': 0.8424867362761725, 'Macro ROC value': 0.8424867362761725}\n",
            "Training Epoch Loss after 2 is 7.61249765753746\n",
            "{'Validation Epoch Loss': 1.9804073870182037, 'Epoch': 2, 'Micro F1': 0.7655974149304339, 'Patience Count': 0, 'Best Val F1': 0.7655974149304339, 'Best Val Loss': 1.9804073870182037, 'Micro ROC value': 0.8470021751043441, 'Macro ROC value': 0.8470021751043441}\n",
            "Training Epoch Loss after 3 is 7.511278510093689\n",
            "{'Validation Epoch Loss': 2.0054618418216705, 'Epoch': 3, 'Micro F1': 0.7626146423540959, 'Patience Count': 1, 'Best Val F1': 0.7655974149304339, 'Best Val Loss': 1.9804073870182037, 'Micro ROC value': 0.8475543415506244, 'Macro ROC value': 0.8475543415506244}\n",
            "Training Epoch Loss after 4 is 7.440461963415146\n",
            "{'Validation Epoch Loss': 1.944589912891388, 'Epoch': 4, 'Micro F1': 0.7685114655873025, 'Patience Count': 0, 'Best Val F1': 0.7685114655873025, 'Best Val Loss': 1.944589912891388, 'Micro ROC value': 0.8503558536260423, 'Macro ROC value': 0.8503558536260423}\n",
            "Training Epoch Loss after 5 is 7.263422667980194\n",
            "{'Validation Epoch Loss': 1.9221826791763306, 'Epoch': 5, 'Micro F1': 0.774612653839045, 'Patience Count': 0, 'Best Val F1': 0.774612653839045, 'Best Val Loss': 1.9221826791763306, 'Micro ROC value': 0.853107763931209, 'Macro ROC value': 0.853107763931209}\n",
            "Training Epoch Loss after 6 is 7.220061898231506\n",
            "{'Validation Epoch Loss': 1.9285948276519775, 'Epoch': 6, 'Micro F1': 0.7741569524732156, 'Patience Count': 1, 'Best Val F1': 0.774612653839045, 'Best Val Loss': 1.9221826791763306, 'Micro ROC value': 0.8545700855019069, 'Macro ROC value': 0.8545700855019069}\n",
            "Training Epoch Loss after 7 is 7.219019263982773\n",
            "{'Validation Epoch Loss': 1.9448529779911041, 'Epoch': 7, 'Micro F1': 0.7730352260342509, 'Patience Count': 2, 'Best Val F1': 0.774612653839045, 'Best Val Loss': 1.9221826791763306, 'Micro ROC value': 0.8555230638354838, 'Macro ROC value': 0.8555230638354838}\n",
            "Training Epoch Loss after 8 is 7.233161985874176\n",
            "{'Validation Epoch Loss': 1.9135741889476776, 'Epoch': 8, 'Micro F1': 0.7761550276926215, 'Patience Count': 0, 'Best Val F1': 0.7761550276926215, 'Best Val Loss': 1.9135741889476776, 'Micro ROC value': 0.8554796992727925, 'Macro ROC value': 0.8554796992727925}\n",
            "Training Epoch Loss after 9 is 7.200049072504044\n",
            "{'Validation Epoch Loss': 1.9094910025596619, 'Epoch': 9, 'Micro F1': 0.774800670486485, 'Patience Count': 0, 'Best Val F1': 0.7761550276926215, 'Best Val Loss': 1.9094910025596619, 'Micro ROC value': 0.8563124993361011, 'Macro ROC value': 0.8563124993361011}\n",
            "Training Epoch Loss after 10 is 7.164714306592941\n",
            "{'Validation Epoch Loss': 1.9178096652030945, 'Epoch': 10, 'Micro F1': 0.7729523712404638, 'Patience Count': 1, 'Best Val F1': 0.7761550276926215, 'Best Val Loss': 1.9094910025596619, 'Micro ROC value': 0.857114549893232, 'Macro ROC value': 0.857114549893232}\n",
            "Training Epoch Loss after 11 is 7.117839306592941\n",
            "{'Validation Epoch Loss': 1.9031248092651367, 'Epoch': 11, 'Micro F1': 0.776601168889937, 'Patience Count': 0, 'Best Val F1': 0.776601168889937, 'Best Val Loss': 1.9031248092651367, 'Micro ROC value': 0.8574590572151556, 'Macro ROC value': 0.8574590572151556}\n",
            "Training Epoch Loss after 12 is 7.118564754724503\n",
            "{'Validation Epoch Loss': 1.8940313756465912, 'Epoch': 12, 'Micro F1': 0.7784239743532546, 'Patience Count': 0, 'Best Val F1': 0.7784239743532546, 'Best Val Loss': 1.8940313756465912, 'Micro ROC value': 0.8576093468793007, 'Macro ROC value': 0.8576093468793007}\n",
            "Training Epoch Loss after 13 is 7.115322411060333\n",
            "{'Validation Epoch Loss': 1.9026635885238647, 'Epoch': 13, 'Micro F1': 0.7784972689785279, 'Patience Count': 0, 'Best Val F1': 0.7784972689785279, 'Best Val Loss': 1.8940313756465912, 'Micro ROC value': 0.8588266762626494, 'Macro ROC value': 0.8588266762626494}\n",
            "Training Epoch Loss after 14 is 7.153656601905823\n",
            "{'Validation Epoch Loss': 1.9251573979854584, 'Epoch': 14, 'Micro F1': 0.7756547122070605, 'Patience Count': 1, 'Best Val F1': 0.7784972689785279, 'Best Val Loss': 1.8940313756465912, 'Micro ROC value': 0.8587885205267071, 'Macro ROC value': 0.8587885205267071}\n",
            "Training Epoch Loss after 15 is 7.153431564569473\n",
            "{'Validation Epoch Loss': 1.884598731994629, 'Epoch': 15, 'Micro F1': 0.7789593437900333, 'Patience Count': 0, 'Best Val F1': 0.7789593437900333, 'Best Val Loss': 1.884598731994629, 'Micro ROC value': 0.8589890004916259, 'Macro ROC value': 0.8589890004916259}\n",
            "Training Epoch Loss after 16 is 7.09190434217453\n",
            "{'Validation Epoch Loss': 1.8979081511497498, 'Epoch': 16, 'Micro F1': 0.7773373018655075, 'Patience Count': 1, 'Best Val F1': 0.7789593437900333, 'Best Val Loss': 1.884598731994629, 'Micro ROC value': 0.8598871292159949, 'Macro ROC value': 0.8598871292159949}\n",
            "Training Epoch Loss after 17 is 7.054807364940643\n",
            "{'Validation Epoch Loss': 1.8740022778511047, 'Epoch': 17, 'Micro F1': 0.7812505975105322, 'Patience Count': 0, 'Best Val F1': 0.7812505975105322, 'Best Val Loss': 1.8740022778511047, 'Micro ROC value': 0.8605159793629503, 'Macro ROC value': 0.8605159793629503}\n",
            "Training Epoch Loss after 18 is 7.017055451869965\n",
            "{'Validation Epoch Loss': 1.8802629709243774, 'Epoch': 18, 'Micro F1': 0.7813940000382407, 'Patience Count': 0, 'Best Val F1': 0.7813940000382407, 'Best Val Loss': 1.8740022778511047, 'Micro ROC value': 0.8603506284977472, 'Macro ROC value': 0.8603506284977472}\n",
            "Training Epoch Loss after 19 is 7.023199170827866\n",
            "{'Validation Epoch Loss': 1.891622394323349, 'Epoch': 19, 'Micro F1': 0.7791792276658529, 'Patience Count': 1, 'Best Val F1': 0.7813940000382407, 'Best Val Loss': 1.8740022778511047, 'Micro ROC value': 0.8621989796368531, 'Macro ROC value': 0.8621989796368531}\n",
            "Training Epoch Loss after 20 is 7.040210872888565\n",
            "{'Validation Epoch Loss': 1.9006724059581757, 'Epoch': 20, 'Micro F1': 0.7779268455905315, 'Patience Count': 2, 'Best Val F1': 0.7813940000382407, 'Best Val Loss': 1.8740022778511047, 'Micro ROC value': 0.861701236179726, 'Macro ROC value': 0.861701236179726}\n",
            "Training Epoch Loss after 21 is 7.02862423658371\n",
            "{'Validation Epoch Loss': 1.8659119009971619, 'Epoch': 21, 'Micro F1': 0.7818082740071765, 'Patience Count': 0, 'Best Val F1': 0.7818082740071765, 'Best Val Loss': 1.8659119009971619, 'Micro ROC value': 0.8618041967713864, 'Macro ROC value': 0.8618041967713864}\n",
            "Training Epoch Loss after 22 is 7.013669908046722\n",
            "{'Validation Epoch Loss': 1.8923258781433105, 'Epoch': 22, 'Micro F1': 0.7781467294663514, 'Patience Count': 1, 'Best Val F1': 0.7818082740071765, 'Best Val Loss': 1.8659119009971619, 'Micro ROC value': 0.8629014769164691, 'Macro ROC value': 0.8629014769164691}\n",
            "Training Epoch Loss after 23 is 6.935376077890396\n",
            "{'Validation Epoch Loss': 1.8484203219413757, 'Epoch': 23, 'Micro F1': 0.7850364242420381, 'Patience Count': 0, 'Best Val F1': 0.7850364242420381, 'Best Val Loss': 1.8484203219413757, 'Micro ROC value': 0.8657442246869842, 'Macro ROC value': 0.8657442246869842}\n",
            "Training Epoch Loss after 24 is 6.882631421089172\n",
            "{'Validation Epoch Loss': 1.843053936958313, 'Epoch': 24, 'Micro F1': 0.7853072956832652, 'Patience Count': 0, 'Best Val F1': 0.7853072956832652, 'Best Val Loss': 1.843053936958313, 'Micro ROC value': 0.8661718949115371, 'Macro ROC value': 0.8661718949115371}\n",
            "Training Epoch Loss after 25 is 6.83628848195076\n",
            "{'Validation Epoch Loss': 1.8319091796875, 'Epoch': 25, 'Micro F1': 0.7864991300246652, 'Patience Count': 0, 'Best Val F1': 0.7864991300246652, 'Best Val Loss': 1.8319091796875, 'Micro ROC value': 0.8681140670819643, 'Macro ROC value': 0.8681140670819643}\n",
            "Training Epoch Loss after 26 is 6.780245780944824\n",
            "{'Validation Epoch Loss': 1.8247313499450684, 'Epoch': 26, 'Micro F1': 0.7881211719491908, 'Patience Count': 0, 'Best Val F1': 0.7881211719491908, 'Best Val Loss': 1.8247313499450684, 'Micro ROC value': 0.8699203945716203, 'Macro ROC value': 0.8699203945716203}\n",
            "Training Epoch Loss after 27 is 6.764130383729935\n",
            "{'Validation Epoch Loss': 1.8165851533412933, 'Epoch': 27, 'Micro F1': 0.7887617032396225, 'Patience Count': 0, 'Best Val F1': 0.7887617032396225, 'Best Val Loss': 1.8165851533412933, 'Micro ROC value': 0.8703839799897998, 'Macro ROC value': 0.8703839799897998}\n",
            "Training Epoch Loss after 28 is 6.714253753423691\n",
            "{'Validation Epoch Loss': 1.8128519654273987, 'Epoch': 28, 'Micro F1': 0.7891632303172064, 'Patience Count': 0, 'Best Val F1': 0.7891632303172064, 'Best Val Loss': 1.8128519654273987, 'Micro ROC value': 0.8712677152786026, 'Macro ROC value': 0.8712677152786026}\n",
            "Training Epoch Loss after 29 is 6.685323566198349\n",
            "{'Validation Epoch Loss': 1.8010704815387726, 'Epoch': 29, 'Micro F1': 0.79118042587364, 'Patience Count': 0, 'Best Val F1': 0.79118042587364, 'Best Val Loss': 1.8010704815387726, 'Micro ROC value': 0.8738537158290396, 'Macro ROC value': 0.8738537158290396}\n",
            "Training Epoch Loss after 30 is 6.663145661354065\n",
            "{'Validation Epoch Loss': 1.7951196134090424, 'Epoch': 30, 'Micro F1': 0.7926208245964017, 'Patience Count': 0, 'Best Val F1': 0.7926208245964017, 'Best Val Loss': 1.7951196134090424, 'Micro ROC value': 0.8739846120015212, 'Macro ROC value': 0.8739846120015212}\n",
            "Training Epoch Loss after 31 is 6.62691143155098\n",
            "{'Validation Epoch Loss': 1.7843031287193298, 'Epoch': 31, 'Micro F1': 0.7945264848535063, 'Patience Count': 0, 'Best Val F1': 0.7945264848535063, 'Best Val Loss': 1.7843031287193298, 'Micro ROC value': 0.8755969360066183, 'Macro ROC value': 0.8755969360066183}\n",
            "Training Epoch Loss after 32 is 6.573591321706772\n",
            "{'Validation Epoch Loss': 1.781151294708252, 'Epoch': 32, 'Micro F1': 0.7947240616694604, 'Patience Count': 0, 'Best Val F1': 0.7947240616694604, 'Best Val Loss': 1.781151294708252, 'Micro ROC value': 0.8755583570830796, 'Macro ROC value': 0.8755583570830796}\n",
            "Training Epoch Loss after 33 is 6.558588892221451\n",
            "{'Validation Epoch Loss': 1.7774096131324768, 'Epoch': 33, 'Micro F1': 0.7952466842148871, 'Patience Count': 0, 'Best Val F1': 0.7952466842148871, 'Best Val Loss': 1.7774096131324768, 'Micro ROC value': 0.8769677990522918, 'Macro ROC value': 0.8769677990522918}\n",
            "Training Epoch Loss after 34 is 6.527548313140869\n",
            "{'Validation Epoch Loss': 1.7696169018745422, 'Epoch': 34, 'Micro F1': 0.7970694896782046, 'Patience Count': 0, 'Best Val F1': 0.7970694896782046, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8785111536546998, 'Macro ROC value': 0.8785111536546998}\n",
            "Training Epoch Loss after 35 is 6.505774438381195\n",
            "{'Validation Epoch Loss': 1.7874237895011902, 'Epoch': 35, 'Micro F1': 0.7954825017048968, 'Patience Count': 1, 'Best Val F1': 0.7970694896782046, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8802718300565919, 'Macro ROC value': 0.8802718300565919}\n",
            "Training Epoch Loss after 36 is 6.569785177707672\n",
            "{'Validation Epoch Loss': 1.7820078432559967, 'Epoch': 36, 'Micro F1': 0.7975825520551174, 'Patience Count': 0, 'Best Val F1': 0.7975825520551174, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8791181491763689, 'Macro ROC value': 0.8791181491763689}\n",
            "Training Epoch Loss after 37 is 6.61754846572876\n",
            "{'Validation Epoch Loss': 1.7881104946136475, 'Epoch': 37, 'Micro F1': 0.7948515305829791, 'Patience Count': 1, 'Best Val F1': 0.7975825520551174, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8786041778682163, 'Macro ROC value': 0.8786041778682163}\n",
            "Training Epoch Loss after 38 is 6.587221443653107\n",
            "{'Validation Epoch Loss': 1.7839958369731903, 'Epoch': 38, 'Micro F1': 0.7949630658823079, 'Patience Count': 2, 'Best Val F1': 0.7975825520551174, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8787185781329487, 'Macro ROC value': 0.8787185781329487}\n",
            "Training Epoch Loss after 39 is 6.550777941942215\n",
            "{'Validation Epoch Loss': 1.7712641060352325, 'Epoch': 39, 'Micro F1': 0.7978024359309374, 'Patience Count': 0, 'Best Val F1': 0.7978024359309374, 'Best Val Loss': 1.7696169018745422, 'Micro ROC value': 0.8816743826552231, 'Macro ROC value': 0.8816743826552231}\n",
            "Training Epoch Loss after 40 is 6.433620601892471\n",
            "{'Validation Epoch Loss': 1.7309394478797913, 'Epoch': 40, 'Micro F1': 0.8024773583342363, 'Patience Count': 0, 'Best Val F1': 0.8024773583342363, 'Best Val Loss': 1.7309394478797913, 'Micro ROC value': 0.8832390112693729, 'Macro ROC value': 0.8832390112693729}\n",
            "Training Epoch Loss after 41 is 6.3752201199531555\n",
            "{'Validation Epoch Loss': 1.740339994430542, 'Epoch': 41, 'Micro F1': 0.8019706694029993, 'Patience Count': 1, 'Best Val F1': 0.8024773583342363, 'Best Val Loss': 1.7309394478797913, 'Micro ROC value': 0.8850642485177785, 'Macro ROC value': 0.8850642485177785}\n",
            "Training Epoch Loss after 42 is 6.342151403427124\n",
            "{'Validation Epoch Loss': 1.7280455231666565, 'Epoch': 42, 'Micro F1': 0.8034238150171126, 'Patience Count': 0, 'Best Val F1': 0.8034238150171126, 'Best Val Loss': 1.7280455231666565, 'Micro ROC value': 0.8866182746537457, 'Macro ROC value': 0.8866182746537457}\n",
            "Training Epoch Loss after 43 is 6.31422483921051\n",
            "{'Validation Epoch Loss': 1.728271096944809, 'Epoch': 43, 'Micro F1': 0.803796661589155, 'Patience Count': 0, 'Best Val F1': 0.803796661589155, 'Best Val Loss': 1.7280455231666565, 'Micro ROC value': 0.8872466512433008, 'Macro ROC value': 0.8872466512433008}\n",
            "Training Epoch Loss after 44 is 6.287674695253372\n",
            "{'Validation Epoch Loss': 1.7272992134094238, 'Epoch': 44, 'Micro F1': 0.8042969770747159, 'Patience Count': 0, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.887305101724813, 'Macro ROC value': 0.887305101724813}\n",
            "Training Epoch Loss after 45 is 6.301920771598816\n",
            "{'Validation Epoch Loss': 1.7343743741512299, 'Epoch': 45, 'Micro F1': 0.8032262382011587, 'Patience Count': 1, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8876232576571862, 'Macro ROC value': 0.8876232576571862}\n",
            "Training Epoch Loss after 46 is 6.277000665664673\n",
            "{'Validation Epoch Loss': 1.7724520862102509, 'Epoch': 46, 'Micro F1': 0.798035066698109, 'Patience Count': 2, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.88822623338414, 'Macro ROC value': 0.88822623338414}\n",
            "Training Epoch Loss after 47 is 6.2807303965091705\n",
            "{'Validation Epoch Loss': 1.7391178905963898, 'Epoch': 47, 'Micro F1': 0.8038349022632105, 'Patience Count': 3, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8887077881893231, 'Macro ROC value': 0.8887077881893231}\n",
            "Training Epoch Loss after 48 is 6.296945363283157\n",
            "{'Validation Epoch Loss': 1.756032109260559, 'Epoch': 48, 'Micro F1': 0.8004282955494229, 'Patience Count': 4, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8882554023752298, 'Macro ROC value': 0.8882554023752298}\n",
            "Training Epoch Loss after 49 is 6.248223692178726\n",
            "{'Validation Epoch Loss': 1.7705816626548767, 'Epoch': 49, 'Micro F1': 0.7962313815718193, 'Patience Count': 5, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.886977878029382, 'Macro ROC value': 0.886977878029382}\n",
            "Training Epoch Loss after 50 is 6.311265796422958\n",
            "{'Validation Epoch Loss': 1.791502982378006, 'Epoch': 50, 'Micro F1': 0.7981115480462202, 'Patience Count': 6, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8869545470610745, 'Macro ROC value': 0.8869545470610745}\n",
            "Training Epoch Loss after 51 is 6.352000027894974\n",
            "{'Validation Epoch Loss': 1.7719486057758331, 'Epoch': 51, 'Micro F1': 0.7974200291903814, 'Patience Count': 7, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8866732692792447, 'Macro ROC value': 0.8866732692792447}\n",
            "Training Epoch Loss after 52 is 6.322919815778732\n",
            "{'Validation Epoch Loss': 1.7303375005722046, 'Epoch': 52, 'Micro F1': 0.8034588689683302, 'Patience Count': 8, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8878289263009326, 'Macro ROC value': 0.8878289263009326}\n",
            "Training Epoch Loss after 53 is 6.351389557123184\n",
            "{'Validation Epoch Loss': 1.7374910712242126, 'Epoch': 53, 'Micro F1': 0.802506038839778, 'Patience Count': 9, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.883590935605886, 'Macro ROC value': 0.883590935605886}\n",
            "Training Epoch Loss after 54 is 6.562422662973404\n",
            "{'Validation Epoch Loss': 1.727839708328247, 'Epoch': 54, 'Micro F1': 0.8025920803564031, 'Patience Count': 10, 'Best Val F1': 0.8042969770747159, 'Best Val Loss': 1.7272992134094238, 'Micro ROC value': 0.8846667408896332, 'Macro ROC value': 0.8846667408896332}\n",
            "Training Epoch Loss after 55 is 6.389870077371597\n",
            "{'Validation Epoch Loss': 1.6941848397254944, 'Epoch': 55, 'Micro F1': 0.8079776419525689, 'Patience Count': 0, 'Best Val F1': 0.8079776419525689, 'Best Val Loss': 1.6941848397254944, 'Micro ROC value': 0.8886960533045156, 'Macro ROC value': 0.8886960533045156}\n",
            "Training Epoch Loss after 56 is 6.427990198135376\n",
            "{'Validation Epoch Loss': 1.7127845287322998, 'Epoch': 56, 'Micro F1': 0.8049120145824437, 'Patience Count': 1, 'Best Val F1': 0.8079776419525689, 'Best Val Loss': 1.6941848397254944, 'Micro ROC value': 0.8885414403675853, 'Macro ROC value': 0.8885414403675853}\n",
            "Training Epoch Loss after 57 is 6.361257255077362\n",
            "{'Validation Epoch Loss': 1.7274542450904846, 'Epoch': 57, 'Micro F1': 0.8027514164983014, 'Patience Count': 2, 'Best Val F1': 0.8079776419525689, 'Best Val Loss': 1.6941848397254944, 'Micro ROC value': 0.8868517602655488, 'Macro ROC value': 0.8868517602655488}\n",
            "Training Epoch Loss after 58 is 6.241948962211609\n",
            "{'Validation Epoch Loss': 1.7182189226150513, 'Epoch': 58, 'Micro F1': 0.8047558651633833, 'Patience Count': 3, 'Best Val F1': 0.8079776419525689, 'Best Val Loss': 1.6941848397254944, 'Micro ROC value': 0.8897980966254434, 'Macro ROC value': 0.8897980966254434}\n",
            "Training Epoch Loss after 59 is 6.154774755239487\n",
            "{'Validation Epoch Loss': 1.6850065290927887, 'Epoch': 59, 'Micro F1': 0.8102593355045539, 'Patience Count': 0, 'Best Val F1': 0.8102593355045539, 'Best Val Loss': 1.6850065290927887, 'Micro ROC value': 0.8917703874202515, 'Macro ROC value': 0.8917703874202515}\n",
            "Training Epoch Loss after 60 is 6.080500364303589\n",
            "{'Validation Epoch Loss': 1.6646190583705902, 'Epoch': 60, 'Micro F1': 0.8136755023868554, 'Patience Count': 0, 'Best Val F1': 0.8136755023868554, 'Best Val Loss': 1.6646190583705902, 'Micro ROC value': 0.8931539198933929, 'Macro ROC value': 0.8931539198933929}\n",
            "Training Epoch Loss after 61 is 6.01472344994545\n",
            "{'Validation Epoch Loss': 1.6586970090866089, 'Epoch': 61, 'Micro F1': 0.8144180088081019, 'Patience Count': 0, 'Best Val F1': 0.8144180088081019, 'Best Val Loss': 1.6586970090866089, 'Micro ROC value': 0.8943910485702945, 'Macro ROC value': 0.8943910485702945}\n",
            "Training Epoch Loss after 62 is 5.9733846783638\n",
            "{'Validation Epoch Loss': 1.6499871015548706, 'Epoch': 62, 'Micro F1': 0.8152879841428672, 'Patience Count': 0, 'Best Val F1': 0.8152879841428672, 'Best Val Loss': 1.6499871015548706, 'Micro ROC value': 0.895249423129856, 'Macro ROC value': 0.895249423129856}\n",
            "Training Epoch Loss after 63 is 5.9692991971969604\n",
            "{'Validation Epoch Loss': 1.6450642943382263, 'Epoch': 63, 'Micro F1': 0.8160081835042478, 'Patience Count': 0, 'Best Val F1': 0.8160081835042478, 'Best Val Loss': 1.6450642943382263, 'Micro ROC value': 0.8961267818045596, 'Macro ROC value': 0.8961267818045596}\n",
            "Training Epoch Loss after 64 is 6.0017878115177155\n",
            "{'Validation Epoch Loss': 1.6706941723823547, 'Epoch': 64, 'Micro F1': 0.812426307034372, 'Patience Count': 1, 'Best Val F1': 0.8160081835042478, 'Best Val Loss': 1.6450642943382263, 'Micro ROC value': 0.8946400051455745, 'Macro ROC value': 0.8946400051455745}\n",
            "Training Epoch Loss after 65 is 5.97314390540123\n",
            "{'Validation Epoch Loss': 1.6689754724502563, 'Epoch': 65, 'Micro F1': 0.8128075363944625, 'Patience Count': 2, 'Best Val F1': 0.8160081835042478, 'Best Val Loss': 1.6450642943382263, 'Micro ROC value': 0.8952238956903837, 'Macro ROC value': 0.8952238956903837}\n",
            "Training Epoch Loss after 66 is 5.938750654459\n",
            "{'Validation Epoch Loss': 1.6548866033554077, 'Epoch': 66, 'Micro F1': 0.8150350541746336, 'Patience Count': 3, 'Best Val F1': 0.8160081835042478, 'Best Val Loss': 1.6450642943382263, 'Micro ROC value': 0.895375075053808, 'Macro ROC value': 0.895375075053808}\n",
            "Training Epoch Loss after 67 is 5.917966693639755\n",
            "{'Validation Epoch Loss': 1.644336313009262, 'Epoch': 67, 'Micro F1': 0.8161260922492527, 'Patience Count': 0, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8957312457683482, 'Macro ROC value': 0.8957312457683482}\n",
            "Training Epoch Loss after 68 is 5.8983154296875\n",
            "{'Validation Epoch Loss': 1.6453823745250702, 'Epoch': 68, 'Micro F1': 0.8161101586350629, 'Patience Count': 1, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8962594088094359, 'Macro ROC value': 0.8962594088094359}\n",
            "Training Epoch Loss after 69 is 5.899573504924774\n",
            "{'Validation Epoch Loss': 1.6497524976730347, 'Epoch': 69, 'Micro F1': 0.8160496109011416, 'Patience Count': 2, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8963550502014165, 'Macro ROC value': 0.8963550502014165}\n",
            "Training Epoch Loss after 70 is 5.876431733369827\n",
            "{'Validation Epoch Loss': 1.6624688506126404, 'Epoch': 70, 'Micro F1': 0.8145263573845928, 'Patience Count': 3, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8963370764476761, 'Macro ROC value': 0.8963370764476761}\n",
            "Training Epoch Loss after 71 is 5.8668133020401\n",
            "{'Validation Epoch Loss': 1.651526778936386, 'Epoch': 71, 'Micro F1': 0.8155333618013907, 'Patience Count': 4, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8965079615123145, 'Macro ROC value': 0.8965079615123145}\n",
            "Training Epoch Loss after 72 is 5.848241031169891\n",
            "{'Validation Epoch Loss': 1.6560693085193634, 'Epoch': 72, 'Micro F1': 0.8150521666528575, 'Patience Count': 5, 'Best Val F1': 0.8161260922492527, 'Best Val Loss': 1.644336313009262, 'Micro ROC value': 0.8961434005293271, 'Macro ROC value': 0.8961434005293271}\n",
            "Training Epoch Loss after 73 is 5.83931103348732\n",
            "{'Validation Epoch Loss': 1.6423382461071014, 'Epoch': 73, 'Micro F1': 0.8166588058331634, 'Patience Count': 0, 'Best Val F1': 0.8166588058331634, 'Best Val Loss': 1.6423382461071014, 'Micro ROC value': 0.8972870371154865, 'Macro ROC value': 0.8972870371154865}\n",
            "Training Epoch Loss after 74 is 5.853957623243332\n",
            "{'Validation Epoch Loss': 1.641737014055252, 'Epoch': 74, 'Micro F1': 0.8173051489685811, 'Patience Count': 0, 'Best Val F1': 0.8173051489685811, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.897347828458654, 'Macro ROC value': 0.897347828458654}\n",
            "Training Epoch Loss after 75 is 5.893933862447739\n",
            "{'Validation Epoch Loss': 1.6480236947536469, 'Epoch': 75, 'Micro F1': 0.81758879803188, 'Patience Count': 0, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.8972783699615936, 'Macro ROC value': 0.8972783699615936}\n",
            "Training Epoch Loss after 76 is 6.005493998527527\n",
            "{'Validation Epoch Loss': 1.6561652719974518, 'Epoch': 76, 'Micro F1': 0.8149597516905563, 'Patience Count': 1, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.8955788673212328, 'Macro ROC value': 0.8955788673212328}\n",
            "Training Epoch Loss after 77 is 5.950461149215698\n",
            "{'Validation Epoch Loss': 1.68204727768898, 'Epoch': 77, 'Micro F1': 0.8091376090655892, 'Patience Count': 2, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.8955263554396279, 'Macro ROC value': 0.8955263554396279}\n",
            "Training Epoch Loss after 78 is 5.999930411577225\n",
            "{'Validation Epoch Loss': 1.665925145149231, 'Epoch': 78, 'Micro F1': 0.8158743411450533, 'Patience Count': 3, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.895290613138737, 'Macro ROC value': 0.895290613138737}\n",
            "Training Epoch Loss after 79 is 5.927587568759918\n",
            "{'Validation Epoch Loss': 1.6675509810447693, 'Epoch': 79, 'Micro F1': 0.8128533278946597, 'Patience Count': 4, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.641737014055252, 'Micro ROC value': 0.897281356606633, 'Macro ROC value': 0.897281356606633}\n",
            "Training Epoch Loss after 80 is 5.911485940217972\n",
            "{'Validation Epoch Loss': 1.6394768953323364, 'Epoch': 80, 'Micro F1': 0.8173306734820046, 'Patience Count': 0, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.6394768953323364, 'Micro ROC value': 0.8974623677892177, 'Macro ROC value': 0.8974623677892177}\n",
            "Training Epoch Loss after 81 is 5.826921910047531\n",
            "{'Validation Epoch Loss': 1.6615748405456543, 'Epoch': 81, 'Micro F1': 0.8142809797260693, 'Patience Count': 1, 'Best Val F1': 0.81758879803188, 'Best Val Loss': 1.6394768953323364, 'Micro ROC value': 0.8973532261726574, 'Macro ROC value': 0.8973532261726574}\n",
            "Training Epoch Loss after 82 is 5.8311072289943695\n",
            "{'Validation Epoch Loss': 1.6227697730064392, 'Epoch': 82, 'Micro F1': 0.818971835743558, 'Patience Count': 0, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.8991455487647928, 'Macro ROC value': 0.8991455487647928}\n",
            "Training Epoch Loss after 83 is 5.813282340764999\n",
            "{'Validation Epoch Loss': 1.6389691531658173, 'Epoch': 83, 'Micro F1': 0.8169450800186104, 'Patience Count': 1, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.8971908216769608, 'Macro ROC value': 0.8971908216769608}\n",
            "Training Epoch Loss after 84 is 5.830368280410767\n",
            "{'Validation Epoch Loss': 1.6530228555202484, 'Epoch': 84, 'Micro F1': 0.8153166646484088, 'Patience Count': 2, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.8956053521907641, 'Macro ROC value': 0.8956053521907641}\n",
            "Training Epoch Loss after 85 is 5.807772487401962\n",
            "{'Validation Epoch Loss': 1.6626640260219574, 'Epoch': 85, 'Micro F1': 0.8142172452693098, 'Patience Count': 3, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.8973180212948211, 'Macro ROC value': 0.8973180212948211}\n",
            "Training Epoch Loss after 86 is 5.781939893960953\n",
            "{'Validation Epoch Loss': 1.669247180223465, 'Epoch': 86, 'Micro F1': 0.8140561314726943, 'Patience Count': 4, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.8975059269553362, 'Macro ROC value': 0.8975059269553362}\n",
            "Training Epoch Loss after 87 is 5.735217928886414\n",
            "{'Validation Epoch Loss': 1.63213449716568, 'Epoch': 87, 'Micro F1': 0.8184237194154276, 'Patience Count': 5, 'Best Val F1': 0.818971835743558, 'Best Val Loss': 1.6227697730064392, 'Micro ROC value': 0.9000888420411761, 'Macro ROC value': 0.9000888420411761}\n",
            "Training Epoch Loss after 88 is 5.740355014801025\n",
            "{'Validation Epoch Loss': 1.6137662827968597, 'Epoch': 88, 'Micro F1': 0.8204058610206436, 'Patience Count': 0, 'Best Val F1': 0.8204058610206436, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8999311637563875, 'Macro ROC value': 0.8999311637563875}\n",
            "Training Epoch Loss after 89 is 5.7594877779483795\n",
            "{'Validation Epoch Loss': 1.6431622803211212, 'Epoch': 89, 'Micro F1': 0.8176429723201254, 'Patience Count': 1, 'Best Val F1': 0.8204058610206436, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8988296333950457, 'Macro ROC value': 0.8988296333950457}\n",
            "Training Epoch Loss after 90 is 5.832517117261887\n",
            "{'Validation Epoch Loss': 1.6454170644283295, 'Epoch': 90, 'Micro F1': 0.8153995194421959, 'Patience Count': 2, 'Best Val F1': 0.8204058610206436, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8971495154519428, 'Macro ROC value': 0.8971495154519428}\n",
            "Training Epoch Loss after 91 is 5.725860625505447\n",
            "{'Validation Epoch Loss': 1.6355803310871124, 'Epoch': 91, 'Micro F1': 0.8164925653756191, 'Patience Count': 3, 'Best Val F1': 0.8204058610206436, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8993396062140975, 'Macro ROC value': 0.8993396062140975}\n",
            "Training Epoch Loss after 92 is 5.689728021621704\n",
            "{'Validation Epoch Loss': 1.6322002410888672, 'Epoch': 92, 'Micro F1': 0.8195422591315543, 'Patience Count': 4, 'Best Val F1': 0.8204058610206436, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8996187394608957, 'Macro ROC value': 0.8996187394608957}\n",
            "Training Epoch Loss after 93 is 5.699526518583298\n",
            "{'Validation Epoch Loss': 1.6154752373695374, 'Epoch': 93, 'Micro F1': 0.820663985570519, 'Patience Count': 0, 'Best Val F1': 0.820663985570519, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.9005078643187405, 'Macro ROC value': 0.9005078643187405}\n",
            "Training Epoch Loss after 94 is 5.639935374259949\n",
            "{'Validation Epoch Loss': 1.615461826324463, 'Epoch': 94, 'Micro F1': 0.820871122554987, 'Patience Count': 0, 'Best Val F1': 0.820871122554987, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.9005831546037776, 'Macro ROC value': 0.9005831546037776}\n",
            "Training Epoch Loss after 95 is 5.645502388477325\n",
            "{'Validation Epoch Loss': 1.6349610686302185, 'Epoch': 95, 'Micro F1': 0.8187646987590902, 'Patience Count': 1, 'Best Val F1': 0.820871122554987, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.8994003671869599, 'Macro ROC value': 0.8994003671869599}\n",
            "Training Epoch Loss after 96 is 5.633679300546646\n",
            "{'Validation Epoch Loss': 1.6360920071601868, 'Epoch': 96, 'Micro F1': 0.8185639352202981, 'Patience Count': 2, 'Best Val F1': 0.820871122554987, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.9000531936247178, 'Macro ROC value': 0.9000531936247178}\n",
            "Training Epoch Loss after 97 is 5.593825697898865\n",
            "{'Validation Epoch Loss': 1.6196843683719635, 'Epoch': 97, 'Micro F1': 0.8202305912645553, 'Patience Count': 3, 'Best Val F1': 0.820871122554987, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.9005392037812491, 'Macro ROC value': 0.9005392037812491}\n",
            "Training Epoch Loss after 98 is 5.560081839561462\n",
            "{'Validation Epoch Loss': 1.6180728375911713, 'Epoch': 98, 'Micro F1': 0.8203803672379398, 'Patience Count': 4, 'Best Val F1': 0.820871122554987, 'Best Val Loss': 1.6137662827968597, 'Micro ROC value': 0.9010928857493217, 'Macro ROC value': 0.9010928857493217}\n",
            "Training Epoch Loss after 99 is 5.543804556131363\n",
            "{'Validation Epoch Loss': 1.6070927679538727, 'Epoch': 99, 'Micro F1': 0.8223816291801836, 'Patience Count': 0, 'Best Val F1': 0.8223816291801836, 'Best Val Loss': 1.6070927679538727, 'Micro ROC value': 0.9021020850100243, 'Macro ROC value': 0.9021020850100243}\n",
            "Training Epoch Loss after 100 is 5.546206891536713\n",
            "{'Validation Epoch Loss': 1.6006880104541779, 'Epoch': 100, 'Micro F1': 0.8233312725858981, 'Patience Count': 0, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9026236781428894, 'Macro ROC value': 0.9026236781428894}\n",
            "Training Epoch Loss after 101 is 5.551364064216614\n",
            "{'Validation Epoch Loss': 1.609578549861908, 'Epoch': 101, 'Micro F1': 0.8220916374019286, 'Patience Count': 1, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9014025750660012, 'Macro ROC value': 0.9014025750660012}\n",
            "Training Epoch Loss after 102 is 5.577205181121826\n",
            "{'Validation Epoch Loss': 1.6141939163208008, 'Epoch': 102, 'Micro F1': 0.8196665413222352, 'Patience Count': 2, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9011347369025029, 'Macro ROC value': 0.9011347369025029}\n",
            "Training Epoch Loss after 103 is 5.599148273468018\n",
            "{'Validation Epoch Loss': 1.6349206566810608, 'Epoch': 103, 'Micro F1': 0.8172637523024072, 'Patience Count': 3, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9016025461943464, 'Macro ROC value': 0.9016025461943464}\n",
            "Training Epoch Loss after 104 is 5.580676168203354\n",
            "{'Validation Epoch Loss': 1.6442656219005585, 'Epoch': 104, 'Micro F1': 0.8170088144753698, 'Patience Count': 4, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9003410373948617, 'Macro ROC value': 0.9003410373948617}\n",
            "Training Epoch Loss after 105 is 5.58679661154747\n",
            "{'Validation Epoch Loss': 1.6523960530757904, 'Epoch': 105, 'Micro F1': 0.8153166646484088, 'Patience Count': 5, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9010576883050935, 'Macro ROC value': 0.9010576883050935}\n",
            "Training Epoch Loss after 106 is 5.5757066905498505\n",
            "{'Validation Epoch Loss': 1.6101194024085999, 'Epoch': 106, 'Micro F1': 0.820653282345443, 'Patience Count': 6, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9015338688199843, 'Macro ROC value': 0.9015338688199843}\n",
            "Training Epoch Loss after 107 is 5.592152804136276\n",
            "{'Validation Epoch Loss': 1.6125978529453278, 'Epoch': 107, 'Micro F1': 0.820651238679167, 'Patience Count': 7, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9002821423998282, 'Macro ROC value': 0.9002821423998282}\n",
            "Training Epoch Loss after 108 is 5.6244179010391235\n",
            "{'Validation Epoch Loss': 1.6290502846240997, 'Epoch': 108, 'Micro F1': 0.8186754705196271, 'Patience Count': 8, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.8987930036813749, 'Macro ROC value': 0.8987930036813749}\n",
            "Training Epoch Loss after 109 is 5.59749099612236\n",
            "{'Validation Epoch Loss': 1.6649487614631653, 'Epoch': 109, 'Micro F1': 0.8187377706393366, 'Patience Count': 9, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9011300413049355, 'Macro ROC value': 0.9011300413049355}\n",
            "Training Epoch Loss after 110 is 5.628194153308868\n",
            "{'Validation Epoch Loss': 1.6160277426242828, 'Epoch': 110, 'Micro F1': 0.8205110228742964, 'Patience Count': 10, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.899961265157545, 'Macro ROC value': 0.899961265157545}\n",
            "Training Epoch Loss after 111 is 5.540430337190628\n",
            "{'Validation Epoch Loss': 1.6142741441726685, 'Epoch': 111, 'Micro F1': 0.8209985914685056, 'Patience Count': 11, 'Best Val F1': 0.8233312725858981, 'Best Val Loss': 1.6006880104541779, 'Micro ROC value': 0.9017427140057301, 'Macro ROC value': 0.9017427140057301}\n",
            "Epoch   112: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 112 is 5.424390226602554\n",
            "{'Validation Epoch Loss': 1.5886991024017334, 'Epoch': 112, 'Micro F1': 0.8238666420226767, 'Patience Count': 0, 'Best Val F1': 0.8238666420226767, 'Best Val Loss': 1.5886991024017334, 'Micro ROC value': 0.9037997939775978, 'Macro ROC value': 0.9037997939775978}\n",
            "Training Epoch Loss after 113 is 5.397469162940979\n",
            "{'Validation Epoch Loss': 1.5789224803447723, 'Epoch': 113, 'Micro F1': 0.8254886839472024, 'Patience Count': 0, 'Best Val F1': 0.8254886839472024, 'Best Val Loss': 1.5789224803447723, 'Micro ROC value': 0.9048020526448781, 'Macro ROC value': 0.9048020526448781}\n",
            "Training Epoch Loss after 114 is 5.381655842065811\n",
            "{'Validation Epoch Loss': 1.5781365931034088, 'Epoch': 114, 'Micro F1': 0.8261547090203377, 'Patience Count': 0, 'Best Val F1': 0.8261547090203377, 'Best Val Loss': 1.5781365931034088, 'Micro ROC value': 0.9049892091263607, 'Macro ROC value': 0.9049892091263607}\n",
            "Training Epoch Loss after 115 is 5.36834129691124\n",
            "{'Validation Epoch Loss': 1.5783140659332275, 'Epoch': 115, 'Micro F1': 0.8257659288341056, 'Patience Count': 1, 'Best Val F1': 0.8261547090203377, 'Best Val Loss': 1.5781365931034088, 'Micro ROC value': 0.9049794187394072, 'Macro ROC value': 0.9049794187394072}\n",
            "Training Epoch Loss after 116 is 5.355504631996155\n",
            "{'Validation Epoch Loss': 1.578003853559494, 'Epoch': 116, 'Micro F1': 0.8256065926922072, 'Patience Count': 0, 'Best Val F1': 0.8261547090203377, 'Best Val Loss': 1.578003853559494, 'Micro ROC value': 0.9049693397009796, 'Macro ROC value': 0.9049693397009796}\n",
            "Training Epoch Loss after 117 is 5.362021565437317\n",
            "{'Validation Epoch Loss': 1.5851083397865295, 'Epoch': 117, 'Micro F1': 0.8250019120946359, 'Patience Count': 1, 'Best Val F1': 0.8261547090203377, 'Best Val Loss': 1.578003853559494, 'Micro ROC value': 0.9039275532637085, 'Macro ROC value': 0.9039275532637085}\n",
            "Training Epoch Loss after 118 is 5.341615974903107\n",
            "{'Validation Epoch Loss': 1.5787566304206848, 'Epoch': 118, 'Micro F1': 0.8254759370558505, 'Patience Count': 2, 'Best Val F1': 0.8261547090203377, 'Best Val Loss': 1.578003853559494, 'Micro ROC value': 0.904838717779895, 'Macro ROC value': 0.904838717779895}\n",
            "Training Epoch Loss after 119 is 5.350700527429581\n",
            "{'Validation Epoch Loss': 1.5750086605548859, 'Epoch': 119, 'Micro F1': 0.826316410071736, 'Patience Count': 0, 'Best Val F1': 0.826316410071736, 'Best Val Loss': 1.5750086605548859, 'Micro ROC value': 0.9054709365431912, 'Macro ROC value': 0.9054709365431912}\n",
            "Training Epoch Loss after 120 is 5.347140163183212\n",
            "{'Validation Epoch Loss': 1.5787005424499512, 'Epoch': 120, 'Micro F1': 0.8252305593973269, 'Patience Count': 1, 'Best Val F1': 0.826316410071736, 'Best Val Loss': 1.5750086605548859, 'Micro ROC value': 0.9048905445103015, 'Macro ROC value': 0.9048905445103015}\n",
            "Training Epoch Loss after 121 is 5.348712474107742\n",
            "{'Validation Epoch Loss': 1.5781508684158325, 'Epoch': 121, 'Micro F1': 0.825421762767605, 'Patience Count': 2, 'Best Val F1': 0.826316410071736, 'Best Val Loss': 1.5750086605548859, 'Micro ROC value': 0.9050708624784569, 'Macro ROC value': 0.9050708624784569}\n",
            "Training Epoch Loss after 122 is 5.334261506795883\n",
            "{'Validation Epoch Loss': 1.577925443649292, 'Epoch': 122, 'Micro F1': 0.8260973480092543, 'Patience Count': 3, 'Best Val F1': 0.826316410071736, 'Best Val Loss': 1.5750086605548859, 'Micro ROC value': 0.9051458545661294, 'Macro ROC value': 0.9051458545661294}\n",
            "Training Epoch Loss after 123 is 5.343845903873444\n",
            "{'Validation Epoch Loss': 1.5783224403858185, 'Epoch': 123, 'Micro F1': 0.8256384599205867, 'Patience Count': 4, 'Best Val F1': 0.826316410071736, 'Best Val Loss': 1.5750086605548859, 'Micro ROC value': 0.9048674033021068, 'Macro ROC value': 0.9048674033021068}\n",
            "Training Epoch Loss after 124 is 5.340884417295456\n",
            "{'Validation Epoch Loss': 1.5701435804367065, 'Epoch': 124, 'Micro F1': 0.8268016137564451, 'Patience Count': 0, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9059366567945126, 'Macro ROC value': 0.9059366567945126}\n",
            "Training Epoch Loss after 125 is 5.32634311914444\n",
            "{'Validation Epoch Loss': 1.5764685571193695, 'Epoch': 125, 'Micro F1': 0.8262789912110184, 'Patience Count': 1, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054365373246692, 'Macro ROC value': 0.9054365373246692}\n",
            "Training Epoch Loss after 126 is 5.336741954088211\n",
            "{'Validation Epoch Loss': 1.580990970134735, 'Epoch': 126, 'Micro F1': 0.8256894474859943, 'Patience Count': 2, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9047545139487047, 'Macro ROC value': 0.9047545139487047}\n",
            "Training Epoch Loss after 127 is 5.324042618274689\n",
            "{'Validation Epoch Loss': 1.572122037410736, 'Epoch': 127, 'Micro F1': 0.8267538129138756, 'Patience Count': 3, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9056956907387252, 'Macro ROC value': 0.9056956907387252}\n",
            "Training Epoch Loss after 128 is 5.333967924118042\n",
            "{'Validation Epoch Loss': 1.580605924129486, 'Epoch': 128, 'Micro F1': 0.8260368002753329, 'Patience Count': 4, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9049617170841613, 'Macro ROC value': 0.9049617170841613}\n",
            "Training Epoch Loss after 129 is 5.318300396203995\n",
            "{'Validation Epoch Loss': 1.5821131765842438, 'Epoch': 129, 'Micro F1': 0.8253229743596282, 'Patience Count': 5, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.904493337455838, 'Macro ROC value': 0.904493337455838}\n",
            "Training Epoch Loss after 130 is 5.315946102142334\n",
            "{'Validation Epoch Loss': 1.573602169752121, 'Epoch': 130, 'Micro F1': 0.8258424101822168, 'Patience Count': 6, 'Best Val F1': 0.8268016137564451, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9055258534675646, 'Macro ROC value': 0.9055258534675646}\n",
            "Training Epoch Loss after 131 is 5.331658750772476\n",
            "{'Validation Epoch Loss': 1.5739063322544098, 'Epoch': 131, 'Micro F1': 0.826832804856674, 'Patience Count': 0, 'Best Val F1': 0.826832804856674, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054608346812534, 'Macro ROC value': 0.9054608346812534}\n",
            "Training Epoch Loss after 132 is 5.322187155485153\n",
            "{'Validation Epoch Loss': 1.5757894814014435, 'Epoch': 132, 'Micro F1': 0.8257946093396472, 'Patience Count': 1, 'Best Val F1': 0.826832804856674, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9052931286225976, 'Macro ROC value': 0.9052931286225976}\n",
            "Training Epoch Loss after 133 is 5.326387137174606\n",
            "{'Validation Epoch Loss': 1.580102413892746, 'Epoch': 133, 'Micro F1': 0.8257117545458601, 'Patience Count': 2, 'Best Val F1': 0.826832804856674, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9051193854980931, 'Macro ROC value': 0.9051193854980931}\n",
            "Training Epoch Loss after 134 is 5.305974364280701\n",
            "{'Validation Epoch Loss': 1.5743738412857056, 'Epoch': 134, 'Micro F1': 0.8270342445236168, 'Patience Count': 0, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9057944223181894, 'Macro ROC value': 0.9057944223181894}\n",
            "Training Epoch Loss after 135 is 5.3006773591041565\n",
            "{'Validation Epoch Loss': 1.5747124552726746, 'Epoch': 135, 'Micro F1': 0.8262853646566943, 'Patience Count': 1, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.905366528293285, 'Macro ROC value': 0.905366528293285}\n",
            "Training Epoch Loss after 136 is 5.311002999544144\n",
            "{'Validation Epoch Loss': 1.5735180079936981, 'Epoch': 136, 'Micro F1': 0.8261738293573655, 'Patience Count': 2, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054424291700244, 'Macro ROC value': 0.9054424291700244}\n",
            "Training Epoch Loss after 137 is 5.315586715936661\n",
            "{'Validation Epoch Loss': 1.5726277828216553, 'Epoch': 137, 'Micro F1': 0.8264192070158889, 'Patience Count': 3, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9056681458691627, 'Macro ROC value': 0.9056681458691627}\n",
            "Training Epoch Loss after 138 is 5.291621446609497\n",
            "{'Validation Epoch Loss': 1.5792960226535797, 'Epoch': 138, 'Micro F1': 0.82632360533075, 'Patience Count': 4, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9052755487079184, 'Macro ROC value': 0.9052755487079184}\n",
            "Training Epoch Loss after 139 is 5.30607271194458\n",
            "{'Validation Epoch Loss': 1.5741982460021973, 'Epoch': 139, 'Micro F1': 0.8261929496943933, 'Patience Count': 5, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9053153316119488, 'Macro ROC value': 0.9053153316119488}\n",
            "Training Epoch Loss after 140 is 5.30696639418602\n",
            "{'Validation Epoch Loss': 1.5713891088962555, 'Epoch': 140, 'Micro F1': 0.8269513897298296, 'Patience Count': 6, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9060455753418961, 'Macro ROC value': 0.9060455753418961}\n",
            "Training Epoch Loss after 141 is 5.302735805511475\n",
            "{'Validation Epoch Loss': 1.575156033039093, 'Epoch': 141, 'Micro F1': 0.8265275555923799, 'Patience Count': 7, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054588822431298, 'Macro ROC value': 0.9054588822431298}\n",
            "Training Epoch Loss after 142 is 5.304587781429291\n",
            "{'Validation Epoch Loss': 1.5785942077636719, 'Epoch': 142, 'Micro F1': 0.8265626095435975, 'Patience Count': 8, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9053465173043819, 'Macro ROC value': 0.9053465173043819}\n",
            "Training Epoch Loss after 143 is 5.306801378726959\n",
            "{'Validation Epoch Loss': 1.5771388709545135, 'Epoch': 143, 'Micro F1': 0.8267952403107692, 'Patience Count': 9, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9055226788700386, 'Macro ROC value': 0.9055226788700386}\n",
            "Training Epoch Loss after 144 is 5.300856024026871\n",
            "{'Validation Epoch Loss': 1.5749466121196747, 'Epoch': 144, 'Micro F1': 0.8267569996367136, 'Patience Count': 10, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9055443014895698, 'Macro ROC value': 0.9055443014895698}\n",
            "Training Epoch Loss after 145 is 5.288510382175446\n",
            "{'Validation Epoch Loss': 1.5751001238822937, 'Epoch': 145, 'Micro F1': 0.8266582112287366, 'Patience Count': 11, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9055595605545923, 'Macro ROC value': 0.9055595605545923}\n",
            "Epoch   146: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 146 is 5.294509172439575\n",
            "{'Validation Epoch Loss': 1.5774092376232147, 'Epoch': 146, 'Micro F1': 0.8261132816234441, 'Patience Count': 12, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9053405148891824, 'Macro ROC value': 0.9053405148891824}\n",
            "Training Epoch Loss after 147 is 5.294553279876709\n",
            "{'Validation Epoch Loss': 1.5781316459178925, 'Epoch': 147, 'Micro F1': 0.8262152567542591, 'Patience Count': 13, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9051876570555805, 'Macro ROC value': 0.9051876570555805}\n",
            "Training Epoch Loss after 148 is 5.290355831384659\n",
            "{'Validation Epoch Loss': 1.5728400945663452, 'Epoch': 148, 'Micro F1': 0.8269736967896955, 'Patience Count': 14, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9059430115343055, 'Macro ROC value': 0.9059430115343055}\n",
            "Training Epoch Loss after 149 is 5.289312452077866\n",
            "{'Validation Epoch Loss': 1.5763505399227142, 'Epoch': 149, 'Micro F1': 0.8258688710571769, 'Patience Count': 15, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054575598646114, 'Macro ROC value': 0.9054575598646114}\n",
            "Training Epoch Loss after 150 is 5.301767110824585\n",
            "{'Validation Epoch Loss': 1.5756872296333313, 'Epoch': 150, 'Micro F1': 0.8262439372598007, 'Patience Count': 16, 'Best Val F1': 0.8270342445236168, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054220927271933, 'Macro ROC value': 0.9054220927271933}\n",
            "Training Epoch Loss after 151 is 5.296308517456055\n",
            "{'Validation Epoch Loss': 1.5738144516944885, 'Epoch': 151, 'Micro F1': 0.8271967673883532, 'Patience Count': 0, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9060495412327969, 'Macro ROC value': 0.9060495412327969}\n",
            "Training Epoch Loss after 152 is 5.293229013681412\n",
            "{'Validation Epoch Loss': 1.5757033824920654, 'Epoch': 152, 'Micro F1': 0.8265307423152178, 'Patience Count': 1, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.905506921793888, 'Macro ROC value': 0.905506921793888}\n",
            "Training Epoch Loss after 153 is 5.296783000230789\n",
            "{'Validation Epoch Loss': 1.5779154002666473, 'Epoch': 153, 'Micro F1': 0.8262248169227729, 'Patience Count': 2, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9052194120283523, 'Macro ROC value': 0.9052194120283523}\n",
            "Training Epoch Loss after 154 is 5.293492406606674\n",
            "{'Validation Epoch Loss': 1.5747250616550446, 'Epoch': 154, 'Micro F1': 0.8259411985901938, 'Patience Count': 3, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9054425516823867, 'Macro ROC value': 0.9054425516823867}\n",
            "Training Epoch Loss after 155 is 5.287559241056442\n",
            "{'Validation Epoch Loss': 1.5739317834377289, 'Epoch': 155, 'Micro F1': 0.826916335778612, 'Patience Count': 4, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9058475793394953, 'Macro ROC value': 0.9058475793394953}\n",
            "Training Epoch Loss after 156 is 5.277375757694244\n",
            "{'Validation Epoch Loss': 1.576088011264801, 'Epoch': 156, 'Micro F1': 0.8262023735165521, 'Patience Count': 5, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5701435804367065, 'Micro ROC value': 0.9055738835003542, 'Macro ROC value': 0.9055738835003542}\n",
            "Training Epoch Loss after 157 is 5.2883526384830475\n",
            "{'Validation Epoch Loss': 1.5692510604858398, 'Epoch': 157, 'Micro F1': 0.8270501781378067, 'Patience Count': 0, 'Best Val F1': 0.8271967673883532, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.906426361474625, 'Macro ROC value': 0.906426361474625}\n",
            "Training Epoch Loss after 158 is 5.275510251522064\n",
            "{'Validation Epoch Loss': 1.571705013513565, 'Epoch': 158, 'Micro F1': 0.827215887725381, 'Patience Count': 0, 'Best Val F1': 0.827215887725381, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9061298843809837, 'Macro ROC value': 0.9061298843809837}\n",
            "Training Epoch Loss after 159 is 5.298200219869614\n",
            "{'Validation Epoch Loss': 1.5735392570495605, 'Epoch': 159, 'Micro F1': 0.8264956883640002, 'Patience Count': 1, 'Best Val F1': 0.827215887725381, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9056821858427524, 'Macro ROC value': 0.9056821858427524}\n",
            "Training Epoch Loss after 160 is 5.29122120141983\n",
            "{'Validation Epoch Loss': 1.5776562690734863, 'Epoch': 160, 'Micro F1': 0.8264351406300788, 'Patience Count': 2, 'Best Val F1': 0.827215887725381, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.905390027878574, 'Macro ROC value': 0.905390027878574}\n",
            "Training Epoch Loss after 161 is 5.298629492521286\n",
            "{'Validation Epoch Loss': 1.578555852174759, 'Epoch': 161, 'Micro F1': 0.8259858127099254, 'Patience Count': 3, 'Best Val F1': 0.827215887725381, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9051927095328326, 'Macro ROC value': 0.9051927095328326}\n",
            "Training Epoch Loss after 162 is 5.2958859503269196\n",
            "{'Validation Epoch Loss': 1.5797256231307983, 'Epoch': 162, 'Micro F1': 0.8256034059693691, 'Patience Count': 4, 'Best Val F1': 0.827215887725381, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9049534471138078, 'Macro ROC value': 0.9049534471138078}\n",
            "Epoch   163: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 163 is 5.284813761711121\n",
            "{'Validation Epoch Loss': 1.573787897825241, 'Epoch': 163, 'Micro F1': 0.8272381947852466, 'Patience Count': 0, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9059337754995435, 'Macro ROC value': 0.9059337754995435}\n",
            "Training Epoch Loss after 164 is 5.283618599176407\n",
            "{'Validation Epoch Loss': 1.5755885243415833, 'Epoch': 164, 'Micro F1': 0.8259571322043837, 'Patience Count': 1, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.905273481941427, 'Macro ROC value': 0.905273481941427}\n",
            "Training Epoch Loss after 165 is 5.28704509139061\n",
            "{'Validation Epoch Loss': 1.5805011987686157, 'Epoch': 165, 'Micro F1': 0.8253612150336836, 'Patience Count': 2, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9048034810553438, 'Macro ROC value': 0.9048034810553438}\n",
            "Training Epoch Loss after 166 is 5.2930677235126495\n",
            "{'Validation Epoch Loss': 1.578963816165924, 'Epoch': 166, 'Micro F1': 0.8260527338895227, 'Patience Count': 3, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9052087549558013, 'Macro ROC value': 0.9052087549558013}\n",
            "Training Epoch Loss after 167 is 5.2780894339084625\n",
            "{'Validation Epoch Loss': 1.5757176578044891, 'Epoch': 167, 'Micro F1': 0.8261062158415525, 'Patience Count': 4, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9052530661368794, 'Macro ROC value': 0.9052530661368794}\n",
            "Training Epoch Loss after 168 is 5.278903990983963\n",
            "{'Validation Epoch Loss': 1.575423002243042, 'Epoch': 168, 'Micro F1': 0.8260782276722264, 'Patience Count': 5, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9056365132868907, 'Macro ROC value': 0.9056365132868907}\n",
            "Training Epoch Loss after 169 is 5.290531605482101\n",
            "{'Validation Epoch Loss': 1.5746239125728607, 'Epoch': 169, 'Micro F1': 0.8269131490557741, 'Patience Count': 6, 'Best Val F1': 0.8272381947852466, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9057584211303084, 'Macro ROC value': 0.9057584211303084}\n",
            "Training Epoch Loss after 170 is 5.2908176481723785\n",
            "{'Validation Epoch Loss': 1.5698535740375519, 'Epoch': 170, 'Micro F1': 0.8272477549537607, 'Patience Count': 0, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9061282350339337, 'Macro ROC value': 0.9061282350339337}\n",
            "Training Epoch Loss after 171 is 5.291098058223724\n",
            "{'Validation Epoch Loss': 1.572760671377182, 'Epoch': 171, 'Micro F1': 0.8265944767719774, 'Patience Count': 1, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9060965634556843, 'Macro ROC value': 0.9060965634556843}\n",
            "Training Epoch Loss after 172 is 5.294014781713486\n",
            "{'Validation Epoch Loss': 1.5717556178569794, 'Epoch': 172, 'Micro F1': 0.8268876552730703, 'Patience Count': 2, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5692510604858398, 'Micro ROC value': 0.9058133626237882, 'Macro ROC value': 0.9058133626237882}\n",
            "Training Epoch Loss after 173 is 5.2941555082798\n",
            "{'Validation Epoch Loss': 1.5680293440818787, 'Epoch': 173, 'Micro F1': 0.8266900784571163, 'Patience Count': 0, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.906376136402438, 'Macro ROC value': 0.906376136402438}\n",
            "Epoch   174: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Training Epoch Loss after 174 is 5.28871876001358\n",
            "{'Validation Epoch Loss': 1.576857715845108, 'Epoch': 174, 'Micro F1': 0.8265817298806254, 'Patience Count': 1, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9056082105548799, 'Macro ROC value': 0.9056082105548799}\n",
            "Training Epoch Loss after 175 is 5.283580839633942\n",
            "{'Validation Epoch Loss': 1.5770070850849152, 'Epoch': 175, 'Micro F1': 0.8261419621289858, 'Patience Count': 2, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9052287434407759, 'Macro ROC value': 0.9052287434407759}\n",
            "Training Epoch Loss after 176 is 5.279208451509476\n",
            "{'Validation Epoch Loss': 1.5779637098312378, 'Epoch': 176, 'Micro F1': 0.8262248169227729, 'Patience Count': 3, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9054133328089425, 'Macro ROC value': 0.9054133328089425}\n",
            "Training Epoch Loss after 177 is 5.278522372245789\n",
            "{'Validation Epoch Loss': 1.5783230662345886, 'Epoch': 177, 'Micro F1': 0.826428767184403, 'Patience Count': 4, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9052442416608161, 'Macro ROC value': 0.9052442416608161}\n",
            "Training Epoch Loss after 178 is 5.278705298900604\n",
            "{'Validation Epoch Loss': 1.5763896703720093, 'Epoch': 178, 'Micro F1': 0.8255970325236932, 'Patience Count': 5, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9053506865430565, 'Macro ROC value': 0.9053506865430565}\n",
            "Training Epoch Loss after 179 is 5.287456393241882\n",
            "{'Validation Epoch Loss': 1.5757773220539093, 'Epoch': 179, 'Micro F1': 0.8265944767719774, 'Patience Count': 6, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055078253022496, 'Macro ROC value': 0.9055078253022496}\n",
            "Training Epoch Loss after 180 is 5.284786224365234\n",
            "{'Validation Epoch Loss': 1.5721579194068909, 'Epoch': 180, 'Micro F1': 0.8271776470513253, 'Patience Count': 7, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9059413788824083, 'Macro ROC value': 0.9059413788824083}\n",
            "Training Epoch Loss after 181 is 5.27352899312973\n",
            "{'Validation Epoch Loss': 1.5746113955974579, 'Epoch': 181, 'Micro F1': 0.8261929496943933, 'Patience Count': 8, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9056029370598018, 'Macro ROC value': 0.9056029370598018}\n",
            "Training Epoch Loss after 182 is 5.282463520765305\n",
            "{'Validation Epoch Loss': 1.5799167156219482, 'Epoch': 182, 'Micro F1': 0.8253291947634769, 'Patience Count': 9, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9047896511078445, 'Macro ROC value': 0.9047896511078445}\n",
            "Training Epoch Loss after 183 is 5.2884208261966705\n",
            "{'Validation Epoch Loss': 1.5754302144050598, 'Epoch': 183, 'Micro F1': 0.8266231572775189, 'Patience Count': 10, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055140239362707, 'Macro ROC value': 0.9055140239362707}\n",
            "Training Epoch Loss after 184 is 5.300090849399567\n",
            "{'Validation Epoch Loss': 1.5748147666454315, 'Epoch': 184, 'Micro F1': 0.826212070031421, 'Patience Count': 11, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9056675647275474, 'Macro ROC value': 0.9056675647275474}\n",
            "Epoch   185: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Training Epoch Loss after 185 is 5.301552057266235\n",
            "{'Validation Epoch Loss': 1.574748545885086, 'Epoch': 185, 'Micro F1': 0.8263650327276435, 'Patience Count': 12, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055048186920823, 'Macro ROC value': 0.9055048186920823}\n",
            "Training Epoch Loss after 186 is 5.276376336812973\n",
            "{'Validation Epoch Loss': 1.5756868720054626, 'Epoch': 186, 'Micro F1': 0.8257754890026195, 'Patience Count': 13, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055109358204484, 'Macro ROC value': 0.9055109358204484}\n",
            "Training Epoch Loss after 187 is 5.283169478178024\n",
            "{'Validation Epoch Loss': 1.5742686688899994, 'Epoch': 187, 'Micro F1': 0.8263841530646713, 'Patience Count': 14, 'Best Val F1': 0.8272477549537607, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9057095303298603, 'Macro ROC value': 0.9057095303298603}\n",
            "Training Epoch Loss after 188 is 5.28983461856842\n",
            "{'Validation Epoch Loss': 1.5689189434051514, 'Epoch': 188, 'Micro F1': 0.8274293981555247, 'Patience Count': 0, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9064215053583674, 'Macro ROC value': 0.9064215053583674}\n",
            "Training Epoch Loss after 189 is 5.303728550672531\n",
            "{'Validation Epoch Loss': 1.5739747881889343, 'Epoch': 189, 'Micro F1': 0.8267697465280655, 'Patience Count': 1, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9056653320656689, 'Macro ROC value': 0.9056653320656689}\n",
            "Training Epoch Loss after 190 is 5.2882223427295685\n",
            "{'Validation Epoch Loss': 1.569627434015274, 'Epoch': 190, 'Micro F1': 0.8273720371444414, 'Patience Count': 2, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9059655387792709, 'Macro ROC value': 0.9059655387792709}\n",
            "Training Epoch Loss after 191 is 5.292465358972549\n",
            "{'Validation Epoch Loss': 1.57730171084404, 'Epoch': 191, 'Micro F1': 0.8264692478560024, 'Patience Count': 3, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.905321289059746, 'Macro ROC value': 0.905321289059746}\n",
            "Training Epoch Loss after 192 is 5.279966443777084\n",
            "{'Validation Epoch Loss': 1.5767791867256165, 'Epoch': 192, 'Micro F1': 0.8259475720358698, 'Patience Count': 4, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9053358522295313, 'Macro ROC value': 0.9053358522295313}\n",
            "Training Epoch Loss after 193 is 5.289306402206421\n",
            "{'Validation Epoch Loss': 1.57416170835495, 'Epoch': 193, 'Micro F1': 0.825922078253166, 'Patience Count': 5, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055666575178218, 'Macro ROC value': 0.9055666575178218}\n",
            "Training Epoch Loss after 194 is 5.27914097905159\n",
            "{'Validation Epoch Loss': 1.5722821652889252, 'Epoch': 194, 'Micro F1': 0.8271649001599735, 'Patience Count': 6, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9056104400077145, 'Macro ROC value': 0.9056104400077145}\n",
            "Training Epoch Loss after 195 is 5.29343968629837\n",
            "{'Validation Epoch Loss': 1.5755314230918884, 'Epoch': 195, 'Micro F1': 0.8266805182886023, 'Patience Count': 7, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055758853471804, 'Macro ROC value': 0.9055758853471804}\n",
            "Training Epoch Loss after 196 is 5.290748476982117\n",
            "{'Validation Epoch Loss': 1.5814174711704254, 'Epoch': 196, 'Micro F1': 0.8251700116634055, 'Patience Count': 8, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9046860152396489, 'Macro ROC value': 0.9046860152396489}\n",
            "Training Epoch Loss after 197 is 5.288796901702881\n",
            "{'Validation Epoch Loss': 1.5758931636810303, 'Epoch': 197, 'Micro F1': 0.8260718542265505, 'Patience Count': 9, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9054647114308858, 'Macro ROC value': 0.9054647114308858}\n",
            "Training Epoch Loss after 198 is 5.301569849252701\n",
            "{'Validation Epoch Loss': 1.5710084736347198, 'Epoch': 198, 'Micro F1': 0.8264925016411623, 'Patience Count': 10, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9062818246108031, 'Macro ROC value': 0.9062818246108031}\n",
            "Training Epoch Loss after 199 is 5.293634921312332\n",
            "{'Validation Epoch Loss': 1.5749830901622772, 'Epoch': 199, 'Micro F1': 0.8262184434770969, 'Patience Count': 11, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055522512257694, 'Macro ROC value': 0.9055522512257694}\n",
            "Epoch   200: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Training Epoch Loss after 200 is 5.286012232303619\n",
            "{'Validation Epoch Loss': 1.5720925629138947, 'Epoch': 200, 'Micro F1': 0.8264574476899448, 'Patience Count': 12, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9058286332657416, 'Macro ROC value': 0.9058286332657416}\n",
            "Training Epoch Loss after 201 is 5.283569723367691\n",
            "{'Validation Epoch Loss': 1.577284812927246, 'Epoch': 201, 'Micro F1': 0.8258615305192446, 'Patience Count': 13, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.905196788065239, 'Macro ROC value': 0.905196788065239}\n",
            "Training Epoch Loss after 202 is 5.287952899932861\n",
            "{'Validation Epoch Loss': 1.5726379454135895, 'Epoch': 202, 'Micro F1': 0.8270087507409131, 'Patience Count': 14, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9058211743101657, 'Macro ROC value': 0.9058211743101657}\n",
            "Training Epoch Loss after 203 is 5.289047390222549\n",
            "{'Validation Epoch Loss': 1.572288453578949, 'Epoch': 203, 'Micro F1': 0.8271712736056493, 'Patience Count': 15, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9058358026355644, 'Macro ROC value': 0.9058358026355644}\n",
            "Training Epoch Loss after 204 is 5.286536872386932\n",
            "{'Validation Epoch Loss': 1.5752975046634674, 'Epoch': 204, 'Micro F1': 0.8264701945812964, 'Patience Count': 16, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9055489226144429, 'Macro ROC value': 0.9055489226144429}\n",
            "Training Epoch Loss after 205 is 5.298919349908829\n",
            "{'Validation Epoch Loss': 1.5766239762306213, 'Epoch': 205, 'Micro F1': 0.8266008502176532, 'Patience Count': 17, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9054094953001111, 'Macro ROC value': 0.9054094953001111}\n",
            "Training Epoch Loss after 206 is 5.294592618942261\n",
            "{'Validation Epoch Loss': 1.5762002170085907, 'Epoch': 206, 'Micro F1': 0.8262970044614404, 'Patience Count': 18, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.905543156277378, 'Macro ROC value': 0.905543156277378}\n",
            "Training Epoch Loss after 207 is 5.291451841592789\n",
            "{'Validation Epoch Loss': 1.5768125355243683, 'Epoch': 207, 'Micro F1': 0.8256798873174804, 'Patience Count': 19, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9053915870271159, 'Macro ROC value': 0.9053915870271159}\n",
            "Training Epoch Loss after 208 is 5.279770374298096\n",
            "{'Validation Epoch Loss': 1.5759089887142181, 'Epoch': 208, 'Micro F1': 0.8266868917342783, 'Patience Count': 20, 'Best Val F1': 0.8274293981555247, 'Best Val Loss': 1.5680293440818787, 'Micro ROC value': 0.9058373713503071, 'Macro ROC value': 0.9058373713503071}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.8254119691655805, 'Micro Recall': 0.8254119691655805, 'Micro Precision': 0.8254119691655805, 'Micro ROC_AUC_Score': 0.9047367196137781, 'Macro ROC_AUC_Score': 0.9047367196137781}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Epoch Loss after 0 is 9.244023144245148\n",
            "{'Validation Epoch Loss': 2.2020846605300903, 'Epoch': 0, 'Micro F1': 0.7235386617503524, 'Patience Count': 0, 'Best Val F1': 0.7235386617503524, 'Best Val Loss': 2.2020846605300903, 'Micro ROC value': 0.8198001482980076, 'Macro ROC value': 0.8198001482980076}\n",
            "Training Epoch Loss after 1 is 7.89466592669487\n",
            "{'Validation Epoch Loss': 2.1907161474227905, 'Epoch': 1, 'Micro F1': 0.7284338888286399, 'Patience Count': 0, 'Best Val F1': 0.7284338888286399, 'Best Val Loss': 2.1907161474227905, 'Micro ROC value': 0.8427237922303721, 'Macro ROC value': 0.8427237922303721}\n",
            "Training Epoch Loss after 2 is 7.770985901355743\n",
            "{'Validation Epoch Loss': 2.2403229475021362, 'Epoch': 2, 'Micro F1': 0.7189115424935835, 'Patience Count': 1, 'Best Val F1': 0.7284338888286399, 'Best Val Loss': 2.1907161474227905, 'Micro ROC value': 0.8463180492783421, 'Macro ROC value': 0.8463180492783421}\n",
            "Training Epoch Loss after 3 is 7.63462570309639\n",
            "{'Validation Epoch Loss': 1.9678619801998138, 'Epoch': 3, 'Micro F1': 0.7642308016725108, 'Patience Count': 0, 'Best Val F1': 0.7642308016725108, 'Best Val Loss': 1.9678619801998138, 'Micro ROC value': 0.848422261360887, 'Macro ROC value': 0.848422261360887}\n",
            "Training Epoch Loss after 4 is 7.312804013490677\n",
            "{'Validation Epoch Loss': 2.017274349927902, 'Epoch': 4, 'Micro F1': 0.7537926713178855, 'Patience Count': 1, 'Best Val F1': 0.7642308016725108, 'Best Val Loss': 1.9678619801998138, 'Micro ROC value': 0.8524889046252995, 'Macro ROC value': 0.8524889046252995}\n",
            "Training Epoch Loss after 5 is 7.310435086488724\n",
            "{'Validation Epoch Loss': 1.9566178619861603, 'Epoch': 5, 'Micro F1': 0.7658906602079794, 'Patience Count': 0, 'Best Val F1': 0.7658906602079794, 'Best Val Loss': 1.9566178619861603, 'Micro ROC value': 0.8549067392055558, 'Macro ROC value': 0.8549067392055558}\n",
            "Training Epoch Loss after 6 is 7.240106076002121\n",
            "{'Validation Epoch Loss': 1.9286246597766876, 'Epoch': 6, 'Micro F1': 0.7709244598681751, 'Patience Count': 0, 'Best Val F1': 0.7709244598681751, 'Best Val Loss': 1.9286246597766876, 'Micro ROC value': 0.8556399129334572, 'Macro ROC value': 0.8556399129334572}\n",
            "Training Epoch Loss after 7 is 7.167046427726746\n",
            "{'Validation Epoch Loss': 1.9369450807571411, 'Epoch': 7, 'Micro F1': 0.7703646354248257, 'Patience Count': 1, 'Best Val F1': 0.7709244598681751, 'Best Val Loss': 1.9286246597766876, 'Micro ROC value': 0.8568067218802016, 'Macro ROC value': 0.8568067218802016}\n",
            "Training Epoch Loss after 8 is 7.126922965049744\n",
            "{'Validation Epoch Loss': 1.9691203832626343, 'Epoch': 8, 'Micro F1': 0.7644778223619998, 'Patience Count': 2, 'Best Val F1': 0.7709244598681751, 'Best Val Loss': 1.9286246597766876, 'Micro ROC value': 0.85766132006364, 'Macro ROC value': 0.85766132006364}\n",
            "Training Epoch Loss after 9 is 7.151012539863586\n",
            "{'Validation Epoch Loss': 1.965778112411499, 'Epoch': 9, 'Micro F1': 0.7649507766089482, 'Patience Count': 3, 'Best Val F1': 0.7709244598681751, 'Best Val Loss': 1.9286246597766876, 'Micro ROC value': 0.8579284094159175, 'Macro ROC value': 0.8579284094159175}\n",
            "Training Epoch Loss after 10 is 7.150389105081558\n",
            "{'Validation Epoch Loss': 1.9219213128089905, 'Epoch': 10, 'Micro F1': 0.7722822386773752, 'Patience Count': 0, 'Best Val F1': 0.7722822386773752, 'Best Val Loss': 1.9219213128089905, 'Micro ROC value': 0.8587788908911892, 'Macro ROC value': 0.8587788908911892}\n",
            "Training Epoch Loss after 11 is 7.10599285364151\n",
            "{'Validation Epoch Loss': 1.8877559304237366, 'Epoch': 11, 'Micro F1': 0.7782898938413525, 'Patience Count': 0, 'Best Val F1': 0.7782898938413525, 'Best Val Loss': 1.8877559304237366, 'Micro ROC value': 0.8599338675700661, 'Macro ROC value': 0.8599338675700661}\n",
            "Training Epoch Loss after 12 is 7.067950814962387\n",
            "{'Validation Epoch Loss': 1.883481115102768, 'Epoch': 12, 'Micro F1': 0.7799199884320624, 'Patience Count': 0, 'Best Val F1': 0.7799199884320624, 'Best Val Loss': 1.883481115102768, 'Micro ROC value': 0.8597825269044408, 'Macro ROC value': 0.8597825269044408}\n",
            "Training Epoch Loss after 13 is 7.026272475719452\n",
            "{'Validation Epoch Loss': 1.8775420188903809, 'Epoch': 13, 'Micro F1': 0.780760100736242, 'Patience Count': 0, 'Best Val F1': 0.780760100736242, 'Best Val Loss': 1.8775420188903809, 'Micro ROC value': 0.8606074265535474, 'Macro ROC value': 0.8606074265535474}\n",
            "Training Epoch Loss after 14 is 6.998343199491501\n",
            "{'Validation Epoch Loss': 1.891923874616623, 'Epoch': 14, 'Micro F1': 0.7777958524623745, 'Patience Count': 1, 'Best Val F1': 0.780760100736242, 'Best Val Loss': 1.8775420188903809, 'Micro ROC value': 0.8600779945345598, 'Macro ROC value': 0.8600779945345598}\n",
            "Training Epoch Loss after 15 is 6.981880784034729\n",
            "{'Validation Epoch Loss': 1.8924744427204132, 'Epoch': 15, 'Micro F1': 0.7783230307631132, 'Patience Count': 2, 'Best Val F1': 0.780760100736242, 'Best Val Loss': 1.8775420188903809, 'Micro ROC value': 0.86213409343321, 'Macro ROC value': 0.86213409343321}\n",
            "Training Epoch Loss after 16 is 6.982134461402893\n",
            "{'Validation Epoch Loss': 1.9455736577510834, 'Epoch': 16, 'Micro F1': 0.771740833122462, 'Patience Count': 3, 'Best Val F1': 0.780760100736242, 'Best Val Loss': 1.8775420188903809, 'Micro ROC value': 0.8624151076548627, 'Macro ROC value': 0.8624151076548627}\n",
            "Training Epoch Loss after 17 is 7.070745497941971\n",
            "{'Validation Epoch Loss': 2.025883734226227, 'Epoch': 17, 'Micro F1': 0.7631674077287351, 'Patience Count': 4, 'Best Val F1': 0.780760100736242, 'Best Val Loss': 1.8775420188903809, 'Micro ROC value': 0.8612317342775078, 'Macro ROC value': 0.8612317342775078}\n",
            "Training Epoch Loss after 18 is 7.111368924379349\n",
            "{'Validation Epoch Loss': 1.8682108521461487, 'Epoch': 18, 'Micro F1': 0.7827633782790492, 'Patience Count': 0, 'Best Val F1': 0.7827633782790492, 'Best Val Loss': 1.8682108521461487, 'Micro ROC value': 0.8623764364240617, 'Macro ROC value': 0.8623764364240617}\n",
            "Training Epoch Loss after 19 is 6.98879337310791\n",
            "{'Validation Epoch Loss': 1.8528391420841217, 'Epoch': 19, 'Micro F1': 0.7850498258805384, 'Patience Count': 0, 'Best Val F1': 0.7850498258805384, 'Best Val Loss': 1.8528391420841217, 'Micro ROC value': 0.8638208670534062, 'Macro ROC value': 0.8638208670534062}\n",
            "Training Epoch Loss after 20 is 6.892422944307327\n",
            "{'Validation Epoch Loss': 1.8686735332012177, 'Epoch': 20, 'Micro F1': 0.7823928472448156, 'Patience Count': 1, 'Best Val F1': 0.7850498258805384, 'Best Val Loss': 1.8528391420841217, 'Micro ROC value': 0.865186364510676, 'Macro ROC value': 0.865186364510676}\n",
            "Training Epoch Loss after 21 is 6.874224424362183\n",
            "{'Validation Epoch Loss': 1.889176070690155, 'Epoch': 21, 'Micro F1': 0.7798834785332996, 'Patience Count': 2, 'Best Val F1': 0.7850498258805384, 'Best Val Loss': 1.8528391420841217, 'Micro ROC value': 0.8663435448882852, 'Macro ROC value': 0.8663435448882852}\n",
            "Training Epoch Loss after 22 is 6.885247826576233\n",
            "{'Validation Epoch Loss': 1.8868597447872162, 'Epoch': 22, 'Micro F1': 0.7797358686090927, 'Patience Count': 3, 'Best Val F1': 0.7850498258805384, 'Best Val Loss': 1.8528391420841217, 'Micro ROC value': 0.8667319993839895, 'Macro ROC value': 0.8667319993839895}\n",
            "Training Epoch Loss after 23 is 6.85908430814743\n",
            "{'Validation Epoch Loss': 1.8897784650325775, 'Epoch': 23, 'Micro F1': 0.7796515200809746, 'Patience Count': 4, 'Best Val F1': 0.7850498258805384, 'Best Val Loss': 1.8528391420841217, 'Micro ROC value': 0.8668650788747787, 'Macro ROC value': 0.8668650788747787}\n",
            "Training Epoch Loss after 24 is 6.846479713916779\n",
            "{'Validation Epoch Loss': 1.8454679250717163, 'Epoch': 24, 'Micro F1': 0.7854896432057259, 'Patience Count': 0, 'Best Val F1': 0.7854896432057259, 'Best Val Loss': 1.8454679250717163, 'Micro ROC value': 0.8686176519603344, 'Macro ROC value': 0.8686176519603344}\n",
            "Training Epoch Loss after 25 is 6.803997218608856\n",
            "{'Validation Epoch Loss': 1.8365368247032166, 'Epoch': 25, 'Micro F1': 0.7861764812204027, 'Patience Count': 0, 'Best Val F1': 0.7861764812204027, 'Best Val Loss': 1.8365368247032166, 'Micro ROC value': 0.8693329509042045, 'Macro ROC value': 0.8693329509042045}\n",
            "Training Epoch Loss after 26 is 6.768710821866989\n",
            "{'Validation Epoch Loss': 1.8284713923931122, 'Epoch': 26, 'Micro F1': 0.7874989456433985, 'Patience Count': 0, 'Best Val F1': 0.7874989456433985, 'Best Val Loss': 1.8284713923931122, 'Micro ROC value': 0.8696596265603549, 'Macro ROC value': 0.8696596265603549}\n",
            "Training Epoch Loss after 27 is 6.740207880735397\n",
            "{'Validation Epoch Loss': 1.8243180811405182, 'Epoch': 27, 'Micro F1': 0.787709816963694, 'Patience Count': 0, 'Best Val F1': 0.787709816963694, 'Best Val Loss': 1.8243180811405182, 'Micro ROC value': 0.8697304044645338, 'Macro ROC value': 0.8697304044645338}\n",
            "Training Epoch Loss after 28 is 6.712186843156815\n",
            "{'Validation Epoch Loss': 1.8065036535263062, 'Epoch': 28, 'Micro F1': 0.7903005217558954, 'Patience Count': 0, 'Best Val F1': 0.7903005217558954, 'Best Val Loss': 1.8065036535263062, 'Micro ROC value': 0.8722505903847759, 'Macro ROC value': 0.8722505903847759}\n",
            "Training Epoch Loss after 29 is 6.6876868307590485\n",
            "{'Validation Epoch Loss': 1.8041077256202698, 'Epoch': 29, 'Micro F1': 0.7907463639759487, 'Patience Count': 0, 'Best Val F1': 0.7907463639759487, 'Best Val Loss': 1.8041077256202698, 'Micro ROC value': 0.8724779701086637, 'Macro ROC value': 0.8724779701086637}\n",
            "Training Epoch Loss after 30 is 6.655689716339111\n",
            "{'Validation Epoch Loss': 1.8035631775856018, 'Epoch': 30, 'Micro F1': 0.79047825615443, 'Patience Count': 0, 'Best Val F1': 0.7907463639759487, 'Best Val Loss': 1.8035631775856018, 'Micro ROC value': 0.8732569377455703, 'Macro ROC value': 0.8732569377455703}\n",
            "Training Epoch Loss after 31 is 6.632160514593124\n",
            "{'Validation Epoch Loss': 1.7897096574306488, 'Epoch': 31, 'Micro F1': 0.7926683054380701, 'Patience Count': 0, 'Best Val F1': 0.7926683054380701, 'Best Val Loss': 1.7897096574306488, 'Micro ROC value': 0.874640088778583, 'Macro ROC value': 0.874640088778583}\n",
            "Training Epoch Loss after 32 is 6.584351688623428\n",
            "{'Validation Epoch Loss': 1.7795626819133759, 'Epoch': 32, 'Micro F1': 0.794572172215595, 'Patience Count': 0, 'Best Val F1': 0.794572172215595, 'Best Val Loss': 1.7795626819133759, 'Micro ROC value': 0.8760889333009794, 'Macro ROC value': 0.8760889333009794}\n",
            "Training Epoch Loss after 33 is 6.571920871734619\n",
            "{'Validation Epoch Loss': 1.7761739492416382, 'Epoch': 33, 'Micro F1': 0.7951867114918845, 'Patience Count': 0, 'Best Val F1': 0.7951867114918845, 'Best Val Loss': 1.7761739492416382, 'Micro ROC value': 0.8763156770601125, 'Macro ROC value': 0.8763156770601125}\n",
            "Training Epoch Loss after 34 is 6.527199774980545\n",
            "{'Validation Epoch Loss': 1.764011412858963, 'Epoch': 34, 'Micro F1': 0.7969007940811433, 'Patience Count': 0, 'Best Val F1': 0.7969007940811433, 'Best Val Loss': 1.764011412858963, 'Micro ROC value': 0.8783624358616677, 'Macro ROC value': 0.8783624358616677}\n",
            "Training Epoch Loss after 35 is 6.493880420923233\n",
            "{'Validation Epoch Loss': 1.7572161853313446, 'Epoch': 35, 'Micro F1': 0.7981901215823783, 'Patience Count': 0, 'Best Val F1': 0.7981901215823783, 'Best Val Loss': 1.7572161853313446, 'Micro ROC value': 0.8799019914818269, 'Macro ROC value': 0.8799019914818269}\n",
            "Training Epoch Loss after 36 is 6.4715336561203\n",
            "{'Validation Epoch Loss': 1.753713220357895, 'Epoch': 36, 'Micro F1': 0.7979220137608599, 'Patience Count': 0, 'Best Val F1': 0.7981901215823783, 'Best Val Loss': 1.753713220357895, 'Micro ROC value': 0.8818024190585148, 'Macro ROC value': 0.8818024190585148}\n",
            "Training Epoch Loss after 37 is 6.455285578966141\n",
            "{'Validation Epoch Loss': 1.76534503698349, 'Epoch': 37, 'Micro F1': 0.7969520056875007, 'Patience Count': 1, 'Best Val F1': 0.7981901215823783, 'Best Val Loss': 1.753713220357895, 'Micro ROC value': 0.8814211324518653, 'Macro ROC value': 0.8814211324518653}\n",
            "Training Epoch Loss after 38 is 6.456697642803192\n",
            "{'Validation Epoch Loss': 1.7838801145553589, 'Epoch': 38, 'Micro F1': 0.792710479702129, 'Patience Count': 2, 'Best Val F1': 0.7981901215823783, 'Best Val Loss': 1.753713220357895, 'Micro ROC value': 0.8823987327356261, 'Macro ROC value': 0.8823987327356261}\n",
            "Training Epoch Loss after 39 is 6.457904100418091\n",
            "{'Validation Epoch Loss': 1.780171513557434, 'Epoch': 39, 'Micro F1': 0.7935298654038487, 'Patience Count': 3, 'Best Val F1': 0.7981901215823783, 'Best Val Loss': 1.753713220357895, 'Micro ROC value': 0.883045181017139, 'Macro ROC value': 0.883045181017139}\n",
            "Training Epoch Loss after 40 is 6.475975751876831\n",
            "{'Validation Epoch Loss': 1.7289826571941376, 'Epoch': 40, 'Micro F1': 0.8021304028244707, 'Patience Count': 0, 'Best Val F1': 0.8021304028244707, 'Best Val Loss': 1.7289826571941376, 'Micro ROC value': 0.8829512525620666, 'Macro ROC value': 0.8829512525620666}\n",
            "Training Epoch Loss after 41 is 6.488785743713379\n",
            "{'Validation Epoch Loss': 1.73223876953125, 'Epoch': 41, 'Micro F1': 0.8032992324283942, 'Patience Count': 0, 'Best Val F1': 0.8032992324283942, 'Best Val Loss': 1.7289826571941376, 'Micro ROC value': 0.8841319217642878, 'Macro ROC value': 0.8841319217642878}\n",
            "Training Epoch Loss after 42 is 6.39408341050148\n",
            "{'Validation Epoch Loss': 1.7376670837402344, 'Epoch': 42, 'Micro F1': 0.8019165190567424, 'Patience Count': 1, 'Best Val F1': 0.8032992324283942, 'Best Val Loss': 1.7289826571941376, 'Micro ROC value': 0.8852545072564169, 'Macro ROC value': 0.8852545072564169}\n",
            "Training Epoch Loss after 43 is 6.305079400539398\n",
            "{'Validation Epoch Loss': 1.7122197151184082, 'Epoch': 43, 'Micro F1': 0.8049651158587283, 'Patience Count': 0, 'Best Val F1': 0.8049651158587283, 'Best Val Loss': 1.7122197151184082, 'Micro ROC value': 0.8874104426392214, 'Macro ROC value': 0.8874104426392214}\n",
            "Training Epoch Loss after 44 is 6.244025468826294\n",
            "{'Validation Epoch Loss': 1.6916569769382477, 'Epoch': 44, 'Micro F1': 0.8076853558905397, 'Patience Count': 0, 'Best Val F1': 0.8076853558905397, 'Best Val Loss': 1.6916569769382477, 'Micro ROC value': 0.8893502732054711, 'Macro ROC value': 0.8893502732054711}\n",
            "Training Epoch Loss after 45 is 6.203195720911026\n",
            "{'Validation Epoch Loss': 1.7116305828094482, 'Epoch': 45, 'Micro F1': 0.8057453397438215, 'Patience Count': 1, 'Best Val F1': 0.8076853558905397, 'Best Val Loss': 1.6916569769382477, 'Micro ROC value': 0.8889158039554667, 'Macro ROC value': 0.8889158039554667}\n",
            "Training Epoch Loss after 46 is 6.1903135776519775\n",
            "{'Validation Epoch Loss': 1.7133193016052246, 'Epoch': 46, 'Micro F1': 0.8049319789369676, 'Patience Count': 2, 'Best Val F1': 0.8076853558905397, 'Best Val Loss': 1.6916569769382477, 'Micro ROC value': 0.8902412339089748, 'Macro ROC value': 0.8902412339089748}\n",
            "Training Epoch Loss after 47 is 6.17702317237854\n",
            "{'Validation Epoch Loss': 1.6892265975475311, 'Epoch': 47, 'Micro F1': 0.8083119449565606, 'Patience Count': 0, 'Best Val F1': 0.8083119449565606, 'Best Val Loss': 1.6892265975475311, 'Micro ROC value': 0.8917055192163916, 'Macro ROC value': 0.8917055192163916}\n",
            "Training Epoch Loss after 48 is 6.1305699944496155\n",
            "{'Validation Epoch Loss': 1.6816954910755157, 'Epoch': 48, 'Micro F1': 0.8099534833275087, 'Patience Count': 0, 'Best Val F1': 0.8099534833275087, 'Best Val Loss': 1.6816954910755157, 'Micro ROC value': 0.8919125673485826, 'Macro ROC value': 0.8919125673485826}\n",
            "Training Epoch Loss after 49 is 6.136418431997299\n",
            "{'Validation Epoch Loss': 1.67213773727417, 'Epoch': 49, 'Micro F1': 0.81119184470231, 'Patience Count': 0, 'Best Val F1': 0.81119184470231, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8921308393765235, 'Macro ROC value': 0.8921308393765235}\n",
            "Training Epoch Loss after 50 is 6.122702419757843\n",
            "{'Validation Epoch Loss': 1.6792627274990082, 'Epoch': 50, 'Micro F1': 0.8117642097145404, 'Patience Count': 0, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8920182537229044, 'Macro ROC value': 0.8920182537229044}\n",
            "Training Epoch Loss after 51 is 6.1149172484874725\n",
            "{'Validation Epoch Loss': 1.6850627064704895, 'Epoch': 51, 'Micro F1': 0.8096615214064514, 'Patience Count': 1, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8922708865109583, 'Macro ROC value': 0.8922708865109583}\n",
            "Training Epoch Loss after 52 is 6.100701183080673\n",
            "{'Validation Epoch Loss': 1.7106585502624512, 'Epoch': 52, 'Micro F1': 0.8048295557242527, 'Patience Count': 2, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8927010098747583, 'Macro ROC value': 0.8927010098747583}\n",
            "Training Epoch Loss after 53 is 6.10061514377594\n",
            "{'Validation Epoch Loss': 1.7411391139030457, 'Epoch': 53, 'Micro F1': 0.7991872416826326, 'Patience Count': 3, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.893928435281969, 'Macro ROC value': 0.893928435281969}\n",
            "Training Epoch Loss after 54 is 6.141498684883118\n",
            "{'Validation Epoch Loss': 1.701691895723343, 'Epoch': 54, 'Micro F1': 0.8044289002156912, 'Patience Count': 4, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8933771668214514, 'Macro ROC value': 0.8933771668214514}\n",
            "Training Epoch Loss after 55 is 6.141407012939453\n",
            "{'Validation Epoch Loss': 1.6731319427490234, 'Epoch': 55, 'Micro F1': 0.8105411560568268, 'Patience Count': 5, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8934053449241958, 'Macro ROC value': 0.8934053449241958}\n",
            "Training Epoch Loss after 56 is 6.107247978448868\n",
            "{'Validation Epoch Loss': 1.737881451845169, 'Epoch': 56, 'Micro F1': 0.8073931484895589, 'Patience Count': 6, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8919736575838199, 'Macro ROC value': 0.8919736575838199}\n",
            "Training Epoch Loss after 57 is 6.1038981676101685\n",
            "{'Validation Epoch Loss': 1.7252280116081238, 'Epoch': 57, 'Micro F1': 0.8047150827218065, 'Patience Count': 7, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8947853846303242, 'Macro ROC value': 0.8947853846303242}\n",
            "Training Epoch Loss after 58 is 6.149226903915405\n",
            "{'Validation Epoch Loss': 1.7334253191947937, 'Epoch': 58, 'Micro F1': 0.7989673330200388, 'Patience Count': 8, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8937268725146262, 'Macro ROC value': 0.8937268725146262}\n",
            "Training Epoch Loss after 59 is 6.167511582374573\n",
            "{'Validation Epoch Loss': 1.797672688961029, 'Epoch': 59, 'Micro F1': 0.7985998144332381, 'Patience Count': 9, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8927052190757295, 'Macro ROC value': 0.8927052190757295}\n",
            "Training Epoch Loss after 60 is 6.196524888277054\n",
            "{'Validation Epoch Loss': 1.7802183628082275, 'Epoch': 60, 'Micro F1': 0.8016664859198207, 'Patience Count': 10, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8919145203937556, 'Macro ROC value': 0.8919145203937556}\n",
            "Training Epoch Loss after 61 is 6.277371644973755\n",
            "{'Validation Epoch Loss': 1.8270418643951416, 'Epoch': 61, 'Micro F1': 0.7834713034257552, 'Patience Count': 11, 'Best Val F1': 0.8117642097145404, 'Best Val Loss': 1.67213773727417, 'Micro ROC value': 0.8913552950050354, 'Macro ROC value': 0.8913552950050354}\n",
            "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 62 is 6.267153561115265\n",
            "{'Validation Epoch Loss': 1.6528334021568298, 'Epoch': 62, 'Micro F1': 0.8136530142549012, 'Patience Count': 0, 'Best Val F1': 0.8136530142549012, 'Best Val Loss': 1.6528334021568298, 'Micro ROC value': 0.8943010668148605, 'Macro ROC value': 0.8943010668148605}\n",
            "Training Epoch Loss after 63 is 6.035852551460266\n",
            "{'Validation Epoch Loss': 1.6321145296096802, 'Epoch': 63, 'Micro F1': 0.8163280675752208, 'Patience Count': 0, 'Best Val F1': 0.8163280675752208, 'Best Val Loss': 1.6321145296096802, 'Micro ROC value': 0.896499229186542, 'Macro ROC value': 0.896499229186542}\n",
            "Training Epoch Loss after 64 is 5.96381339430809\n",
            "{'Validation Epoch Loss': 1.6251444518566132, 'Epoch': 64, 'Micro F1': 0.8165238766583524, 'Patience Count': 0, 'Best Val F1': 0.8165238766583524, 'Best Val Loss': 1.6251444518566132, 'Micro ROC value': 0.8973603943423517, 'Macro ROC value': 0.8973603943423517}\n",
            "Training Epoch Loss after 65 is 5.958260118961334\n",
            "{'Validation Epoch Loss': 1.6176255345344543, 'Epoch': 65, 'Micro F1': 0.8188012869175433, 'Patience Count': 0, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6176255345344543, 'Micro ROC value': 0.8983833740690466, 'Macro ROC value': 0.8983833740690466}\n",
            "Training Epoch Loss after 66 is 5.939402788877487\n",
            "{'Validation Epoch Loss': 1.6231555044651031, 'Epoch': 66, 'Micro F1': 0.8177951294749907, 'Patience Count': 1, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6176255345344543, 'Micro ROC value': 0.8975061603499135, 'Macro ROC value': 0.8975061603499135}\n",
            "Training Epoch Loss after 67 is 5.9450132846832275\n",
            "{'Validation Epoch Loss': 1.6190778017044067, 'Epoch': 67, 'Micro F1': 0.8180060007952861, 'Patience Count': 2, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6176255345344543, 'Micro ROC value': 0.8980990179469865, 'Macro ROC value': 0.8980990179469865}\n",
            "Training Epoch Loss after 68 is 5.913422077894211\n",
            "{'Validation Epoch Loss': 1.617870718240738, 'Epoch': 68, 'Micro F1': 0.8180550200028919, 'Patience Count': 3, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6176255345344543, 'Micro ROC value': 0.898224527171871, 'Macro ROC value': 0.898224527171871}\n",
            "Training Epoch Loss after 69 is 5.9230029582977295\n",
            "{'Validation Epoch Loss': 1.6180430948734283, 'Epoch': 69, 'Micro F1': 0.8186777765727988, 'Patience Count': 4, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6176255345344543, 'Micro ROC value': 0.898215857068644, 'Macro ROC value': 0.898215857068644}\n",
            "Training Epoch Loss after 70 is 5.917128145694733\n",
            "{'Validation Epoch Loss': 1.6162330508232117, 'Epoch': 70, 'Micro F1': 0.8185572786754871, 'Patience Count': 0, 'Best Val F1': 0.8188012869175433, 'Best Val Loss': 1.6162330508232117, 'Micro ROC value': 0.8983663194145631, 'Macro ROC value': 0.8983663194145631}\n",
            "Training Epoch Loss after 71 is 5.907872349023819\n",
            "{'Validation Epoch Loss': 1.6116800904273987, 'Epoch': 71, 'Micro F1': 0.8191567557146128, 'Patience Count': 0, 'Best Val F1': 0.8191567557146128, 'Best Val Loss': 1.6116800904273987, 'Micro ROC value': 0.8989855739774754, 'Macro ROC value': 0.8989855739774754}\n",
            "Training Epoch Loss after 72 is 5.896446108818054\n",
            "{'Validation Epoch Loss': 1.612672358751297, 'Epoch': 72, 'Micro F1': 0.8190452951595995, 'Patience Count': 1, 'Best Val F1': 0.8191567557146128, 'Best Val Loss': 1.6116800904273987, 'Micro ROC value': 0.8989383179847732, 'Macro ROC value': 0.8989383179847732}\n",
            "Training Epoch Loss after 73 is 5.89667734503746\n",
            "{'Validation Epoch Loss': 1.6150648295879364, 'Epoch': 73, 'Micro F1': 0.8184819674896673, 'Patience Count': 2, 'Best Val F1': 0.8191567557146128, 'Best Val Loss': 1.6116800904273987, 'Micro ROC value': 0.8986761933683076, 'Macro ROC value': 0.8986761933683076}\n",
            "Training Epoch Loss after 74 is 5.890939235687256\n",
            "{'Validation Epoch Loss': 1.6114700138568878, 'Epoch': 74, 'Micro F1': 0.819524862477632, 'Patience Count': 0, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.6114700138568878, 'Micro ROC value': 0.8991084017542826, 'Macro ROC value': 0.8991084017542826}\n",
            "Training Epoch Loss after 75 is 5.87804302573204\n",
            "{'Validation Epoch Loss': 1.6156435310840607, 'Epoch': 75, 'Micro F1': 0.8188916603405271, 'Patience Count': 1, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.6114700138568878, 'Micro ROC value': 0.8987187976345633, 'Macro ROC value': 0.8987187976345633}\n",
            "Training Epoch Loss after 76 is 5.87721586227417\n",
            "{'Validation Epoch Loss': 1.610511064529419, 'Epoch': 76, 'Micro F1': 0.819126631240285, 'Patience Count': 0, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.610511064529419, 'Micro ROC value': 0.8992342595551752, 'Macro ROC value': 0.8992342595551752}\n",
            "Training Epoch Loss after 77 is 5.8878165781497955\n",
            "{'Validation Epoch Loss': 1.6138994097709656, 'Epoch': 77, 'Micro F1': 0.8189910711058092, 'Patience Count': 1, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.610511064529419, 'Micro ROC value': 0.8988644975954427, 'Macro ROC value': 0.8988644975954427}\n",
            "Training Epoch Loss after 78 is 5.894184619188309\n",
            "{'Validation Epoch Loss': 1.6137252151966095, 'Epoch': 78, 'Micro F1': 0.8194218510886985, 'Patience Count': 2, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.610511064529419, 'Micro ROC value': 0.8988049105111335, 'Macro ROC value': 0.8988049105111335}\n",
            "Training Epoch Loss after 79 is 5.891868591308594\n",
            "{'Validation Epoch Loss': 1.6112555861473083, 'Epoch': 79, 'Micro F1': 0.8191989299786718, 'Patience Count': 3, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.610511064529419, 'Micro ROC value': 0.8991791961876261, 'Macro ROC value': 0.8991791961876261}\n",
            "Training Epoch Loss after 80 is 5.8707530200481415\n",
            "{'Validation Epoch Loss': 1.6117836236953735, 'Epoch': 80, 'Micro F1': 0.8190844569762258, 'Patience Count': 4, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.610511064529419, 'Micro ROC value': 0.8990913409600148, 'Macro ROC value': 0.8990913409600148}\n",
            "Training Epoch Loss after 81 is 5.8710538148880005\n",
            "{'Validation Epoch Loss': 1.6092917025089264, 'Epoch': 81, 'Micro F1': 0.8192923158490885, 'Patience Count': 0, 'Best Val F1': 0.819524862477632, 'Best Val Loss': 1.6092917025089264, 'Micro ROC value': 0.8993589732236421, 'Macro ROC value': 0.8993589732236421}\n",
            "Training Epoch Loss after 82 is 5.868140518665314\n",
            "{'Validation Epoch Loss': 1.6086629927158356, 'Epoch': 82, 'Micro F1': 0.8199158924676765, 'Patience Count': 0, 'Best Val F1': 0.8199158924676765, 'Best Val Loss': 1.6086629927158356, 'Micro ROC value': 0.8995169504623303, 'Macro ROC value': 0.8995169504623303}\n",
            "Training Epoch Loss after 83 is 5.865843266248703\n",
            "{'Validation Epoch Loss': 1.6095019578933716, 'Epoch': 83, 'Micro F1': 0.8197803323332008, 'Patience Count': 1, 'Best Val F1': 0.8199158924676765, 'Best Val Loss': 1.6086629927158356, 'Micro ROC value': 0.8994893983783163, 'Macro ROC value': 0.8994893983783163}\n",
            "Training Epoch Loss after 84 is 5.8557029366493225\n",
            "{'Validation Epoch Loss': 1.6123376488685608, 'Epoch': 84, 'Micro F1': 0.8197953945703648, 'Patience Count': 2, 'Best Val F1': 0.8199158924676765, 'Best Val Loss': 1.6086629927158356, 'Micro ROC value': 0.8991389671698704, 'Macro ROC value': 0.8991389671698704}\n",
            "Training Epoch Loss after 85 is 5.850004881620407\n",
            "{'Validation Epoch Loss': 1.6082699596881866, 'Epoch': 85, 'Micro F1': 0.8201809878417623, 'Patience Count': 0, 'Best Val F1': 0.8201809878417623, 'Best Val Loss': 1.6082699596881866, 'Micro ROC value': 0.899572782704734, 'Macro ROC value': 0.899572782704734}\n",
            "Training Epoch Loss after 86 is 5.854804039001465\n",
            "{'Validation Epoch Loss': 1.607738435268402, 'Epoch': 86, 'Micro F1': 0.8197291207268433, 'Patience Count': 0, 'Best Val F1': 0.8201809878417623, 'Best Val Loss': 1.607738435268402, 'Micro ROC value': 0.8997225936768211, 'Macro ROC value': 0.8997225936768211}\n",
            "Training Epoch Loss after 87 is 5.850653141736984\n",
            "{'Validation Epoch Loss': 1.6117576658725739, 'Epoch': 87, 'Micro F1': 0.8193164154285508, 'Patience Count': 1, 'Best Val F1': 0.8201809878417623, 'Best Val Loss': 1.607738435268402, 'Micro ROC value': 0.899139848917579, 'Macro ROC value': 0.899139848917579}\n",
            "Training Epoch Loss after 88 is 5.855191916227341\n",
            "{'Validation Epoch Loss': 1.6110659837722778, 'Epoch': 88, 'Micro F1': 0.8194790875899215, 'Patience Count': 2, 'Best Val F1': 0.8201809878417623, 'Best Val Loss': 1.607738435268402, 'Micro ROC value': 0.899148349192724, 'Macro ROC value': 0.899148349192724}\n",
            "Training Epoch Loss after 89 is 5.85966283082962\n",
            "{'Validation Epoch Loss': 1.6025586426258087, 'Epoch': 89, 'Micro F1': 0.8203255853185362, 'Patience Count': 0, 'Best Val F1': 0.8203255853185362, 'Best Val Loss': 1.6025586426258087, 'Micro ROC value': 0.900168491670838, 'Macro ROC value': 0.900168491670838}\n",
            "Training Epoch Loss after 90 is 5.846590548753738\n",
            "{'Validation Epoch Loss': 1.6062991321086884, 'Epoch': 90, 'Micro F1': 0.8202382243429852, 'Patience Count': 1, 'Best Val F1': 0.8203255853185362, 'Best Val Loss': 1.6025586426258087, 'Micro ROC value': 0.8998563547425085, 'Macro ROC value': 0.8998563547425085}\n",
            "Training Epoch Loss after 91 is 5.845645934343338\n",
            "{'Validation Epoch Loss': 1.6112908124923706, 'Epoch': 91, 'Micro F1': 0.8195122245116823, 'Patience Count': 2, 'Best Val F1': 0.8203255853185362, 'Best Val Loss': 1.6025586426258087, 'Micro ROC value': 0.8992263001949157, 'Macro ROC value': 0.8992263001949157}\n",
            "Training Epoch Loss after 92 is 5.8340029418468475\n",
            "{'Validation Epoch Loss': 1.6051506400108337, 'Epoch': 92, 'Micro F1': 0.8203978840569232, 'Patience Count': 0, 'Best Val F1': 0.8203978840569232, 'Best Val Loss': 1.6025586426258087, 'Micro ROC value': 0.8999221398514718, 'Macro ROC value': 0.8999221398514718}\n",
            "Training Epoch Loss after 93 is 5.835500985383987\n",
            "{'Validation Epoch Loss': 1.600306659936905, 'Epoch': 93, 'Micro F1': 0.8206147802720842, 'Patience Count': 0, 'Best Val F1': 0.8206147802720842, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.9005158093199892, 'Macro ROC value': 0.9005158093199892}\n",
            "Training Epoch Loss after 94 is 5.845873802900314\n",
            "{'Validation Epoch Loss': 1.606379896402359, 'Epoch': 94, 'Micro F1': 0.8203044981865066, 'Patience Count': 1, 'Best Val F1': 0.8206147802720842, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.8998569326464398, 'Macro ROC value': 0.8998569326464398}\n",
            "Training Epoch Loss after 95 is 5.832439541816711\n",
            "{'Validation Epoch Loss': 1.608001708984375, 'Epoch': 95, 'Micro F1': 0.8201147139982408, 'Patience Count': 2, 'Best Val F1': 0.8206147802720842, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.8996333966797352, 'Macro ROC value': 0.8996333966797352}\n",
            "Training Epoch Loss after 96 is 5.837592154741287\n",
            "{'Validation Epoch Loss': 1.607194721698761, 'Epoch': 96, 'Micro F1': 0.8196869464627842, 'Patience Count': 3, 'Best Val F1': 0.8206147802720842, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.8996589545859109, 'Macro ROC value': 0.8996589545859109}\n",
            "Training Epoch Loss after 97 is 5.84059077501297\n",
            "{'Validation Epoch Loss': 1.6044252812862396, 'Epoch': 97, 'Micro F1': 0.8203255853185362, 'Patience Count': 4, 'Best Val F1': 0.8206147802720842, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.900017967724908, 'Macro ROC value': 0.900017967724908}\n",
            "Training Epoch Loss after 98 is 5.816955596208572\n",
            "{'Validation Epoch Loss': 1.6023415923118591, 'Epoch': 98, 'Micro F1': 0.8210455602549735, 'Patience Count': 0, 'Best Val F1': 0.8210455602549735, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.9002848696454497, 'Macro ROC value': 0.9002848696454497}\n",
            "Training Epoch Loss after 99 is 5.817487150430679\n",
            "{'Validation Epoch Loss': 1.60315802693367, 'Epoch': 99, 'Micro F1': 0.8209973610960489, 'Patience Count': 1, 'Best Val F1': 0.8210455602549735, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.900264638234489, 'Macro ROC value': 0.900264638234489}\n",
            "Training Epoch Loss after 100 is 5.814166963100433\n",
            "{'Validation Epoch Loss': 1.6044013798236847, 'Epoch': 100, 'Micro F1': 0.8204521081107135, 'Patience Count': 2, 'Best Val F1': 0.8210455602549735, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.9000710235746049, 'Macro ROC value': 0.9000710235746049}\n",
            "Training Epoch Loss after 101 is 5.821993410587311\n",
            "{'Validation Epoch Loss': 1.6059688031673431, 'Epoch': 101, 'Micro F1': 0.8203587222402969, 'Patience Count': 3, 'Best Val F1': 0.8210455602549735, 'Best Val Loss': 1.600306659936905, 'Micro ROC value': 0.8998785188946439, 'Macro ROC value': 0.8998785188946439}\n",
            "Training Epoch Loss after 102 is 5.814622700214386\n",
            "{'Validation Epoch Loss': 1.598448395729065, 'Epoch': 102, 'Micro F1': 0.8220788297244213, 'Patience Count': 0, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.598448395729065, 'Micro ROC value': 0.9009714390267265, 'Macro ROC value': 0.9009714390267265}\n",
            "Training Epoch Loss after 103 is 5.816970080137253\n",
            "{'Validation Epoch Loss': 1.5982707738876343, 'Epoch': 103, 'Micro F1': 0.8217564978491125, 'Patience Count': 0, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9009443409387848, 'Macro ROC value': 0.9009443409387848}\n",
            "Training Epoch Loss after 104 is 5.813991874456406\n",
            "{'Validation Epoch Loss': 1.603259563446045, 'Epoch': 104, 'Micro F1': 0.8206087553772187, 'Patience Count': 1, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9002430925508178, 'Macro ROC value': 0.9002430925508178}\n",
            "Training Epoch Loss after 105 is 5.811759382486343\n",
            "{'Validation Epoch Loss': 1.601833462715149, 'Epoch': 105, 'Micro F1': 0.8210847220715999, 'Patience Count': 2, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9005287862316267, 'Macro ROC value': 0.9005287862316267}\n",
            "Training Epoch Loss after 106 is 5.805894434452057\n",
            "{'Validation Epoch Loss': 1.6039976179599762, 'Epoch': 106, 'Micro F1': 0.8207292532745304, 'Patience Count': 3, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9001770806071653, 'Macro ROC value': 0.9001770806071653}\n",
            "Training Epoch Loss after 107 is 5.797441214323044\n",
            "{'Validation Epoch Loss': 1.601730853319168, 'Epoch': 107, 'Micro F1': 0.8213618672354168, 'Patience Count': 4, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.900424854889479, 'Macro ROC value': 0.900424854889479}\n",
            "Training Epoch Loss after 108 is 5.795829355716705\n",
            "{'Validation Epoch Loss': 1.6011938154697418, 'Epoch': 108, 'Micro F1': 0.8213046307341937, 'Patience Count': 5, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9005615016089412, 'Macro ROC value': 0.9005615016089412}\n",
            "Training Epoch Loss after 109 is 5.801815629005432\n",
            "{'Validation Epoch Loss': 1.6037988066673279, 'Epoch': 109, 'Micro F1': 0.820632854956681, 'Patience Count': 6, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9003268709249176, 'Macro ROC value': 0.9003268709249176}\n",
            "Training Epoch Loss after 110 is 5.792920678853989\n",
            "{'Validation Epoch Loss': 1.5998295545578003, 'Epoch': 110, 'Micro F1': 0.8213106556290592, 'Patience Count': 7, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5982707738876343, 'Micro ROC value': 0.9007738525999726, 'Macro ROC value': 0.9007738525999726}\n",
            "Training Epoch Loss after 111 is 5.794005036354065\n",
            "{'Validation Epoch Loss': 1.5970653891563416, 'Epoch': 111, 'Micro F1': 0.8215420764862708, 'Patience Count': 0, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5970653891563416, 'Micro ROC value': 0.9010642851020172, 'Macro ROC value': 0.9010642851020172}\n",
            "Training Epoch Loss after 112 is 5.788094162940979\n",
            "{'Validation Epoch Loss': 1.5990558564662933, 'Epoch': 112, 'Micro F1': 0.8217384231645157, 'Patience Count': 1, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5970653891563416, 'Micro ROC value': 0.9008159947572494, 'Macro ROC value': 0.9008159947572494}\n",
            "Training Epoch Loss after 113 is 5.79720276594162\n",
            "{'Validation Epoch Loss': 1.5946872532367706, 'Epoch': 113, 'Micro F1': 0.8215847883454434, 'Patience Count': 0, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5946872532367706, 'Micro ROC value': 0.9012841642950529, 'Macro ROC value': 0.9012841642950529}\n",
            "Epoch   114: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 114 is 5.790148973464966\n",
            "{'Validation Epoch Loss': 1.597523421049118, 'Epoch': 114, 'Micro F1': 0.8214281410789381, 'Patience Count': 1, 'Best Val F1': 0.8220788297244213, 'Best Val Loss': 1.5946872532367706, 'Micro ROC value': 0.9010397105872736, 'Macro ROC value': 0.9010397105872736}\n",
            "Training Epoch Loss after 115 is 5.791211903095245\n",
            "{'Validation Epoch Loss': 1.593300998210907, 'Epoch': 115, 'Micro F1': 0.8222656014652544, 'Patience Count': 0, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.593300998210907, 'Micro ROC value': 0.9014597290083527, 'Macro ROC value': 0.9014597290083527}\n",
            "Training Epoch Loss after 116 is 5.791383117437363\n",
            "{'Validation Epoch Loss': 1.5975687503814697, 'Epoch': 116, 'Micro F1': 0.8216359999518008, 'Patience Count': 1, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.593300998210907, 'Micro ROC value': 0.9009386293436794, 'Macro ROC value': 0.9009386293436794}\n",
            "Training Epoch Loss after 117 is 5.782021373510361\n",
            "{'Validation Epoch Loss': 1.600027322769165, 'Epoch': 117, 'Micro F1': 0.8213166805239248, 'Patience Count': 2, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.593300998210907, 'Micro ROC value': 0.9007287601216144, 'Macro ROC value': 0.9007287601216144}\n",
            "Training Epoch Loss after 118 is 5.787510633468628\n",
            "{'Validation Epoch Loss': 1.5928311944007874, 'Epoch': 118, 'Micro F1': 0.8221752280422706, 'Patience Count': 0, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9015480059982064, 'Macro ROC value': 0.9015480059982064}\n",
            "Training Epoch Loss after 119 is 5.78749805688858\n",
            "{'Validation Epoch Loss': 1.5963298380374908, 'Epoch': 119, 'Micro F1': 0.821690224005591, 'Patience Count': 1, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9012250997218543, 'Macro ROC value': 0.9012250997218543}\n",
            "Training Epoch Loss after 120 is 5.793990850448608\n",
            "{'Validation Epoch Loss': 1.5940613448619843, 'Epoch': 120, 'Micro F1': 0.8220306305654966, 'Patience Count': 2, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9014160631310689, 'Macro ROC value': 0.9014160631310689}\n",
            "Training Epoch Loss after 121 is 5.785590648651123\n",
            "{'Validation Epoch Loss': 1.6023444831371307, 'Epoch': 121, 'Micro F1': 0.8213648796828495, 'Patience Count': 3, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9005402321452356, 'Macro ROC value': 0.9005402321452356}\n",
            "Training Epoch Loss after 122 is 5.7820901572704315\n",
            "{'Validation Epoch Loss': 1.6025875508785248, 'Epoch': 122, 'Micro F1': 0.8209792864114521, 'Patience Count': 4, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9004614621569258, 'Macro ROC value': 0.9004614621569258}\n",
            "Training Epoch Loss after 123 is 5.78333255648613\n",
            "{'Validation Epoch Loss': 1.5964834988117218, 'Epoch': 123, 'Micro F1': 0.8222053525165985, 'Patience Count': 5, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9013255067774698, 'Macro ROC value': 0.9013255067774698}\n",
            "Training Epoch Loss after 124 is 5.7818520069122314\n",
            "{'Validation Epoch Loss': 1.5985351204872131, 'Epoch': 124, 'Micro F1': 0.821648049741532, 'Patience Count': 6, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9011323870774837, 'Macro ROC value': 0.9011323870774837}\n",
            "Training Epoch Loss after 125 is 5.791137397289276\n",
            "{'Validation Epoch Loss': 1.5974890291690826, 'Epoch': 125, 'Micro F1': 0.8220607550398246, 'Patience Count': 7, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9010052784318157, 'Macro ROC value': 0.9010052784318157}\n",
            "Training Epoch Loss after 126 is 5.79026472568512\n",
            "{'Validation Epoch Loss': 1.5953661799430847, 'Epoch': 126, 'Micro F1': 0.8217384231645157, 'Patience Count': 8, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9012405987491948, 'Macro ROC value': 0.9012405987491948}\n",
            "Epoch   127: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 127 is 5.781203925609589\n",
            "{'Validation Epoch Loss': 1.598548799753189, 'Epoch': 127, 'Micro F1': 0.8215426140813843, 'Patience Count': 9, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9009685069460727, 'Macro ROC value': 0.9009685069460727}\n",
            "Training Epoch Loss after 128 is 5.7767945528030396\n",
            "{'Validation Epoch Loss': 1.6048092246055603, 'Epoch': 128, 'Micro F1': 0.8207744399860223, 'Patience Count': 10, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9001765413076014, 'Macro ROC value': 0.9001765413076014}\n",
            "Training Epoch Loss after 129 is 5.78095468878746\n",
            "{'Validation Epoch Loss': 1.5943576395511627, 'Epoch': 129, 'Micro F1': 0.8220758172769885, 'Patience Count': 11, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9014012915247159, 'Macro ROC value': 0.9014012915247159}\n",
            "Training Epoch Loss after 130 is 5.778601586818695\n",
            "{'Validation Epoch Loss': 1.5984511375427246, 'Epoch': 130, 'Micro F1': 0.8211299087830918, 'Patience Count': 12, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9009340452066379, 'Macro ROC value': 0.9009340452066379}\n",
            "Training Epoch Loss after 131 is 5.779877126216888\n",
            "{'Validation Epoch Loss': 1.601298213005066, 'Epoch': 131, 'Micro F1': 0.8206402713637172, 'Patience Count': 13, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.90044412758888, 'Macro ROC value': 0.90044412758888}\n",
            "Training Epoch Loss after 132 is 5.77968692779541\n",
            "{'Validation Epoch Loss': 1.5938834249973297, 'Epoch': 132, 'Micro F1': 0.8220366554603622, 'Patience Count': 14, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9014473857020115, 'Macro ROC value': 0.9014473857020115}\n",
            "Training Epoch Loss after 133 is 5.776368230581284\n",
            "{'Validation Epoch Loss': 1.595386654138565, 'Epoch': 133, 'Micro F1': 0.822127028883346, 'Patience Count': 15, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9012846837751671, 'Macro ROC value': 0.9012846837751671}\n",
            "Training Epoch Loss after 134 is 5.78628408908844\n",
            "{'Validation Epoch Loss': 1.6002372801303864, 'Epoch': 134, 'Micro F1': 0.821488713695696, 'Patience Count': 16, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9008037873577048, 'Macro ROC value': 0.9008037873577048}\n",
            "Training Epoch Loss after 135 is 5.778554826974869\n",
            "{'Validation Epoch Loss': 1.5977160036563873, 'Epoch': 135, 'Micro F1': 0.8216359999518008, 'Patience Count': 17, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9009729514213337, 'Macro ROC value': 0.9009729514213337}\n",
            "Training Epoch Loss after 136 is 5.785645663738251\n",
            "{'Validation Epoch Loss': 1.601483553647995, 'Epoch': 136, 'Micro F1': 0.8213136680764921, 'Patience Count': 18, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9004490995393262, 'Macro ROC value': 0.9004490995393262}\n",
            "Training Epoch Loss after 137 is 5.781389743089676\n",
            "{'Validation Epoch Loss': 1.600712925195694, 'Epoch': 137, 'Micro F1': 0.8209762739640193, 'Patience Count': 19, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.900634943650062, 'Macro ROC value': 0.900634943650062}\n",
            "Epoch   138: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Training Epoch Loss after 138 is 5.792720198631287\n",
            "{'Validation Epoch Loss': 1.5940090417861938, 'Epoch': 138, 'Micro F1': 0.8220456928026606, 'Patience Count': 20, 'Best Val F1': 0.8222656014652544, 'Best Val Loss': 1.5928311944007874, 'Micro ROC value': 0.9014114811356895, 'Macro ROC value': 0.9014114811356895}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.8158700707335056, 'Micro Recall': 0.8158700707335056, 'Micro Precision': 0.8158700707335056, 'Micro ROC_AUC_Score': 0.8959582634312191, 'Macro ROC_AUC_Score': 0.8959582634312191}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Epoch Loss after 0 is 9.224603533744812\n",
            "{'Validation Epoch Loss': 2.2383241653442383, 'Epoch': 0, 'Micro F1': 0.7286032522016503, 'Patience Count': 0, 'Best Val F1': 0.7286032522016503, 'Best Val Loss': 2.2383241653442383, 'Micro ROC value': 0.8069564193430604, 'Macro ROC value': 0.8069564193430604}\n",
            "Training Epoch Loss after 1 is 7.9793999791145325\n",
            "{'Validation Epoch Loss': 2.047705799341202, 'Epoch': 1, 'Micro F1': 0.7532676906008389, 'Patience Count': 0, 'Best Val F1': 0.7532676906008389, 'Best Val Loss': 2.047705799341202, 'Micro ROC value': 0.8363110127174828, 'Macro ROC value': 0.8363110127174828}\n",
            "Training Epoch Loss after 2 is 7.4495594799518585\n",
            "{'Validation Epoch Loss': 1.9754536151885986, 'Epoch': 2, 'Micro F1': 0.7628460724996966, 'Patience Count': 0, 'Best Val F1': 0.7628460724996966, 'Best Val Loss': 1.9754536151885986, 'Micro ROC value': 0.8448187141415024, 'Macro ROC value': 0.8448187141415024}\n",
            "Training Epoch Loss after 3 is 7.349809378385544\n",
            "{'Validation Epoch Loss': 1.9593524932861328, 'Epoch': 3, 'Micro F1': 0.7689734077592484, 'Patience Count': 0, 'Best Val F1': 0.7689734077592484, 'Best Val Loss': 1.9593524932861328, 'Micro ROC value': 0.8481095122378968, 'Macro ROC value': 0.8481095122378968}\n",
            "Training Epoch Loss after 4 is 7.3217713832855225\n",
            "{'Validation Epoch Loss': 1.9645685255527496, 'Epoch': 4, 'Micro F1': 0.7700741947786292, 'Patience Count': 0, 'Best Val F1': 0.7700741947786292, 'Best Val Loss': 1.9593524932861328, 'Micro ROC value': 0.8485885931483067, 'Macro ROC value': 0.8485885931483067}\n",
            "Training Epoch Loss after 5 is 7.301183342933655\n",
            "{'Validation Epoch Loss': 1.959177702665329, 'Epoch': 5, 'Micro F1': 0.7707589363103698, 'Patience Count': 0, 'Best Val F1': 0.7707589363103698, 'Best Val Loss': 1.959177702665329, 'Micro ROC value': 0.8497627659865093, 'Macro ROC value': 0.8497627659865093}\n",
            "Training Epoch Loss after 6 is 7.279932141304016\n",
            "{'Validation Epoch Loss': 1.9384936392307281, 'Epoch': 6, 'Micro F1': 0.7710864260669443, 'Patience Count': 0, 'Best Val F1': 0.7710864260669443, 'Best Val Loss': 1.9384936392307281, 'Micro ROC value': 0.8516020761650344, 'Macro ROC value': 0.8516020761650344}\n",
            "Training Epoch Loss after 7 is 7.202360302209854\n",
            "{'Validation Epoch Loss': 1.9407278895378113, 'Epoch': 7, 'Micro F1': 0.7702692161009604, 'Patience Count': 1, 'Best Val F1': 0.7710864260669443, 'Best Val Loss': 1.9384936392307281, 'Micro ROC value': 0.8531799506215902, 'Macro ROC value': 0.8531799506215902}\n",
            "Training Epoch Loss after 8 is 7.138499438762665\n",
            "{'Validation Epoch Loss': 1.9292299151420593, 'Epoch': 8, 'Micro F1': 0.7735369067017994, 'Patience Count': 0, 'Best Val F1': 0.7735369067017994, 'Best Val Loss': 1.9292299151420593, 'Micro ROC value': 0.853723733997277, 'Macro ROC value': 0.853723733997277}\n",
            "Training Epoch Loss after 9 is 7.110165774822235\n",
            "{'Validation Epoch Loss': 1.9208557307720184, 'Epoch': 9, 'Micro F1': 0.775682141247443, 'Patience Count': 0, 'Best Val F1': 0.775682141247443, 'Best Val Loss': 1.9208557307720184, 'Micro ROC value': 0.8547078758598667, 'Macro ROC value': 0.8547078758598667}\n",
            "Training Epoch Loss after 10 is 7.0867244601249695\n",
            "{'Validation Epoch Loss': 1.9157917201519012, 'Epoch': 10, 'Micro F1': 0.7759768401345214, 'Patience Count': 0, 'Best Val F1': 0.7759768401345214, 'Best Val Loss': 1.9157917201519012, 'Micro ROC value': 0.8556993993411488, 'Macro ROC value': 0.8556993993411488}\n",
            "Training Epoch Loss after 11 is 7.072364836931229\n",
            "{'Validation Epoch Loss': 1.920665055513382, 'Epoch': 11, 'Micro F1': 0.7746097372823325, 'Patience Count': 1, 'Best Val F1': 0.7759768401345214, 'Best Val Loss': 1.9157917201519012, 'Micro ROC value': 0.8548942300722951, 'Macro ROC value': 0.8548942300722951}\n",
            "Training Epoch Loss after 12 is 7.056927531957626\n",
            "{'Validation Epoch Loss': 1.9099799692630768, 'Epoch': 12, 'Micro F1': 0.7766962521235655, 'Patience Count': 0, 'Best Val F1': 0.7766962521235655, 'Best Val Loss': 1.9099799692630768, 'Micro ROC value': 0.8563414197486034, 'Macro ROC value': 0.8563414197486034}\n",
            "Training Epoch Loss after 13 is 7.030659884214401\n",
            "{'Validation Epoch Loss': 1.9127228260040283, 'Epoch': 13, 'Micro F1': 0.7768656651728054, 'Patience Count': 0, 'Best Val F1': 0.7768656651728054, 'Best Val Loss': 1.9099799692630768, 'Micro ROC value': 0.8561937372079427, 'Macro ROC value': 0.8561937372079427}\n",
            "Training Epoch Loss after 14 is 7.01982182264328\n",
            "{'Validation Epoch Loss': 1.9036076962947845, 'Epoch': 14, 'Micro F1': 0.7775254506134583, 'Patience Count': 0, 'Best Val F1': 0.7775254506134583, 'Best Val Loss': 1.9036076962947845, 'Micro ROC value': 0.8574322416648454, 'Macro ROC value': 0.8574322416648454}\n",
            "Training Epoch Loss after 15 is 7.006876468658447\n",
            "{'Validation Epoch Loss': 1.8956549763679504, 'Epoch': 15, 'Micro F1': 0.7786205282456247, 'Patience Count': 0, 'Best Val F1': 0.7786205282456247, 'Best Val Loss': 1.8956549763679504, 'Micro ROC value': 0.8584569642400673, 'Macro ROC value': 0.8584569642400673}\n",
            "Training Epoch Loss after 16 is 7.022381365299225\n",
            "{'Validation Epoch Loss': 1.904474139213562, 'Epoch': 16, 'Micro F1': 0.7792792011926637, 'Patience Count': 0, 'Best Val F1': 0.7792792011926637, 'Best Val Loss': 1.8956549763679504, 'Micro ROC value': 0.8593012500567829, 'Macro ROC value': 0.8593012500567829}\n",
            "Training Epoch Loss after 17 is 7.065127730369568\n",
            "{'Validation Epoch Loss': 1.950342059135437, 'Epoch': 17, 'Micro F1': 0.772547608934964, 'Patience Count': 1, 'Best Val F1': 0.7792792011926637, 'Best Val Loss': 1.8956549763679504, 'Micro ROC value': 0.8587735906109433, 'Macro ROC value': 0.8587735906109433}\n",
            "Training Epoch Loss after 18 is 7.144677370786667\n",
            "{'Validation Epoch Loss': 1.905648648738861, 'Epoch': 18, 'Micro F1': 0.7766263598145018, 'Patience Count': 2, 'Best Val F1': 0.7792792011926637, 'Best Val Loss': 1.8956549763679504, 'Micro ROC value': 0.8587440233381257, 'Macro ROC value': 0.8587440233381257}\n",
            "Training Epoch Loss after 19 is 7.046850144863129\n",
            "{'Validation Epoch Loss': 1.9172284603118896, 'Epoch': 19, 'Micro F1': 0.7747287036716014, 'Patience Count': 3, 'Best Val F1': 0.7792792011926637, 'Best Val Loss': 1.8956549763679504, 'Micro ROC value': 0.8588778209971346, 'Macro ROC value': 0.8588778209971346}\n",
            "Training Epoch Loss after 20 is 6.926881015300751\n",
            "{'Validation Epoch Loss': 1.8869475424289703, 'Epoch': 20, 'Micro F1': 0.7782087508234232, 'Patience Count': 0, 'Best Val F1': 0.7792792011926637, 'Best Val Loss': 1.8869475424289703, 'Micro ROC value': 0.8614229227282895, 'Macro ROC value': 0.8614229227282895}\n",
            "Training Epoch Loss after 21 is 6.861379683017731\n",
            "{'Validation Epoch Loss': 1.8730749189853668, 'Epoch': 21, 'Micro F1': 0.7806140137988419, 'Patience Count': 0, 'Best Val F1': 0.7806140137988419, 'Best Val Loss': 1.8730749189853668, 'Micro ROC value': 0.8621168318105583, 'Macro ROC value': 0.8621168318105583}\n",
            "Training Epoch Loss after 22 is 6.837690591812134\n",
            "{'Validation Epoch Loss': 1.8639688193798065, 'Epoch': 22, 'Micro F1': 0.7824212113857781, 'Patience Count': 0, 'Best Val F1': 0.7824212113857781, 'Best Val Loss': 1.8639688193798065, 'Micro ROC value': 0.8638671580765345, 'Macro ROC value': 0.8638671580765345}\n",
            "Training Epoch Loss after 23 is 6.805669605731964\n",
            "{'Validation Epoch Loss': 1.8645549714565277, 'Epoch': 23, 'Micro F1': 0.7827010253876623, 'Patience Count': 0, 'Best Val F1': 0.7827010253876623, 'Best Val Loss': 1.8639688193798065, 'Micro ROC value': 0.8638127737110819, 'Macro ROC value': 0.8638127737110819}\n",
            "Training Epoch Loss after 24 is 6.805992037057877\n",
            "{'Validation Epoch Loss': 1.8594071865081787, 'Epoch': 24, 'Micro F1': 0.7840463890718719, 'Patience Count': 0, 'Best Val F1': 0.7840463890718719, 'Best Val Loss': 1.8594071865081787, 'Micro ROC value': 0.8655313234614723, 'Macro ROC value': 0.8655313234614723}\n",
            "Training Epoch Loss after 25 is 6.754444509744644\n",
            "{'Validation Epoch Loss': 1.856457769870758, 'Epoch': 25, 'Micro F1': 0.7829239330166765, 'Patience Count': 0, 'Best Val F1': 0.7840463890718719, 'Best Val Loss': 1.856457769870758, 'Micro ROC value': 0.8650248036827577, 'Macro ROC value': 0.8650248036827577}\n",
            "Training Epoch Loss after 26 is 6.7347018122673035\n",
            "{'Validation Epoch Loss': 1.8413770198822021, 'Epoch': 26, 'Micro F1': 0.7861612869673751, 'Patience Count': 0, 'Best Val F1': 0.7861612869673751, 'Best Val Loss': 1.8413770198822021, 'Micro ROC value': 0.867345015223057, 'Macro ROC value': 0.867345015223057}\n",
            "Training Epoch Loss after 27 is 6.717583119869232\n",
            "{'Validation Epoch Loss': 1.8489019274711609, 'Epoch': 27, 'Micro F1': 0.7860356065596505, 'Patience Count': 1, 'Best Val F1': 0.7861612869673751, 'Best Val Loss': 1.8413770198822021, 'Micro ROC value': 0.8669205164223936, 'Macro ROC value': 0.8669205164223936}\n",
            "Training Epoch Loss after 28 is 6.688117980957031\n",
            "{'Validation Epoch Loss': 1.835181176662445, 'Epoch': 28, 'Micro F1': 0.7870063793641439, 'Patience Count': 0, 'Best Val F1': 0.7870063793641439, 'Best Val Loss': 1.835181176662445, 'Micro ROC value': 0.8688667620101638, 'Macro ROC value': 0.8688667620101638}\n",
            "Training Epoch Loss after 29 is 6.6498755514621735\n",
            "{'Validation Epoch Loss': 1.8212591707706451, 'Epoch': 29, 'Micro F1': 0.789736677876781, 'Patience Count': 0, 'Best Val F1': 0.789736677876781, 'Best Val Loss': 1.8212591707706451, 'Micro ROC value': 0.8712200902200233, 'Macro ROC value': 0.8712200902200233}\n",
            "Training Epoch Loss after 30 is 6.60248190164566\n",
            "{'Validation Epoch Loss': 1.8179331719875336, 'Epoch': 30, 'Micro F1': 0.789385639496585, 'Patience Count': 0, 'Best Val F1': 0.789736677876781, 'Best Val Loss': 1.8179331719875336, 'Micro ROC value': 0.8712049315864057, 'Macro ROC value': 0.8712049315864057}\n",
            "Training Epoch Loss after 31 is 6.57095006108284\n",
            "{'Validation Epoch Loss': 1.8190570175647736, 'Epoch': 31, 'Micro F1': 0.7907984606316957, 'Patience Count': 0, 'Best Val F1': 0.7907984606316957, 'Best Val Loss': 1.8179331719875336, 'Micro ROC value': 0.8722453985551897, 'Macro ROC value': 0.8722453985551897}\n",
            "Training Epoch Loss after 32 is 6.528660923242569\n",
            "{'Validation Epoch Loss': 1.821168303489685, 'Epoch': 32, 'Micro F1': 0.7906207745380162, 'Patience Count': 1, 'Best Val F1': 0.7907984606316957, 'Best Val Loss': 1.8179331719875336, 'Micro ROC value': 0.8740156372248844, 'Macro ROC value': 0.8740156372248844}\n",
            "Training Epoch Loss after 33 is 6.535918176174164\n",
            "{'Validation Epoch Loss': 1.8831614255905151, 'Epoch': 33, 'Micro F1': 0.7826335679367611, 'Patience Count': 2, 'Best Val F1': 0.7907984606316957, 'Best Val Loss': 1.8179331719875336, 'Micro ROC value': 0.8740712485443394, 'Macro ROC value': 0.8740712485443394}\n",
            "Training Epoch Loss after 34 is 6.6543135941028595\n",
            "{'Validation Epoch Loss': 1.7941322326660156, 'Epoch': 34, 'Micro F1': 0.7925233158825365, 'Patience Count': 0, 'Best Val F1': 0.7925233158825365, 'Best Val Loss': 1.7941322326660156, 'Micro ROC value': 0.8746179579498163, 'Macro ROC value': 0.8746179579498163}\n",
            "Training Epoch Loss after 35 is 6.56464147567749\n",
            "{'Validation Epoch Loss': 1.7987461984157562, 'Epoch': 35, 'Micro F1': 0.7936197690947544, 'Patience Count': 0, 'Best Val F1': 0.7936197690947544, 'Best Val Loss': 1.7941322326660156, 'Micro ROC value': 0.8750120017369031, 'Macro ROC value': 0.8750120017369031}\n",
            "Training Epoch Loss after 36 is 6.417675703763962\n",
            "{'Validation Epoch Loss': 1.7856037020683289, 'Epoch': 36, 'Micro F1': 0.7949242450507922, 'Patience Count': 0, 'Best Val F1': 0.7949242450507922, 'Best Val Loss': 1.7856037020683289, 'Micro ROC value': 0.8773792982221029, 'Macro ROC value': 0.8773792982221029}\n",
            "Training Epoch Loss after 37 is 6.387877196073532\n",
            "{'Validation Epoch Loss': 1.7686066925525665, 'Epoch': 37, 'Micro F1': 0.7978235620427834, 'Patience Count': 0, 'Best Val F1': 0.7978235620427834, 'Best Val Loss': 1.7686066925525665, 'Micro ROC value': 0.8790241640562141, 'Macro ROC value': 0.8790241640562141}\n",
            "Training Epoch Loss after 38 is 6.3612267673015594\n",
            "{'Validation Epoch Loss': 1.770959198474884, 'Epoch': 38, 'Micro F1': 0.7981832680373054, 'Patience Count': 0, 'Best Val F1': 0.7981832680373054, 'Best Val Loss': 1.7686066925525665, 'Micro ROC value': 0.879074967626432, 'Macro ROC value': 0.879074967626432}\n",
            "Training Epoch Loss after 39 is 6.307554006576538\n",
            "{'Validation Epoch Loss': 1.7674517929553986, 'Epoch': 39, 'Micro F1': 0.7987466629684845, 'Patience Count': 0, 'Best Val F1': 0.7987466629684845, 'Best Val Loss': 1.7674517929553986, 'Micro ROC value': 0.8806398721415127, 'Macro ROC value': 0.8806398721415127}\n",
            "Training Epoch Loss after 40 is 6.311374515295029\n",
            "{'Validation Epoch Loss': 1.7620585262775421, 'Epoch': 40, 'Micro F1': 0.7999557951669383, 'Patience Count': 0, 'Best Val F1': 0.7999557951669383, 'Best Val Loss': 1.7620585262775421, 'Micro ROC value': 0.8817734634501504, 'Macro ROC value': 0.8817734634501504}\n",
            "Training Epoch Loss after 41 is 6.351002246141434\n",
            "{'Validation Epoch Loss': 1.7626651227474213, 'Epoch': 41, 'Micro F1': 0.7994097354644107, 'Patience Count': 1, 'Best Val F1': 0.7999557951669383, 'Best Val Loss': 1.7620585262775421, 'Micro ROC value': 0.8811395287191116, 'Macro ROC value': 0.8811395287191116}\n",
            "Training Epoch Loss after 42 is 6.267846792936325\n",
            "{'Validation Epoch Loss': 1.756243258714676, 'Epoch': 42, 'Micro F1': 0.8019867205242446, 'Patience Count': 0, 'Best Val F1': 0.8019867205242446, 'Best Val Loss': 1.756243258714676, 'Micro ROC value': 0.8821868751142958, 'Macro ROC value': 0.8821868751142958}\n",
            "Training Epoch Loss after 43 is 6.24766007065773\n",
            "{'Validation Epoch Loss': 1.7481074631214142, 'Epoch': 43, 'Micro F1': 0.8023350552993794, 'Patience Count': 0, 'Best Val F1': 0.8023350552993794, 'Best Val Loss': 1.7481074631214142, 'Micro ROC value': 0.883792590375324, 'Macro ROC value': 0.883792590375324}\n",
            "Training Epoch Loss after 44 is 6.290202260017395\n",
            "{'Validation Epoch Loss': 1.7505099475383759, 'Epoch': 44, 'Micro F1': 0.8018453350899697, 'Patience Count': 1, 'Best Val F1': 0.8023350552993794, 'Best Val Loss': 1.7481074631214142, 'Micro ROC value': 0.882746627977383, 'Macro ROC value': 0.882746627977383}\n",
            "Training Epoch Loss after 45 is 6.207976073026657\n",
            "{'Validation Epoch Loss': 1.7387992143630981, 'Epoch': 45, 'Micro F1': 0.8048790001040114, 'Patience Count': 0, 'Best Val F1': 0.8048790001040114, 'Best Val Loss': 1.7387992143630981, 'Micro ROC value': 0.8858646957002523, 'Macro ROC value': 0.8858646957002523}\n",
            "Training Epoch Loss after 46 is 6.17002010345459\n",
            "{'Validation Epoch Loss': 1.7311605215072632, 'Epoch': 46, 'Micro F1': 0.8059494504732517, 'Patience Count': 0, 'Best Val F1': 0.8059494504732517, 'Best Val Loss': 1.7311605215072632, 'Micro ROC value': 0.8855453689543988, 'Macro ROC value': 0.8855453689543988}\n",
            "Training Epoch Loss after 47 is 6.167994290590286\n",
            "{'Validation Epoch Loss': 1.7339187264442444, 'Epoch': 47, 'Micro F1': 0.8051390285337864, 'Patience Count': 1, 'Best Val F1': 0.8059494504732517, 'Best Val Loss': 1.7311605215072632, 'Micro ROC value': 0.8863775941074619, 'Macro ROC value': 0.8863775941074619}\n",
            "Training Epoch Loss after 48 is 6.1275113224983215\n",
            "{'Validation Epoch Loss': 1.7440119981765747, 'Epoch': 48, 'Micro F1': 0.8042159276080851, 'Patience Count': 2, 'Best Val F1': 0.8059494504732517, 'Best Val Loss': 1.7311605215072632, 'Micro ROC value': 0.886366699337269, 'Macro ROC value': 0.886366699337269}\n",
            "Training Epoch Loss after 49 is 6.145521461963654\n",
            "{'Validation Epoch Loss': 1.7144184410572052, 'Epoch': 49, 'Micro F1': 0.8061314703740943, 'Patience Count': 0, 'Best Val F1': 0.8061314703740943, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8878199051677949, 'Macro ROC value': 0.8878199051677949}\n",
            "Training Epoch Loss after 50 is 6.10723277926445\n",
            "{'Validation Epoch Loss': 1.7351990938186646, 'Epoch': 50, 'Micro F1': 0.8062859446035963, 'Patience Count': 0, 'Best Val F1': 0.8062859446035963, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8889570625762866, 'Macro ROC value': 0.8889570625762866}\n",
            "Training Epoch Loss after 51 is 6.069001495838165\n",
            "{'Validation Epoch Loss': 1.7220755815505981, 'Epoch': 51, 'Micro F1': 0.8070415698783067, 'Patience Count': 0, 'Best Val F1': 0.8070415698783067, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8887997714846275, 'Macro ROC value': 0.8887997714846275}\n",
            "Training Epoch Loss after 52 is 6.081973105669022\n",
            "{'Validation Epoch Loss': 1.7305023074150085, 'Epoch': 52, 'Micro F1': 0.8066818638837846, 'Patience Count': 1, 'Best Val F1': 0.8070415698783067, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8886539857081026, 'Macro ROC value': 0.8886539857081026}\n",
            "Training Epoch Loss after 53 is 6.027803391218185\n",
            "{'Validation Epoch Loss': 1.7247956097126007, 'Epoch': 53, 'Micro F1': 0.8075096210519017, 'Patience Count': 0, 'Best Val F1': 0.8075096210519017, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8894974459966698, 'Macro ROC value': 0.8894974459966698}\n",
            "Training Epoch Loss after 54 is 6.043443650007248\n",
            "{'Validation Epoch Loss': 1.733928918838501, 'Epoch': 54, 'Micro F1': 0.8059494504732517, 'Patience Count': 1, 'Best Val F1': 0.8075096210519017, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8880514972233782, 'Macro ROC value': 0.8880514972233782}\n",
            "Training Epoch Loss after 55 is 6.017347127199173\n",
            "{'Validation Epoch Loss': 1.7174935936927795, 'Epoch': 55, 'Micro F1': 0.808372048677322, 'Patience Count': 0, 'Best Val F1': 0.808372048677322, 'Best Val Loss': 1.7144184410572052, 'Micro ROC value': 0.8901628392573756, 'Macro ROC value': 0.8901628392573756}\n",
            "Training Epoch Loss after 56 is 6.019590198993683\n",
            "{'Validation Epoch Loss': 1.7128340005874634, 'Epoch': 56, 'Micro F1': 0.8084543910134174, 'Patience Count': 0, 'Best Val F1': 0.8084543910134174, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.8904063522126309, 'Macro ROC value': 0.8904063522126309}\n",
            "Training Epoch Loss after 57 is 6.026625573635101\n",
            "{'Validation Epoch Loss': 1.7175258994102478, 'Epoch': 57, 'Micro F1': 0.8082073640051314, 'Patience Count': 1, 'Best Val F1': 0.8084543910134174, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.8910514715183409, 'Macro ROC value': 0.8910514715183409}\n",
            "Training Epoch Loss after 58 is 6.023409903049469\n",
            "{'Validation Epoch Loss': 1.7252363562583923, 'Epoch': 58, 'Micro F1': 0.8074749505945984, 'Patience Count': 2, 'Best Val F1': 0.8084543910134174, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.8904637170463887, 'Macro ROC value': 0.8904637170463887}\n",
            "Training Epoch Loss after 59 is 6.022041320800781\n",
            "{'Validation Epoch Loss': 1.7587043941020966, 'Epoch': 59, 'Micro F1': 0.8048038416864588, 'Patience Count': 3, 'Best Val F1': 0.8084543910134174, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.8903501035611607, 'Macro ROC value': 0.8903501035611607}\n",
            "Training Epoch Loss after 60 is 6.057228863239288\n",
            "{'Validation Epoch Loss': 1.77347794175148, 'Epoch': 60, 'Micro F1': 0.8048876677183372, 'Patience Count': 4, 'Best Val F1': 0.8084543910134174, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.888750268710168, 'Macro ROC value': 0.888750268710168}\n",
            "Training Epoch Loss after 61 is 6.093091189861298\n",
            "{'Validation Epoch Loss': 1.719077318906784, 'Epoch': 61, 'Micro F1': 0.8084717262420691, 'Patience Count': 0, 'Best Val F1': 0.8084717262420691, 'Best Val Loss': 1.7128340005874634, 'Micro ROC value': 0.8890838461733415, 'Macro ROC value': 0.8890838461733415}\n",
            "Training Epoch Loss after 62 is 6.00541415810585\n",
            "{'Validation Epoch Loss': 1.6861757040023804, 'Epoch': 62, 'Micro F1': 0.8124414936033006, 'Patience Count': 0, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8919197577018736, 'Macro ROC value': 0.8919197577018736}\n",
            "Training Epoch Loss after 63 is 5.960530549287796\n",
            "{'Validation Epoch Loss': 1.6997859477996826, 'Epoch': 63, 'Micro F1': 0.8117610858787228, 'Patience Count': 1, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8924210276433531, 'Macro ROC value': 0.8924210276433531}\n",
            "Training Epoch Loss after 64 is 5.9283546805381775\n",
            "{'Validation Epoch Loss': 1.712855726480484, 'Epoch': 64, 'Micro F1': 0.8106126269805499, 'Patience Count': 2, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8920881636288454, 'Macro ROC value': 0.8920881636288454}\n",
            "Training Epoch Loss after 65 is 6.03219336271286\n",
            "{'Validation Epoch Loss': 1.6890107095241547, 'Epoch': 65, 'Micro F1': 0.8122118018236661, 'Patience Count': 3, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8916205806125042, 'Macro ROC value': 0.8916205806125042}\n",
            "Training Epoch Loss after 66 is 6.028711676597595\n",
            "{'Validation Epoch Loss': 1.7004576325416565, 'Epoch': 66, 'Micro F1': 0.809682522483476, 'Patience Count': 4, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8904866888218863, 'Macro ROC value': 0.8904866888218863}\n",
            "Training Epoch Loss after 67 is 6.078195482492447\n",
            "{'Validation Epoch Loss': 1.7187275886535645, 'Epoch': 67, 'Micro F1': 0.8099148840273205, 'Patience Count': 5, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8903252282949483, 'Macro ROC value': 0.8903252282949483}\n",
            "Training Epoch Loss after 68 is 6.097796410322189\n",
            "{'Validation Epoch Loss': 1.7380633354187012, 'Epoch': 68, 'Micro F1': 0.8065512952200162, 'Patience Count': 6, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8887862517598926, 'Macro ROC value': 0.8887862517598926}\n",
            "Training Epoch Loss after 69 is 6.186212867498398\n",
            "{'Validation Epoch Loss': 1.7836799621582031, 'Epoch': 69, 'Micro F1': 0.795765003640398, 'Patience Count': 7, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8906259542136434, 'Macro ROC value': 0.8906259542136434}\n",
            "Training Epoch Loss after 70 is 6.138841062784195\n",
            "{'Validation Epoch Loss': 1.7681861817836761, 'Epoch': 70, 'Micro F1': 0.8054813992996568, 'Patience Count': 8, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8899315279909659, 'Macro ROC value': 0.8899315279909659}\n",
            "Training Epoch Loss after 71 is 6.0572991371154785\n",
            "{'Validation Epoch Loss': 1.7197596430778503, 'Epoch': 71, 'Micro F1': 0.8063611621537288, 'Patience Count': 9, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8912272859220998, 'Macro ROC value': 0.8912272859220998}\n",
            "Training Epoch Loss after 72 is 5.945000410079956\n",
            "{'Validation Epoch Loss': 1.7257022857666016, 'Epoch': 72, 'Micro F1': 0.8063698297680546, 'Patience Count': 10, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8916158070838607, 'Macro ROC value': 0.8916158070838607}\n",
            "Training Epoch Loss after 73 is 5.93163874745369\n",
            "{'Validation Epoch Loss': 1.7364424467086792, 'Epoch': 73, 'Micro F1': 0.8047576535034499, 'Patience Count': 11, 'Best Val F1': 0.8124414936033006, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8907773773728085, 'Macro ROC value': 0.8907773773728085}\n",
            "Epoch    74: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 74 is 5.907637894153595\n",
            "{'Validation Epoch Loss': 1.6935772001743317, 'Epoch': 74, 'Micro F1': 0.8131349027493674, 'Patience Count': 0, 'Best Val F1': 0.8131349027493674, 'Best Val Loss': 1.6861757040023804, 'Micro ROC value': 0.8946841402638157, 'Macro ROC value': 0.8946841402638157}\n",
            "Training Epoch Loss after 75 is 5.807919085025787\n",
            "{'Validation Epoch Loss': 1.6659418046474457, 'Epoch': 75, 'Micro F1': 0.8164806018791387, 'Patience Count': 0, 'Best Val F1': 0.8164806018791387, 'Best Val Loss': 1.6659418046474457, 'Micro ROC value': 0.8954708724620741, 'Macro ROC value': 0.8954708724620741}\n",
            "Training Epoch Loss after 76 is 5.7505757212638855\n",
            "{'Validation Epoch Loss': 1.6729159355163574, 'Epoch': 76, 'Micro F1': 0.8155444995319489, 'Patience Count': 1, 'Best Val F1': 0.8164806018791387, 'Best Val Loss': 1.6659418046474457, 'Micro ROC value': 0.8951694237788989, 'Macro ROC value': 0.8951694237788989}\n",
            "Training Epoch Loss after 77 is 5.751012772321701\n",
            "{'Validation Epoch Loss': 1.6655738651752472, 'Epoch': 77, 'Micro F1': 0.8167406303089138, 'Patience Count': 0, 'Best Val F1': 0.8167406303089138, 'Best Val Loss': 1.6655738651752472, 'Micro ROC value': 0.8963728457210003, 'Macro ROC value': 0.8963728457210003}\n",
            "Training Epoch Loss after 78 is 5.716875225305557\n",
            "{'Validation Epoch Loss': 1.6596780717372894, 'Epoch': 78, 'Micro F1': 0.8163765905072288, 'Patience Count': 0, 'Best Val F1': 0.8167406303089138, 'Best Val Loss': 1.6596780717372894, 'Micro ROC value': 0.8959430491141689, 'Macro ROC value': 0.8959430491141689}\n",
            "Training Epoch Loss after 79 is 5.7369987070560455\n",
            "{'Validation Epoch Loss': 1.6556102931499481, 'Epoch': 79, 'Micro F1': 0.8173430295045592, 'Patience Count': 0, 'Best Val F1': 0.8173430295045592, 'Best Val Loss': 1.6556102931499481, 'Micro ROC value': 0.8966894133328894, 'Macro ROC value': 0.8966894133328894}\n",
            "Training Epoch Loss after 80 is 5.710829347372055\n",
            "{'Validation Epoch Loss': 1.6604455411434174, 'Epoch': 80, 'Micro F1': 0.817351697118885, 'Patience Count': 0, 'Best Val F1': 0.817351697118885, 'Best Val Loss': 1.6556102931499481, 'Micro ROC value': 0.8965521781595498, 'Macro ROC value': 0.8965521781595498}\n",
            "Training Epoch Loss after 81 is 5.7141750156879425\n",
            "{'Validation Epoch Loss': 1.65688356757164, 'Epoch': 81, 'Micro F1': 0.8171566757965537, 'Patience Count': 1, 'Best Val F1': 0.817351697118885, 'Best Val Loss': 1.6556102931499481, 'Micro ROC value': 0.8964012641397103, 'Macro ROC value': 0.8964012641397103}\n",
            "Training Epoch Loss after 82 is 5.705965429544449\n",
            "{'Validation Epoch Loss': 1.6546726524829865, 'Epoch': 82, 'Micro F1': 0.8169183164025934, 'Patience Count': 0, 'Best Val F1': 0.817351697118885, 'Best Val Loss': 1.6546726524829865, 'Micro ROC value': 0.896315410192229, 'Macro ROC value': 0.896315410192229}\n",
            "Training Epoch Loss after 83 is 5.69782954454422\n",
            "{'Validation Epoch Loss': 1.6520428359508514, 'Epoch': 83, 'Micro F1': 0.8179410948930417, 'Patience Count': 0, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8971427546847792, 'Macro ROC value': 0.8971427546847792}\n",
            "Training Epoch Loss after 84 is 5.6999803483486176\n",
            "{'Validation Epoch Loss': 1.6626554727554321, 'Epoch': 84, 'Micro F1': 0.8166366189370038, 'Patience Count': 1, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8962329598227338, 'Macro ROC value': 0.8962329598227338}\n",
            "Training Epoch Loss after 85 is 5.693132966756821\n",
            "{'Validation Epoch Loss': 1.6535941064357758, 'Epoch': 85, 'Micro F1': 0.8179107582429012, 'Patience Count': 2, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8970353157316349, 'Macro ROC value': 0.8970353157316349}\n",
            "Training Epoch Loss after 86 is 5.691794365644455\n",
            "{'Validation Epoch Loss': 1.6546401381492615, 'Epoch': 86, 'Micro F1': 0.8176854002704296, 'Patience Count': 3, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8969933470096425, 'Macro ROC value': 0.8969933470096425}\n",
            "Training Epoch Loss after 87 is 5.678165405988693\n",
            "{'Validation Epoch Loss': 1.6576715409755707, 'Epoch': 87, 'Micro F1': 0.8168013036091946, 'Patience Count': 4, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8961276053206977, 'Macro ROC value': 0.8961276053206977}\n",
            "Training Epoch Loss after 88 is 5.6907252967357635\n",
            "{'Validation Epoch Loss': 1.6537100672721863, 'Epoch': 88, 'Micro F1': 0.8176767326561037, 'Patience Count': 5, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8968972580620079, 'Macro ROC value': 0.8968972580620079}\n",
            "Training Epoch Loss after 89 is 5.6807937026023865\n",
            "{'Validation Epoch Loss': 1.6548842787742615, 'Epoch': 89, 'Micro F1': 0.8174123704191659, 'Patience Count': 6, 'Best Val F1': 0.8179410948930417, 'Best Val Loss': 1.6520428359508514, 'Micro ROC value': 0.8970947140800302, 'Macro ROC value': 0.8970947140800302}\n",
            "Training Epoch Loss after 90 is 5.6808174550533295\n",
            "{'Validation Epoch Loss': 1.6493943631649017, 'Epoch': 90, 'Micro F1': 0.8183512321120558, 'Patience Count': 0, 'Best Val F1': 0.8183512321120558, 'Best Val Loss': 1.6493943631649017, 'Micro ROC value': 0.8975442481899155, 'Macro ROC value': 0.8975442481899155}\n",
            "Training Epoch Loss after 91 is 5.6732732355594635\n",
            "{'Validation Epoch Loss': 1.6549341678619385, 'Epoch': 91, 'Micro F1': 0.8173820337690253, 'Patience Count': 1, 'Best Val F1': 0.8183512321120558, 'Best Val Loss': 1.6493943631649017, 'Micro ROC value': 0.8972778941754437, 'Macro ROC value': 0.8972778941754437}\n",
            "Training Epoch Loss after 92 is 5.668229848146439\n",
            "{'Validation Epoch Loss': 1.6483114659786224, 'Epoch': 92, 'Micro F1': 0.8186085011961308, 'Patience Count': 0, 'Best Val F1': 0.8186085011961308, 'Best Val Loss': 1.6483114659786224, 'Micro ROC value': 0.8980395192012338, 'Macro ROC value': 0.8980395192012338}\n",
            "Training Epoch Loss after 93 is 5.674037307500839\n",
            "{'Validation Epoch Loss': 1.6546077728271484, 'Epoch': 93, 'Micro F1': 0.817048330617481, 'Patience Count': 1, 'Best Val F1': 0.8186085011961308, 'Best Val Loss': 1.6483114659786224, 'Micro ROC value': 0.8967529043666144, 'Macro ROC value': 0.8967529043666144}\n",
            "Training Epoch Loss after 94 is 5.671433627605438\n",
            "{'Validation Epoch Loss': 1.6537341177463531, 'Epoch': 94, 'Micro F1': 0.8173473633117221, 'Patience Count': 2, 'Best Val F1': 0.8186085011961308, 'Best Val Loss': 1.6483114659786224, 'Micro ROC value': 0.8969138983233683, 'Macro ROC value': 0.8969138983233683}\n",
            "Training Epoch Loss after 95 is 5.672660082578659\n",
            "{'Validation Epoch Loss': 1.6591423749923706, 'Epoch': 95, 'Micro F1': 0.8172693547827895, 'Patience Count': 3, 'Best Val F1': 0.8186085011961308, 'Best Val Loss': 1.6483114659786224, 'Micro ROC value': 0.8964157842643818, 'Macro ROC value': 0.8964157842643818}\n",
            "Training Epoch Loss after 96 is 5.671946883201599\n",
            "{'Validation Epoch Loss': 1.645462304353714, 'Epoch': 96, 'Micro F1': 0.8185088236313837, 'Patience Count': 0, 'Best Val F1': 0.8186085011961308, 'Best Val Loss': 1.645462304353714, 'Micro ROC value': 0.897933374658007, 'Macro ROC value': 0.897933374658007}\n",
            "Training Epoch Loss after 97 is 5.665513902902603\n",
            "{'Validation Epoch Loss': 1.6454481780529022, 'Epoch': 97, 'Micro F1': 0.818703844953715, 'Patience Count': 0, 'Best Val F1': 0.818703844953715, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8981603970059577, 'Macro ROC value': 0.8981603970059577}\n",
            "Training Epoch Loss after 98 is 5.654029130935669\n",
            "{'Validation Epoch Loss': 1.6521547138690948, 'Epoch': 98, 'Micro F1': 0.8179800991575079, 'Patience Count': 1, 'Best Val F1': 0.818703844953715, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8975192559663366, 'Macro ROC value': 0.8975192559663366}\n",
            "Training Epoch Loss after 99 is 5.651948183774948\n",
            "{'Validation Epoch Loss': 1.6555059552192688, 'Epoch': 99, 'Micro F1': 0.8171480081822279, 'Patience Count': 2, 'Best Val F1': 0.818703844953715, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8968937226727561, 'Macro ROC value': 0.8968937226727561}\n",
            "Training Epoch Loss after 100 is 5.6440852880477905\n",
            "{'Validation Epoch Loss': 1.6520039737224579, 'Epoch': 100, 'Micro F1': 0.8182097909371424, 'Patience Count': 3, 'Best Val F1': 0.818703844953715, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8976325025035592, 'Macro ROC value': 0.8976325025035592}\n",
            "Training Epoch Loss after 101 is 5.636923223733902\n",
            "{'Validation Epoch Loss': 1.651291012763977, 'Epoch': 101, 'Micro F1': 0.8182401275872829, 'Patience Count': 4, 'Best Val F1': 0.818703844953715, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8977364298453178, 'Macro ROC value': 0.8977364298453178}\n",
            "Training Epoch Loss after 102 is 5.6433234214782715\n",
            "{'Validation Epoch Loss': 1.6456217765808105, 'Epoch': 102, 'Micro F1': 0.8187948549041362, 'Patience Count': 0, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6454481780529022, 'Micro ROC value': 0.8980834094602681, 'Macro ROC value': 0.8980834094602681}\n",
            "Training Epoch Loss after 103 is 5.642455905675888\n",
            "{'Validation Epoch Loss': 1.6453713476657867, 'Epoch': 103, 'Micro F1': 0.8179020906285754, 'Patience Count': 0, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8980101518090262, 'Macro ROC value': 0.8980101518090262}\n",
            "Training Epoch Loss after 104 is 5.642471760511398\n",
            "{'Validation Epoch Loss': 1.652372419834137, 'Epoch': 104, 'Micro F1': 0.8177472479847446, 'Patience Count': 1, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8974984459209733, 'Macro ROC value': 0.8974984459209733}\n",
            "Training Epoch Loss after 105 is 5.644297420978546\n",
            "{'Validation Epoch Loss': 1.6456664502620697, 'Epoch': 105, 'Micro F1': 0.8184698193669174, 'Patience Count': 2, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8982615304297019, 'Macro ROC value': 0.8982615304297019}\n",
            "Training Epoch Loss after 106 is 5.631372898817062\n",
            "{'Validation Epoch Loss': 1.6556066870689392, 'Epoch': 106, 'Micro F1': 0.8178717539784349, 'Patience Count': 3, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8971626237774722, 'Macro ROC value': 0.8971626237774722}\n",
            "Training Epoch Loss after 107 is 5.6384497582912445\n",
            "{'Validation Epoch Loss': 1.6526905000209808, 'Epoch': 107, 'Micro F1': 0.8172086814825087, 'Patience Count': 4, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8971238068810861, 'Macro ROC value': 0.8971238068810861}\n",
            "Training Epoch Loss after 108 is 5.630977421998978\n",
            "{'Validation Epoch Loss': 1.6499733924865723, 'Epoch': 108, 'Micro F1': 0.8182227923586312, 'Patience Count': 5, 'Best Val F1': 0.8187948549041362, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8978832348771522, 'Macro ROC value': 0.8978832348771522}\n",
            "Training Epoch Loss after 109 is 5.628930598497391\n",
            "{'Validation Epoch Loss': 1.6463097035884857, 'Epoch': 109, 'Micro F1': 0.819219568006102, 'Patience Count': 0, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8986790977383174, 'Macro ROC value': 0.8986790977383174}\n",
            "Training Epoch Loss after 110 is 5.630256175994873\n",
            "{'Validation Epoch Loss': 1.6582774817943573, 'Epoch': 110, 'Micro F1': 0.8165889470582117, 'Patience Count': 1, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.6453713476657867, 'Micro ROC value': 0.8964023252410586, 'Macro ROC value': 0.8964023252410586}\n",
            "Training Epoch Loss after 111 is 5.628925710916519\n",
            "{'Validation Epoch Loss': 1.642344981431961, 'Epoch': 111, 'Micro F1': 0.8185608293173386, 'Patience Count': 0, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.898266884656651, 'Macro ROC value': 0.898266884656651}\n",
            "Training Epoch Loss after 112 is 5.63051775097847\n",
            "{'Validation Epoch Loss': 1.6497506499290466, 'Epoch': 112, 'Micro F1': 0.8180112163790337, 'Patience Count': 1, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8975837016282768, 'Macro ROC value': 0.8975837016282768}\n",
            "Training Epoch Loss after 113 is 5.616115212440491\n",
            "{'Validation Epoch Loss': 1.6485223472118378, 'Epoch': 113, 'Micro F1': 0.8179367610858788, 'Patience Count': 2, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8975064958920641, 'Macro ROC value': 0.8975064958920641}\n",
            "Training Epoch Loss after 114 is 5.627225607633591\n",
            "{'Validation Epoch Loss': 1.6607447564601898, 'Epoch': 114, 'Micro F1': 0.8176940678847554, 'Patience Count': 3, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8967550467786185, 'Macro ROC value': 0.8967550467786185}\n",
            "Training Epoch Loss after 115 is 5.620198994874954\n",
            "{'Validation Epoch Loss': 1.6432107985019684, 'Epoch': 115, 'Micro F1': 0.8188598620115799, 'Patience Count': 4, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8988204226670126, 'Macro ROC value': 0.8988204226670126}\n",
            "Training Epoch Loss after 116 is 5.61707928776741\n",
            "{'Validation Epoch Loss': 1.6455815434455872, 'Epoch': 116, 'Micro F1': 0.8187211801823666, 'Patience Count': 5, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8984039960573716, 'Macro ROC value': 0.8984039960573716}\n",
            "Training Epoch Loss after 117 is 5.614545494318008\n",
            "{'Validation Epoch Loss': 1.6459549069404602, 'Epoch': 117, 'Micro F1': 0.8181664528655133, 'Patience Count': 6, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8982971774698729, 'Macro ROC value': 0.8982971774698729}\n",
            "Training Epoch Loss after 118 is 5.612485736608505\n",
            "{'Validation Epoch Loss': 1.6466685235500336, 'Epoch': 118, 'Micro F1': 0.8180061020004854, 'Patience Count': 7, 'Best Val F1': 0.819219568006102, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8982037111119023, 'Macro ROC value': 0.8982037111119023}\n",
            "Training Epoch Loss after 119 is 5.602388799190521\n",
            "{'Validation Epoch Loss': 1.6467473208904266, 'Epoch': 119, 'Micro F1': 0.8194232569427591, 'Patience Count': 0, 'Best Val F1': 0.8194232569427591, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8985947441253357, 'Macro ROC value': 0.8985947441253357}\n",
            "Training Epoch Loss after 120 is 5.612460374832153\n",
            "{'Validation Epoch Loss': 1.643270581960678, 'Epoch': 120, 'Micro F1': 0.8191443256970988, 'Patience Count': 1, 'Best Val F1': 0.8194232569427591, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8991321920226689, 'Macro ROC value': 0.8991321920226689}\n",
            "Training Epoch Loss after 121 is 5.609323054552078\n",
            "{'Validation Epoch Loss': 1.643699288368225, 'Epoch': 121, 'Micro F1': 0.8188945324688832, 'Patience Count': 2, 'Best Val F1': 0.8194232569427591, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8982694122851067, 'Macro ROC value': 0.8982694122851067}\n",
            "Training Epoch Loss after 122 is 5.600322008132935\n",
            "{'Validation Epoch Loss': 1.6460551023483276, 'Epoch': 122, 'Micro F1': 0.8196052768436016, 'Patience Count': 0, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8988219294249167, 'Macro ROC value': 0.8988219294249167}\n",
            "Training Epoch Loss after 123 is 5.6080501973629\n",
            "{'Validation Epoch Loss': 1.6469207406044006, 'Epoch': 123, 'Micro F1': 0.818491488402732, 'Patience Count': 1, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8983503341848392, 'Macro ROC value': 0.8983503341848392}\n",
            "Training Epoch Loss after 124 is 5.59479558467865\n",
            "{'Validation Epoch Loss': 1.6461677849292755, 'Epoch': 124, 'Micro F1': 0.8188295253614395, 'Patience Count': 2, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8983607596333016, 'Macro ROC value': 0.8983607596333016}\n",
            "Training Epoch Loss after 125 is 5.597456902265549\n",
            "{'Validation Epoch Loss': 1.6468032598495483, 'Epoch': 125, 'Micro F1': 0.8185174912457095, 'Patience Count': 3, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642344981431961, 'Micro ROC value': 0.8980615717631304, 'Macro ROC value': 0.8980615717631304}\n",
            "Training Epoch Loss after 126 is 5.5974162220954895\n",
            "{'Validation Epoch Loss': 1.642130821943283, 'Epoch': 126, 'Micro F1': 0.8188634108076951, 'Patience Count': 0, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642130821943283, 'Micro ROC value': 0.8988812785821196, 'Macro ROC value': 0.8988812785821196}\n",
            "Training Epoch Loss after 127 is 5.595917612314224\n",
            "{'Validation Epoch Loss': 1.6443048417568207, 'Epoch': 127, 'Micro F1': 0.8187861872898103, 'Patience Count': 1, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642130821943283, 'Micro ROC value': 0.8980771862330856, 'Macro ROC value': 0.8980771862330856}\n",
            "Training Epoch Loss after 128 is 5.598700165748596\n",
            "{'Validation Epoch Loss': 1.6467300653457642, 'Epoch': 128, 'Micro F1': 0.8188192873425731, 'Patience Count': 2, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642130821943283, 'Micro ROC value': 0.8982683198574682, 'Macro ROC value': 0.8982683198574682}\n",
            "Training Epoch Loss after 129 is 5.58909398317337\n",
            "{'Validation Epoch Loss': 1.6462975442409515, 'Epoch': 129, 'Micro F1': 0.8189768748049787, 'Patience Count': 3, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.642130821943283, 'Micro ROC value': 0.8985748639513309, 'Macro ROC value': 0.8985748639513309}\n",
            "Training Epoch Loss after 130 is 5.5933734476566315\n",
            "{'Validation Epoch Loss': 1.6399440169334412, 'Epoch': 130, 'Micro F1': 0.8190462157195854, 'Patience Count': 0, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.6399440169334412, 'Micro ROC value': 0.8985845380117209, 'Macro ROC value': 0.8985845380117209}\n",
            "Training Epoch Loss after 131 is 5.584963917732239\n",
            "{'Validation Epoch Loss': 1.6448445320129395, 'Epoch': 131, 'Micro F1': 0.8185498025925172, 'Patience Count': 1, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.6399440169334412, 'Micro ROC value': 0.8982883514093933, 'Macro ROC value': 0.8982883514093933}\n",
            "Training Epoch Loss after 132 is 5.582187682390213\n",
            "{'Validation Epoch Loss': 1.6431603133678436, 'Epoch': 132, 'Micro F1': 0.8189059642371133, 'Patience Count': 2, 'Best Val F1': 0.8196052768436016, 'Best Val Loss': 1.6399440169334412, 'Micro ROC value': 0.8986775445855033, 'Macro ROC value': 0.8986775445855033}\n",
            "Training Epoch Loss after 133 is 5.584976077079773\n",
            "{'Validation Epoch Loss': 1.6404200792312622, 'Epoch': 133, 'Micro F1': 0.8197959643587699, 'Patience Count': 0, 'Best Val F1': 0.8197959643587699, 'Best Val Loss': 1.6399440169334412, 'Micro ROC value': 0.898914103288722, 'Macro ROC value': 0.898914103288722}\n",
            "Training Epoch Loss after 134 is 5.568407982587814\n",
            "{'Validation Epoch Loss': 1.6396042108535767, 'Epoch': 134, 'Micro F1': 0.8186171688104567, 'Patience Count': 0, 'Best Val F1': 0.8197959643587699, 'Best Val Loss': 1.6396042108535767, 'Micro ROC value': 0.8989406306721704, 'Macro ROC value': 0.8989406306721704}\n",
            "Training Epoch Loss after 135 is 5.582968056201935\n",
            "{'Validation Epoch Loss': 1.6350845992565155, 'Epoch': 135, 'Micro F1': 0.819167562320147, 'Patience Count': 0, 'Best Val F1': 0.8197959643587699, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8997627470769081, 'Macro ROC value': 0.8997627470769081}\n",
            "Training Epoch Loss after 136 is 5.574853301048279\n",
            "{'Validation Epoch Loss': 1.6385533511638641, 'Epoch': 136, 'Micro F1': 0.8204070311687411, 'Patience Count': 0, 'Best Val F1': 0.8204070311687411, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8997189235686913, 'Macro ROC value': 0.8997189235686913}\n",
            "Training Epoch Loss after 137 is 5.565926611423492\n",
            "{'Validation Epoch Loss': 1.6353614330291748, 'Epoch': 137, 'Micro F1': 0.8197309572513262, 'Patience Count': 1, 'Best Val F1': 0.8204070311687411, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8996201169950573, 'Macro ROC value': 0.8996201169950573}\n",
            "Training Epoch Loss after 138 is 5.56747180223465\n",
            "{'Validation Epoch Loss': 1.640821635723114, 'Epoch': 138, 'Micro F1': 0.8196832853725341, 'Patience Count': 2, 'Best Val F1': 0.8204070311687411, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8992317524742134, 'Macro ROC value': 0.8992317524742134}\n",
            "Training Epoch Loss after 139 is 5.5641076266765594\n",
            "{'Validation Epoch Loss': 1.6422148942947388, 'Epoch': 139, 'Micro F1': 0.8188771972402316, 'Patience Count': 3, 'Best Val F1': 0.8204070311687411, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8990062398892683, 'Macro ROC value': 0.8990062398892683}\n",
            "Training Epoch Loss after 140 is 5.559698820114136\n",
            "{'Validation Epoch Loss': 1.6441107988357544, 'Epoch': 140, 'Micro F1': 0.819839302430399, 'Patience Count': 4, 'Best Val F1': 0.8204070311687411, 'Best Val Loss': 1.6350845992565155, 'Micro ROC value': 0.8987612406483629, 'Macro ROC value': 0.8987612406483629}\n",
            "Training Epoch Loss after 141 is 5.566417813301086\n",
            "{'Validation Epoch Loss': 1.6347481608390808, 'Epoch': 141, 'Micro F1': 0.8207947438903359, 'Patience Count': 0, 'Best Val F1': 0.8207947438903359, 'Best Val Loss': 1.6347481608390808, 'Micro ROC value': 0.8998983308125319, 'Macro ROC value': 0.8998983308125319}\n",
            "Training Epoch Loss after 142 is 5.564536690711975\n",
            "{'Validation Epoch Loss': 1.6392636895179749, 'Epoch': 142, 'Micro F1': 0.820103664667337, 'Patience Count': 1, 'Best Val F1': 0.8207947438903359, 'Best Val Loss': 1.6347481608390808, 'Micro ROC value': 0.8993729125355261, 'Macro ROC value': 0.8993729125355261}\n",
            "Training Epoch Loss after 143 is 5.567188084125519\n",
            "{'Validation Epoch Loss': 1.6358417570590973, 'Epoch': 143, 'Micro F1': 0.8195985975617683, 'Patience Count': 2, 'Best Val F1': 0.8207947438903359, 'Best Val Loss': 1.6347481608390808, 'Micro ROC value': 0.8997333210550306, 'Macro ROC value': 0.8997333210550306}\n",
            "Training Epoch Loss after 144 is 5.558125466108322\n",
            "{'Validation Epoch Loss': 1.6425300538539886, 'Epoch': 144, 'Micro F1': 0.8196529487223936, 'Patience Count': 3, 'Best Val F1': 0.8207947438903359, 'Best Val Loss': 1.6347481608390808, 'Micro ROC value': 0.8991331413073058, 'Macro ROC value': 0.8991331413073058}\n",
            "Training Epoch Loss after 145 is 5.5498707592487335\n",
            "{'Validation Epoch Loss': 1.6338877081871033, 'Epoch': 145, 'Micro F1': 0.8202813507610165, 'Patience Count': 0, 'Best Val F1': 0.8207947438903359, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.8994875856695695, 'Macro ROC value': 0.8994875856695695}\n",
            "Training Epoch Loss after 146 is 5.567164391279221\n",
            "{'Validation Epoch Loss': 1.6340627670288086, 'Epoch': 146, 'Micro F1': 0.8208187428492182, 'Patience Count': 0, 'Best Val F1': 0.8208187428492182, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.8994060064414424, 'Macro ROC value': 0.8994060064414424}\n",
            "Training Epoch Loss after 147 is 5.550004243850708\n",
            "{'Validation Epoch Loss': 1.6353943943977356, 'Epoch': 147, 'Micro F1': 0.8206237215268869, 'Patience Count': 1, 'Best Val F1': 0.8208187428492182, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.8996894373247184, 'Macro ROC value': 0.8996894373247184}\n",
            "Training Epoch Loss after 148 is 5.550072282552719\n",
            "{'Validation Epoch Loss': 1.6464120149612427, 'Epoch': 148, 'Micro F1': 0.8189162015046978, 'Patience Count': 2, 'Best Val F1': 0.8208187428492182, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.898802243568289, 'Macro ROC value': 0.898802243568289}\n",
            "Training Epoch Loss after 149 is 5.541618317365646\n",
            "{'Validation Epoch Loss': 1.6345434486865997, 'Epoch': 149, 'Micro F1': 0.819843636237562, 'Patience Count': 3, 'Best Val F1': 0.8208187428492182, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.8996664351507404, 'Macro ROC value': 0.8996664351507404}\n",
            "Training Epoch Loss after 150 is 5.555375248193741\n",
            "{'Validation Epoch Loss': 1.637458175420761, 'Epoch': 150, 'Micro F1': 0.8209617584855944, 'Patience Count': 0, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6338877081871033, 'Micro ROC value': 0.8997179218180984, 'Macro ROC value': 0.8997179218180984}\n",
            "Training Epoch Loss after 151 is 5.553094744682312\n",
            "{'Validation Epoch Loss': 1.6327263712882996, 'Epoch': 151, 'Micro F1': 0.8203203550254828, 'Patience Count': 0, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6327263712882996, 'Micro ROC value': 0.899788082975166, 'Macro ROC value': 0.899788082975166}\n",
            "Training Epoch Loss after 152 is 5.547185271978378\n",
            "{'Validation Epoch Loss': 1.6339113414287567, 'Epoch': 152, 'Micro F1': 0.8202120098464099, 'Patience Count': 1, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6327263712882996, 'Micro ROC value': 0.9004480198730345, 'Macro ROC value': 0.9004480198730345}\n",
            "Training Epoch Loss after 153 is 5.547866404056549\n",
            "{'Validation Epoch Loss': 1.6309260725975037, 'Epoch': 153, 'Micro F1': 0.8202900183753422, 'Patience Count': 0, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.9000313388439859, 'Macro ROC value': 0.9000313388439859}\n",
            "Training Epoch Loss after 154 is 5.5448508858680725\n",
            "{'Validation Epoch Loss': 1.644569754600525, 'Epoch': 154, 'Micro F1': 0.8187688520611587, 'Patience Count': 1, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.898734050039008, 'Macro ROC value': 0.898734050039008}\n",
            "Training Epoch Loss after 155 is 5.537039935588837\n",
            "{'Validation Epoch Loss': 1.6395213305950165, 'Epoch': 155, 'Micro F1': 0.8204893735048365, 'Patience Count': 2, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.8997635552613991, 'Macro ROC value': 0.8997635552613991}\n",
            "Training Epoch Loss after 156 is 5.536012709140778\n",
            "{'Validation Epoch Loss': 1.6339418590068817, 'Epoch': 156, 'Micro F1': 0.8200126547169158, 'Patience Count': 3, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.8997617869845349, 'Macro ROC value': 0.8997617869845349}\n",
            "Training Epoch Loss after 157 is 5.527701139450073\n",
            "{'Validation Epoch Loss': 1.6371021270751953, 'Epoch': 157, 'Micro F1': 0.8202336788822244, 'Patience Count': 4, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.8996793076909375, 'Macro ROC value': 0.8996793076909375}\n",
            "Training Epoch Loss after 158 is 5.537913382053375\n",
            "{'Validation Epoch Loss': 1.6343570053577423, 'Epoch': 158, 'Micro F1': 0.8207929889650748, 'Patience Count': 5, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.9000605465735225, 'Macro ROC value': 0.9000605465735225}\n",
            "Training Epoch Loss after 159 is 5.537256062030792\n",
            "{'Validation Epoch Loss': 1.6317047774791718, 'Epoch': 159, 'Micro F1': 0.820255347918039, 'Patience Count': 6, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.9001267848501495, 'Macro ROC value': 0.9001267848501495}\n",
            "Training Epoch Loss after 160 is 5.521446496248245\n",
            "{'Validation Epoch Loss': 1.6419597268104553, 'Epoch': 160, 'Micro F1': 0.8195588107826991, 'Patience Count': 7, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6309260725975037, 'Micro ROC value': 0.8990463590919396, 'Macro ROC value': 0.8990463590919396}\n",
            "Training Epoch Loss after 161 is 5.5178859531879425\n",
            "{'Validation Epoch Loss': 1.6304774582386017, 'Epoch': 161, 'Micro F1': 0.8207450681274486, 'Patience Count': 0, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6304774582386017, 'Micro ROC value': 0.9004593727709547, 'Macro ROC value': 0.9004593727709547}\n",
            "Epoch   162: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 162 is 5.528026819229126\n",
            "{'Validation Epoch Loss': 1.6292070150375366, 'Epoch': 162, 'Micro F1': 0.8208377576007108, 'Patience Count': 0, 'Best Val F1': 0.8209617584855944, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.9001739127140542, 'Macro ROC value': 0.9001739127140542}\n",
            "Training Epoch Loss after 163 is 5.5157181322574615\n",
            "{'Validation Epoch Loss': 1.6317461133003235, 'Epoch': 163, 'Micro F1': 0.8211351107721111, 'Patience Count': 0, 'Best Val F1': 0.8211351107721111, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.9002202289703689, 'Macro ROC value': 0.9002202289703689}\n",
            "Training Epoch Loss after 164 is 5.535708636045456\n",
            "{'Validation Epoch Loss': 1.63094100356102, 'Epoch': 164, 'Micro F1': 0.8211741150365773, 'Patience Count': 0, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.9003928072929881, 'Macro ROC value': 0.9003928072929881}\n",
            "Training Epoch Loss after 165 is 5.518035888671875\n",
            "{'Validation Epoch Loss': 1.6340251564979553, 'Epoch': 165, 'Micro F1': 0.8207927400062407, 'Patience Count': 1, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.8999426033422169, 'Macro ROC value': 0.8999426033422169}\n",
            "Training Epoch Loss after 166 is 5.5328284204006195\n",
            "{'Validation Epoch Loss': 1.6352970004081726, 'Epoch': 166, 'Micro F1': 0.8205630482266061, 'Patience Count': 2, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.900116110917355, 'Macro ROC value': 0.900116110917355}\n",
            "Training Epoch Loss after 167 is 5.524005353450775\n",
            "{'Validation Epoch Loss': 1.6358745396137238, 'Epoch': 167, 'Micro F1': 0.8205933848767465, 'Patience Count': 3, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.8995804324131698, 'Macro ROC value': 0.8995804324131698}\n",
            "Training Epoch Loss after 168 is 5.525929480791092\n",
            "{'Validation Epoch Loss': 1.6310804784297943, 'Epoch': 168, 'Micro F1': 0.8207710709704261, 'Patience Count': 4, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.9003278409797975, 'Macro ROC value': 0.9003278409797975}\n",
            "Training Epoch Loss after 169 is 5.50848925113678\n",
            "{'Validation Epoch Loss': 1.633045256137848, 'Epoch': 169, 'Micro F1': 0.8205067087334882, 'Patience Count': 5, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.8999055019831632, 'Macro ROC value': 0.8999055019831632}\n",
            "Training Epoch Loss after 170 is 5.534154325723648\n",
            "{'Validation Epoch Loss': 1.6308560967445374, 'Epoch': 170, 'Micro F1': 0.820259681725202, 'Patience Count': 6, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6292070150375366, 'Micro ROC value': 0.899710487497438, 'Macro ROC value': 0.899710487497438}\n",
            "Training Epoch Loss after 171 is 5.525016039609909\n",
            "{'Validation Epoch Loss': 1.6275304555892944, 'Epoch': 171, 'Micro F1': 0.8206323891412127, 'Patience Count': 0, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.900313797727174, 'Macro ROC value': 0.900313797727174}\n",
            "Training Epoch Loss after 172 is 5.516271144151688\n",
            "{'Validation Epoch Loss': 1.6408714056015015, 'Epoch': 172, 'Micro F1': 0.818978108516918, 'Patience Count': 1, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8992088948778705, 'Macro ROC value': 0.8992088948778705}\n",
            "Training Epoch Loss after 173 is 5.528254002332687\n",
            "{'Validation Epoch Loss': 1.6359643638134003, 'Epoch': 173, 'Micro F1': 0.8197959643587699, 'Patience Count': 2, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8992553522877572, 'Macro ROC value': 0.8992553522877572}\n",
            "Training Epoch Loss after 174 is 5.532673895359039\n",
            "{'Validation Epoch Loss': 1.6337072551250458, 'Epoch': 174, 'Micro F1': 0.8201123322816628, 'Patience Count': 3, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.899627896414064, 'Macro ROC value': 0.899627896414064}\n",
            "Epoch   175: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 175 is 5.519417405128479\n",
            "{'Validation Epoch Loss': 1.6322205364704132, 'Epoch': 175, 'Micro F1': 0.820779738584752, 'Patience Count': 4, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9001081860514344, 'Macro ROC value': 0.9001081860514344}\n",
            "Training Epoch Loss after 176 is 5.5291138887405396\n",
            "{'Validation Epoch Loss': 1.6346668601036072, 'Epoch': 176, 'Micro F1': 0.8197652749005366, 'Patience Count': 5, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8996824058342479, 'Macro ROC value': 0.8996824058342479}\n",
            "Training Epoch Loss after 177 is 5.523593246936798\n",
            "{'Validation Epoch Loss': 1.630041629076004, 'Epoch': 177, 'Micro F1': 0.820303019796831, 'Patience Count': 6, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9003027893265799, 'Macro ROC value': 0.9003027893265799}\n",
            "Training Epoch Loss after 178 is 5.531002342700958\n",
            "{'Validation Epoch Loss': 1.6419182419776917, 'Epoch': 178, 'Micro F1': 0.8190548833339112, 'Patience Count': 7, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8992831023718884, 'Macro ROC value': 0.8992831023718884}\n",
            "Training Epoch Loss after 179 is 5.508148610591888\n",
            "{'Validation Epoch Loss': 1.6325927674770355, 'Epoch': 179, 'Micro F1': 0.820255347918039, 'Patience Count': 8, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8997631892400335, 'Macro ROC value': 0.8997631892400335}\n",
            "Training Epoch Loss after 180 is 5.5151075422763824\n",
            "{'Validation Epoch Loss': 1.6404736638069153, 'Epoch': 180, 'Micro F1': 0.8192550729411358, 'Patience Count': 9, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8989367130163154, 'Macro ROC value': 0.8989367130163154}\n",
            "Training Epoch Loss after 181 is 5.510453939437866\n",
            "{'Validation Epoch Loss': 1.6310502886772156, 'Epoch': 181, 'Micro F1': 0.820964994777604, 'Patience Count': 10, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9001094175394878, 'Macro ROC value': 0.9001094175394878}\n",
            "Training Epoch Loss after 182 is 5.528059810400009\n",
            "{'Validation Epoch Loss': 1.6323516070842743, 'Epoch': 182, 'Micro F1': 0.8199966194106507, 'Patience Count': 11, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8997906020856732, 'Macro ROC value': 0.8997906020856732}\n",
            "Training Epoch Loss after 183 is 5.525139510631561\n",
            "{'Validation Epoch Loss': 1.6326488256454468, 'Epoch': 183, 'Micro F1': 0.8208534133065215, 'Patience Count': 12, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8999805915442941, 'Macro ROC value': 0.8999805915442941}\n",
            "Training Epoch Loss after 184 is 5.526063442230225\n",
            "{'Validation Epoch Loss': 1.6401779055595398, 'Epoch': 184, 'Micro F1': 0.8197266234441632, 'Patience Count': 13, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8990754627409487, 'Macro ROC value': 0.8990754627409487}\n",
            "Training Epoch Loss after 185 is 5.527828872203827\n",
            "{'Validation Epoch Loss': 1.6290271580219269, 'Epoch': 185, 'Micro F1': 0.8205189323186404, 'Patience Count': 14, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9001658381678705, 'Macro ROC value': 0.9001658381678705}\n",
            "Epoch   186: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Training Epoch Loss after 186 is 5.529736399650574\n",
            "{'Validation Epoch Loss': 1.6316066682338715, 'Epoch': 186, 'Micro F1': 0.8210050965572235, 'Patience Count': 15, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8999703222484197, 'Macro ROC value': 0.8999703222484197}\n",
            "Training Epoch Loss after 187 is 5.521875232458115\n",
            "{'Validation Epoch Loss': 1.6276161968708038, 'Epoch': 187, 'Micro F1': 0.8211567798079258, 'Patience Count': 16, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9007157857255222, 'Macro ROC value': 0.9007157857255222}\n",
            "Training Epoch Loss after 188 is 5.51157608628273\n",
            "{'Validation Epoch Loss': 1.6358884274959564, 'Epoch': 188, 'Micro F1': 0.8204503692403703, 'Patience Count': 17, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8998012002808822, 'Macro ROC value': 0.8998012002808822}\n",
            "Training Epoch Loss after 189 is 5.525055825710297\n",
            "{'Validation Epoch Loss': 1.6326850950717926, 'Epoch': 189, 'Micro F1': 0.8207364005131228, 'Patience Count': 18, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8999659616958533, 'Macro ROC value': 0.8999659616958533}\n",
            "Training Epoch Loss after 190 is 5.523816645145416\n",
            "{'Validation Epoch Loss': 1.6412613689899445, 'Epoch': 190, 'Micro F1': 0.8195879416149499, 'Patience Count': 19, 'Best Val F1': 0.8211741150365773, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8990878646446836, 'Macro ROC value': 0.8990878646446836}\n",
            "Training Epoch Loss after 191 is 5.512133061885834\n",
            "{'Validation Epoch Loss': 1.6284257769584656, 'Epoch': 191, 'Micro F1': 0.8215381548382623, 'Patience Count': 0, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9004583297377795, 'Macro ROC value': 0.9004583297377795}\n",
            "Training Epoch Loss after 192 is 5.520262658596039\n",
            "{'Validation Epoch Loss': 1.6348234713077545, 'Epoch': 192, 'Micro F1': 0.820363693097112, 'Patience Count': 1, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.8995539285822045, 'Macro ROC value': 0.8995539285822045}\n",
            "Training Epoch Loss after 193 is 5.521955162286758\n",
            "{'Validation Epoch Loss': 1.6332805454730988, 'Epoch': 193, 'Micro F1': 0.821243455951184, 'Patience Count': 2, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9004056346064073, 'Macro ROC value': 0.9004056346064073}\n",
            "Training Epoch Loss after 194 is 5.518730491399765\n",
            "{'Validation Epoch Loss': 1.6320258975028992, 'Epoch': 194, 'Micro F1': 0.8211481121935998, 'Patience Count': 3, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9003010632713921, 'Macro ROC value': 0.9003010632713921}\n",
            "Training Epoch Loss after 195 is 5.508918851613998\n",
            "{'Validation Epoch Loss': 1.6341278553009033, 'Epoch': 195, 'Micro F1': 0.8201454438290385, 'Patience Count': 4, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9000294758614507, 'Macro ROC value': 0.9000294758614507}\n",
            "Training Epoch Loss after 196 is 5.535503000020981\n",
            "{'Validation Epoch Loss': 1.6334372162818909, 'Epoch': 196, 'Micro F1': 0.8204547030475332, 'Patience Count': 5, 'Best Val F1': 0.8215381548382623, 'Best Val Loss': 1.6275304555892944, 'Micro ROC value': 0.9000445052831121, 'Macro ROC value': 0.9000445052831121}\n",
            "Training Epoch Loss after 197 is 5.513548284769058\n",
            "{'Validation Epoch Loss': 1.6273371577262878, 'Epoch': 197, 'Micro F1': 0.821655167631661, 'Patience Count': 0, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9005881984681894, 'Macro ROC value': 0.9005881984681894}\n",
            "Training Epoch Loss after 198 is 5.523165732622147\n",
            "{'Validation Epoch Loss': 1.6346811652183533, 'Epoch': 198, 'Micro F1': 0.8210171372103708, 'Patience Count': 1, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.8999337449646799, 'Macro ROC value': 0.8999337449646799}\n",
            "Training Epoch Loss after 199 is 5.514428585767746\n",
            "{'Validation Epoch Loss': 1.6344149708747864, 'Epoch': 199, 'Micro F1': 0.8201683294472518, 'Patience Count': 2, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.8997092013515396, 'Macro ROC value': 0.8997092013515396}\n",
            "Training Epoch Loss after 200 is 5.526192367076874\n",
            "{'Validation Epoch Loss': 1.6310678720474243, 'Epoch': 200, 'Micro F1': 0.8203853621329266, 'Patience Count': 3, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9001447472059254, 'Macro ROC value': 0.9001447472059254}\n",
            "Training Epoch Loss after 201 is 5.51162976026535\n",
            "{'Validation Epoch Loss': 1.6357599198818207, 'Epoch': 201, 'Micro F1': 0.8197293947352454, 'Patience Count': 4, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.8996101312034743, 'Macro ROC value': 0.8996101312034743}\n",
            "Training Epoch Loss after 202 is 5.5222718715667725\n",
            "{'Validation Epoch Loss': 1.6295082569122314, 'Epoch': 202, 'Micro F1': 0.820983427521409, 'Patience Count': 5, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9000922133358138, 'Macro ROC value': 0.9000922133358138}\n",
            "Training Epoch Loss after 203 is 5.497987300157547\n",
            "{'Validation Epoch Loss': 1.6300852298736572, 'Epoch': 203, 'Micro F1': 0.8207494019346115, 'Patience Count': 6, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9003602798487091, 'Macro ROC value': 0.9003602798487091}\n",
            "Training Epoch Loss after 204 is 5.505903661251068\n",
            "{'Validation Epoch Loss': 1.6285382211208344, 'Epoch': 204, 'Micro F1': 0.8214861491523072, 'Patience Count': 7, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.900373980482195, 'Macro ROC value': 0.900373980482195}\n",
            "Training Epoch Loss after 205 is 5.51597186923027\n",
            "{'Validation Epoch Loss': 1.6312882602214813, 'Epoch': 205, 'Micro F1': 0.8209097527996394, 'Patience Count': 8, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9001291421269125, 'Macro ROC value': 0.9001291421269125}\n",
            "Training Epoch Loss after 206 is 5.5249945521354675\n",
            "{'Validation Epoch Loss': 1.6320334672927856, 'Epoch': 206, 'Micro F1': 0.8207060638629824, 'Patience Count': 9, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.6273371577262878, 'Micro ROC value': 0.9002786888255585, 'Macro ROC value': 0.9002786888255585}\n",
            "Training Epoch Loss after 207 is 5.511444658041\n",
            "{'Validation Epoch Loss': 1.626122921705246, 'Epoch': 207, 'Micro F1': 0.8212087854938807, 'Patience Count': 0, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9009796124909324, 'Macro ROC value': 0.9009796124909324}\n",
            "Training Epoch Loss after 208 is 5.518274754285812\n",
            "{'Validation Epoch Loss': 1.6348749697208405, 'Epoch': 208, 'Micro F1': 0.8200646604028707, 'Patience Count': 1, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8998034328834958, 'Macro ROC value': 0.8998034328834958}\n",
            "Epoch   209: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Training Epoch Loss after 209 is 5.51778370141983\n",
            "{'Validation Epoch Loss': 1.6361647546291351, 'Epoch': 209, 'Micro F1': 0.8198861937306874, 'Patience Count': 2, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8996008829744564, 'Macro ROC value': 0.8996008829744564}\n",
            "Training Epoch Loss after 210 is 5.514697641134262\n",
            "{'Validation Epoch Loss': 1.6337863206863403, 'Epoch': 210, 'Micro F1': 0.8203766945186008, 'Patience Count': 3, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9002642196373578, 'Macro ROC value': 0.9002642196373578}\n",
            "Training Epoch Loss after 211 is 5.525356113910675\n",
            "{'Validation Epoch Loss': 1.6378298699855804, 'Epoch': 211, 'Micro F1': 0.8194839302430398, 'Patience Count': 4, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.899511111031723, 'Macro ROC value': 0.899511111031723}\n",
            "Training Epoch Loss after 212 is 5.520325273275375\n",
            "{'Validation Epoch Loss': 1.6270103752613068, 'Epoch': 212, 'Micro F1': 0.8212261207225323, 'Patience Count': 5, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9005428600131669, 'Macro ROC value': 0.9005428600131669}\n",
            "Training Epoch Loss after 213 is 5.533179879188538\n",
            "{'Validation Epoch Loss': 1.6364185214042664, 'Epoch': 213, 'Micro F1': 0.8204590368546961, 'Patience Count': 6, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8998322280668094, 'Macro ROC value': 0.8998322280668094}\n",
            "Training Epoch Loss after 214 is 5.515542447566986\n",
            "{'Validation Epoch Loss': 1.6328092515468597, 'Epoch': 214, 'Micro F1': 0.8201946746177582, 'Patience Count': 7, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.900104941781393, 'Macro ROC value': 0.900104941781393}\n",
            "Training Epoch Loss after 215 is 5.5227952003479\n",
            "{'Validation Epoch Loss': 1.6352052688598633, 'Epoch': 215, 'Micro F1': 0.8207667371632633, 'Patience Count': 8, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8996901405560388, 'Macro ROC value': 0.8996901405560388}\n",
            "Training Epoch Loss after 216 is 5.524538546800613\n",
            "{'Validation Epoch Loss': 1.6326104402542114, 'Epoch': 216, 'Micro F1': 0.8205658267675586, 'Patience Count': 9, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8996370579433676, 'Macro ROC value': 0.8996370579433676}\n",
            "Training Epoch Loss after 217 is 5.510754346847534\n",
            "{'Validation Epoch Loss': 1.6339513957500458, 'Epoch': 217, 'Micro F1': 0.820619387719724, 'Patience Count': 10, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9003110811905239, 'Macro ROC value': 0.9003110811905239}\n",
            "Training Epoch Loss after 218 is 5.510910719633102\n",
            "{'Validation Epoch Loss': 1.6375274956226349, 'Epoch': 218, 'Micro F1': 0.8200299899455674, 'Patience Count': 11, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8993683108986898, 'Macro ROC value': 0.8993683108986898}\n",
            "Training Epoch Loss after 219 is 5.522409737110138\n",
            "{'Validation Epoch Loss': 1.6344843208789825, 'Epoch': 219, 'Micro F1': 0.8204330340117186, 'Patience Count': 12, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9000797954299135, 'Macro ROC value': 0.9000797954299135}\n",
            "Epoch   220: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Training Epoch Loss after 220 is 5.522993057966232\n",
            "{'Validation Epoch Loss': 1.6284512281417847, 'Epoch': 220, 'Micro F1': 0.8208317442707069, 'Patience Count': 13, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9001878417140761, 'Macro ROC value': 0.9001878417140761}\n",
            "Training Epoch Loss after 221 is 5.520164966583252\n",
            "{'Validation Epoch Loss': 1.6400589644908905, 'Epoch': 221, 'Micro F1': 0.8207250742193426, 'Patience Count': 14, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8995365762248384, 'Macro ROC value': 0.8995365762248384}\n",
            "Training Epoch Loss after 222 is 5.521560877561569\n",
            "{'Validation Epoch Loss': 1.6276522874832153, 'Epoch': 222, 'Micro F1': 0.8208534133065215, 'Patience Count': 15, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.900256899097354, 'Macro ROC value': 0.900256899097354}\n",
            "Training Epoch Loss after 223 is 5.514026880264282\n",
            "{'Validation Epoch Loss': 1.6360696852207184, 'Epoch': 223, 'Micro F1': 0.8199953194882642, 'Patience Count': 16, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8993797717173185, 'Macro ROC value': 0.8993797717173185}\n",
            "Training Epoch Loss after 224 is 5.515212178230286\n",
            "{'Validation Epoch Loss': 1.6352022886276245, 'Epoch': 224, 'Micro F1': 0.820979093714246, 'Patience Count': 17, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8998825918779829, 'Macro ROC value': 0.8998825918779829}\n",
            "Training Epoch Loss after 225 is 5.532052993774414\n",
            "{'Validation Epoch Loss': 1.6307604312896729, 'Epoch': 225, 'Micro F1': 0.8205099312203937, 'Patience Count': 18, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.9001537518627181, 'Macro ROC value': 0.9001537518627181}\n",
            "Training Epoch Loss after 226 is 5.517647594213486\n",
            "{'Validation Epoch Loss': 1.6363720893859863, 'Epoch': 226, 'Micro F1': 0.8201079984744999, 'Patience Count': 19, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.8997197063600747, 'Macro ROC value': 0.8997197063600747}\n",
            "Training Epoch Loss after 227 is 5.527133792638779\n",
            "{'Validation Epoch Loss': 1.6368489563465118, 'Epoch': 227, 'Micro F1': 0.8200039871025899, 'Patience Count': 20, 'Best Val F1': 0.821655167631661, 'Best Val Loss': 1.626122921705246, 'Micro ROC value': 0.899265170818403, 'Macro ROC value': 0.899265170818403}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.8219570172769187, 'Micro Recall': 0.8219570172769187, 'Micro Precision': 0.8219570172769187, 'Micro ROC_AUC_Score': 0.9018130639611761, 'Macro ROC_AUC_Score': 0.9018130639611761}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Epoch Loss after 0 is 9.316532850265503\n",
            "{'Validation Epoch Loss': 2.102894365787506, 'Epoch': 0, 'Micro F1': 0.7444935603580004, 'Patience Count': 0, 'Best Val F1': 0.7444935603580004, 'Best Val Loss': 2.102894365787506, 'Micro ROC value': 0.8251674319666746, 'Macro ROC value': 0.8251674319666746}\n",
            "Training Epoch Loss after 1 is 7.711733013391495\n",
            "{'Validation Epoch Loss': 2.062634199857712, 'Epoch': 1, 'Micro F1': 0.7608382449246889, 'Patience Count': 0, 'Best Val F1': 0.7608382449246889, 'Best Val Loss': 2.062634199857712, 'Micro ROC value': 0.8403624234181779, 'Macro ROC value': 0.8403624234181779}\n",
            "Training Epoch Loss after 2 is 7.405671387910843\n",
            "{'Validation Epoch Loss': 2.0151142477989197, 'Epoch': 2, 'Micro F1': 0.7684457542021393, 'Patience Count': 0, 'Best Val F1': 0.7684457542021393, 'Best Val Loss': 2.0151142477989197, 'Micro ROC value': 0.8479678512644956, 'Macro ROC value': 0.8479678512644956}\n",
            "Training Epoch Loss after 3 is 7.2960638999938965\n",
            "{'Validation Epoch Loss': 1.9973521828651428, 'Epoch': 3, 'Micro F1': 0.7725211336014715, 'Patience Count': 0, 'Best Val F1': 0.7725211336014715, 'Best Val Loss': 1.9973521828651428, 'Micro ROC value': 0.8510732448766435, 'Macro ROC value': 0.8510732448766435}\n",
            "Training Epoch Loss after 4 is 7.2172576785087585\n",
            "{'Validation Epoch Loss': 1.968747317790985, 'Epoch': 4, 'Micro F1': 0.7753376811198733, 'Patience Count': 0, 'Best Val F1': 0.7753376811198733, 'Best Val Loss': 1.968747317790985, 'Micro ROC value': 0.8545232083719081, 'Macro ROC value': 0.8545232083719081}\n",
            "Training Epoch Loss after 5 is 7.172572910785675\n",
            "{'Validation Epoch Loss': 1.9672492742538452, 'Epoch': 5, 'Micro F1': 0.7757149094084261, 'Patience Count': 0, 'Best Val F1': 0.7757149094084261, 'Best Val Loss': 1.9672492742538452, 'Micro ROC value': 0.8557483101714555, 'Macro ROC value': 0.8557483101714555}\n",
            "Training Epoch Loss after 6 is 7.139518111944199\n",
            "{'Validation Epoch Loss': 1.9487294852733612, 'Epoch': 6, 'Micro F1': 0.7766481117659899, 'Patience Count': 0, 'Best Val F1': 0.7766481117659899, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8567126015835365, 'Macro ROC value': 0.8567126015835365}\n",
            "Training Epoch Loss after 7 is 7.128391295671463\n",
            "{'Validation Epoch Loss': 1.9575007259845734, 'Epoch': 7, 'Micro F1': 0.7797969875573019, 'Patience Count': 0, 'Best Val F1': 0.7797969875573019, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8590054859029025, 'Macro ROC value': 0.8590054859029025}\n",
            "Training Epoch Loss after 8 is 7.110894352197647\n",
            "{'Validation Epoch Loss': 1.986441433429718, 'Epoch': 8, 'Micro F1': 0.7783016808557085, 'Patience Count': 1, 'Best Val F1': 0.7797969875573019, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8570652131363083, 'Macro ROC value': 0.8570652131363083}\n",
            "Training Epoch Loss after 9 is 7.160292685031891\n",
            "{'Validation Epoch Loss': 1.9508432149887085, 'Epoch': 9, 'Micro F1': 0.7782525649421523, 'Patience Count': 2, 'Best Val F1': 0.7797969875573019, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8584459170522056, 'Macro ROC value': 0.8584459170522056}\n",
            "Training Epoch Loss after 10 is 7.1118543446063995\n",
            "{'Validation Epoch Loss': 2.024485409259796, 'Epoch': 10, 'Micro F1': 0.7733794691830354, 'Patience Count': 3, 'Best Val F1': 0.7797969875573019, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8591316299358344, 'Macro ROC value': 0.8591316299358344}\n",
            "Training Epoch Loss after 11 is 7.135782837867737\n",
            "{'Validation Epoch Loss': 1.9803370237350464, 'Epoch': 11, 'Micro F1': 0.7780069853743724, 'Patience Count': 4, 'Best Val F1': 0.7797969875573019, 'Best Val Loss': 1.9487294852733612, 'Micro ROC value': 0.8588786727574259, 'Macro ROC value': 0.8588786727574259}\n",
            "Training Epoch Loss after 12 is 7.084181696176529\n",
            "{'Validation Epoch Loss': 1.9444324970245361, 'Epoch': 12, 'Micro F1': 0.7804354944335298, 'Patience Count': 0, 'Best Val F1': 0.7804354944335298, 'Best Val Loss': 1.9444324970245361, 'Micro ROC value': 0.8598500049629362, 'Macro ROC value': 0.8598500049629362}\n",
            "Training Epoch Loss after 13 is 7.037788927555084\n",
            "{'Validation Epoch Loss': 1.9433594048023224, 'Epoch': 13, 'Micro F1': 0.7811795652624742, 'Patience Count': 0, 'Best Val F1': 0.7811795652624742, 'Best Val Loss': 1.9433594048023224, 'Micro ROC value': 0.8605582886908801, 'Macro ROC value': 0.8605582886908801}\n",
            "Training Epoch Loss after 14 is 6.990013927221298\n",
            "{'Validation Epoch Loss': 1.9325799643993378, 'Epoch': 14, 'Micro F1': 0.7826511678672778, 'Patience Count': 0, 'Best Val F1': 0.7826511678672778, 'Best Val Loss': 1.9325799643993378, 'Micro ROC value': 0.8632900383268286, 'Macro ROC value': 0.8632900383268286}\n",
            "Training Epoch Loss after 15 is 6.96462219953537\n",
            "{'Validation Epoch Loss': 1.91930490732193, 'Epoch': 15, 'Micro F1': 0.7838190351451648, 'Patience Count': 0, 'Best Val F1': 0.7838190351451648, 'Best Val Loss': 1.91930490732193, 'Micro ROC value': 0.8639240739144405, 'Macro ROC value': 0.8639240739144405}\n",
            "Training Epoch Loss after 16 is 6.946882605552673\n",
            "{'Validation Epoch Loss': 1.9082178473472595, 'Epoch': 16, 'Micro F1': 0.7839609255621045, 'Patience Count': 0, 'Best Val F1': 0.7839609255621045, 'Best Val Loss': 1.9082178473472595, 'Micro ROC value': 0.8647832138607112, 'Macro ROC value': 0.8647832138607112}\n",
            "Training Epoch Loss after 17 is 6.910133540630341\n",
            "{'Validation Epoch Loss': 1.9207095801830292, 'Epoch': 17, 'Micro F1': 0.7833948218651764, 'Patience Count': 1, 'Best Val F1': 0.7839609255621045, 'Best Val Loss': 1.9082178473472595, 'Micro ROC value': 0.8663285798263707, 'Macro ROC value': 0.8663285798263707}\n",
            "Training Epoch Loss after 18 is 6.882093697786331\n",
            "{'Validation Epoch Loss': 1.9128050804138184, 'Epoch': 18, 'Micro F1': 0.7847631521501856, 'Patience Count': 0, 'Best Val F1': 0.7847631521501856, 'Best Val Loss': 1.9082178473472595, 'Micro ROC value': 0.8674326444620661, 'Macro ROC value': 0.8674326444620661}\n",
            "Training Epoch Loss after 19 is 6.969039112329483\n",
            "{'Validation Epoch Loss': 1.8671521842479706, 'Epoch': 19, 'Micro F1': 0.7867878192534381, 'Patience Count': 0, 'Best Val F1': 0.7867878192534381, 'Best Val Loss': 1.8671521842479706, 'Micro ROC value': 0.867976176989026, 'Macro ROC value': 0.867976176989026}\n",
            "Training Epoch Loss after 20 is 6.900093257427216\n",
            "{'Validation Epoch Loss': 1.9312433302402496, 'Epoch': 20, 'Micro F1': 0.7827622150548197, 'Patience Count': 1, 'Best Val F1': 0.7867878192534381, 'Best Val Loss': 1.8671521842479706, 'Micro ROC value': 0.8674446195801173, 'Macro ROC value': 0.8674446195801173}\n",
            "Training Epoch Loss after 21 is 6.874832957983017\n",
            "{'Validation Epoch Loss': 1.924458235502243, 'Epoch': 21, 'Micro F1': 0.7832460161536782, 'Patience Count': 2, 'Best Val F1': 0.7867878192534381, 'Best Val Loss': 1.8671521842479706, 'Micro ROC value': 0.8693942351600703, 'Macro ROC value': 0.8693942351600703}\n",
            "Training Epoch Loss after 22 is 6.817560821771622\n",
            "{'Validation Epoch Loss': 1.8580136597156525, 'Epoch': 22, 'Micro F1': 0.7898002958176191, 'Patience Count': 0, 'Best Val F1': 0.7898002958176191, 'Best Val Loss': 1.8580136597156525, 'Micro ROC value': 0.8712662336420502, 'Macro ROC value': 0.8712662336420502}\n",
            "Training Epoch Loss after 23 is 6.806290507316589\n",
            "{'Validation Epoch Loss': 1.8669265806674957, 'Epoch': 23, 'Micro F1': 0.7888397729753329, 'Patience Count': 1, 'Best Val F1': 0.7898002958176191, 'Best Val Loss': 1.8580136597156525, 'Micro ROC value': 0.8712921861488477, 'Macro ROC value': 0.8712921861488477}\n",
            "Training Epoch Loss after 24 is 6.820288717746735\n",
            "{'Validation Epoch Loss': 1.9006302952766418, 'Epoch': 24, 'Micro F1': 0.7879272425629915, 'Patience Count': 2, 'Best Val F1': 0.7898002958176191, 'Best Val Loss': 1.8580136597156525, 'Micro ROC value': 0.8721855200389146, 'Macro ROC value': 0.8721855200389146}\n",
            "Training Epoch Loss after 25 is 6.725564569234848\n",
            "{'Validation Epoch Loss': 1.868508130311966, 'Epoch': 25, 'Micro F1': 0.7903121060472934, 'Patience Count': 0, 'Best Val F1': 0.7903121060472934, 'Best Val Loss': 1.8580136597156525, 'Micro ROC value': 0.8738196557929598, 'Macro ROC value': 0.8738196557929598}\n",
            "Training Epoch Loss after 26 is 6.684366315603256\n",
            "{'Validation Epoch Loss': 1.834465891122818, 'Epoch': 26, 'Micro F1': 0.7937240777122899, 'Patience Count': 0, 'Best Val F1': 0.7937240777122899, 'Best Val Loss': 1.834465891122818, 'Micro ROC value': 0.8758064941652026, 'Macro ROC value': 0.8758064941652026}\n",
            "Training Epoch Loss after 27 is 6.619018375873566\n",
            "{'Validation Epoch Loss': 1.8065983653068542, 'Epoch': 27, 'Micro F1': 0.7952630430037109, 'Patience Count': 0, 'Best Val F1': 0.7952630430037109, 'Best Val Loss': 1.8065983653068542, 'Micro ROC value': 0.8783470687297803, 'Macro ROC value': 0.8783470687297803}\n",
            "Training Epoch Loss after 28 is 6.624782592058182\n",
            "{'Validation Epoch Loss': 1.8069941997528076, 'Epoch': 28, 'Micro F1': 0.795115695263043, 'Patience Count': 1, 'Best Val F1': 0.7952630430037109, 'Best Val Loss': 1.8065983653068542, 'Micro ROC value': 0.8784464657032518, 'Macro ROC value': 0.8784464657032518}\n",
            "Training Epoch Loss after 29 is 6.649466723203659\n",
            "{'Validation Epoch Loss': 1.8476126790046692, 'Epoch': 29, 'Micro F1': 0.7961471294477188, 'Patience Count': 0, 'Best Val F1': 0.7961471294477188, 'Best Val Loss': 1.8065983653068542, 'Micro ROC value': 0.8794357500140216, 'Macro ROC value': 0.8794357500140216}\n",
            "Training Epoch Loss after 30 is 6.605886489152908\n",
            "{'Validation Epoch Loss': 1.8104960322380066, 'Epoch': 30, 'Micro F1': 0.7989554283344159, 'Patience Count': 0, 'Best Val F1': 0.7989554283344159, 'Best Val Loss': 1.8065983653068542, 'Micro ROC value': 0.8804759963261743, 'Macro ROC value': 0.8804759963261743}\n",
            "Training Epoch Loss after 31 is 6.514803111553192\n",
            "{'Validation Epoch Loss': 1.7880350351333618, 'Epoch': 31, 'Micro F1': 0.8013043003710979, 'Patience Count': 0, 'Best Val F1': 0.8013043003710979, 'Best Val Loss': 1.7880350351333618, 'Micro ROC value': 0.8832397396953076, 'Macro ROC value': 0.8832397396953076}\n",
            "Training Epoch Loss after 32 is 6.450900465250015\n",
            "{'Validation Epoch Loss': 1.7812207639217377, 'Epoch': 32, 'Micro F1': 0.8027341191879502, 'Patience Count': 0, 'Best Val F1': 0.8027341191879502, 'Best Val Loss': 1.7812207639217377, 'Micro ROC value': 0.8845529455347894, 'Macro ROC value': 0.8845529455347894}\n",
            "Training Epoch Loss after 33 is 6.403531104326248\n",
            "{'Validation Epoch Loss': 1.7912041246891022, 'Epoch': 33, 'Micro F1': 0.802854180309976, 'Patience Count': 0, 'Best Val F1': 0.802854180309976, 'Best Val Loss': 1.7812207639217377, 'Micro ROC value': 0.8846278036122075, 'Macro ROC value': 0.8846278036122075}\n",
            "Training Epoch Loss after 34 is 6.374729573726654\n",
            "{'Validation Epoch Loss': 1.7760741114616394, 'Epoch': 34, 'Micro F1': 0.8041912246234445, 'Patience Count': 0, 'Best Val F1': 0.8041912246234445, 'Best Val Loss': 1.7760741114616394, 'Micro ROC value': 0.8867580424106841, 'Macro ROC value': 0.8867580424106841}\n",
            "Training Epoch Loss after 35 is 6.353269547224045\n",
            "{'Validation Epoch Loss': 1.75403892993927, 'Epoch': 35, 'Micro F1': 0.8067179655097141, 'Patience Count': 0, 'Best Val F1': 0.8067179655097141, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.8880320254483547, 'Macro ROC value': 0.8880320254483547}\n",
            "Training Epoch Loss after 36 is 6.319052845239639\n",
            "{'Validation Epoch Loss': 1.7641614377498627, 'Epoch': 36, 'Micro F1': 0.8060248853962017, 'Patience Count': 1, 'Best Val F1': 0.8067179655097141, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.887330008416263, 'Macro ROC value': 0.887330008416263}\n",
            "Training Epoch Loss after 37 is 6.301600277423859\n",
            "{'Validation Epoch Loss': 1.761383444070816, 'Epoch': 37, 'Micro F1': 0.8041039074437896, 'Patience Count': 2, 'Best Val F1': 0.8067179655097141, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.8881474016825188, 'Macro ROC value': 0.8881474016825188}\n",
            "Training Epoch Loss after 38 is 6.334820240736008\n",
            "{'Validation Epoch Loss': 1.7603374421596527, 'Epoch': 38, 'Micro F1': 0.8040438768827768, 'Patience Count': 3, 'Best Val F1': 0.8067179655097141, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.8880724600357361, 'Macro ROC value': 0.8880724600357361}\n",
            "Training Epoch Loss after 39 is 6.364462167024612\n",
            "{'Validation Epoch Loss': 1.7899615466594696, 'Epoch': 39, 'Micro F1': 0.8048112291118654, 'Patience Count': 4, 'Best Val F1': 0.8067179655097141, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.8868690679019734, 'Macro ROC value': 0.8868690679019734}\n",
            "Training Epoch Loss after 40 is 6.350096017122269\n",
            "{'Validation Epoch Loss': 1.757507085800171, 'Epoch': 40, 'Micro F1': 0.8077439423706614, 'Patience Count': 0, 'Best Val F1': 0.8077439423706614, 'Best Val Loss': 1.75403892993927, 'Micro ROC value': 0.8896310470923318, 'Macro ROC value': 0.8896310470923318}\n",
            "Training Epoch Loss after 41 is 6.304199576377869\n",
            "{'Validation Epoch Loss': 1.7475543022155762, 'Epoch': 41, 'Micro F1': 0.8048930898612733, 'Patience Count': 0, 'Best Val F1': 0.8077439423706614, 'Best Val Loss': 1.7475543022155762, 'Micro ROC value': 0.8902628278272584, 'Macro ROC value': 0.8902628278272584}\n",
            "Training Epoch Loss after 42 is 6.253931313753128\n",
            "{'Validation Epoch Loss': 1.7388291954994202, 'Epoch': 42, 'Micro F1': 0.8089172669722768, 'Patience Count': 0, 'Best Val F1': 0.8089172669722768, 'Best Val Loss': 1.7388291954994202, 'Micro ROC value': 0.8913217798299506, 'Macro ROC value': 0.8913217798299506}\n",
            "Training Epoch Loss after 43 is 6.227193862199783\n",
            "{'Validation Epoch Loss': 1.731346994638443, 'Epoch': 43, 'Micro F1': 0.8111530128306581, 'Patience Count': 0, 'Best Val F1': 0.8111530128306581, 'Best Val Loss': 1.731346994638443, 'Micro ROC value': 0.8917017126652464, 'Macro ROC value': 0.8917017126652464}\n",
            "Training Epoch Loss after 44 is 6.185725033283234\n",
            "{'Validation Epoch Loss': 1.738644003868103, 'Epoch': 44, 'Micro F1': 0.8096727189377692, 'Patience Count': 1, 'Best Val F1': 0.8111530128306581, 'Best Val Loss': 1.731346994638443, 'Micro ROC value': 0.8911213470911572, 'Macro ROC value': 0.8911213470911572}\n",
            "Training Epoch Loss after 45 is 6.164528459310532\n",
            "{'Validation Epoch Loss': 1.7233389616012573, 'Epoch': 45, 'Micro F1': 0.8076110744243666, 'Patience Count': 0, 'Best Val F1': 0.8111530128306581, 'Best Val Loss': 1.7233389616012573, 'Micro ROC value': 0.8932863312846039, 'Macro ROC value': 0.8932863312846039}\n",
            "Training Epoch Loss after 46 is 6.17009574174881\n",
            "{'Validation Epoch Loss': 1.7335981130599976, 'Epoch': 46, 'Micro F1': 0.8074274175944116, 'Patience Count': 1, 'Best Val F1': 0.8111530128306581, 'Best Val Loss': 1.7233389616012573, 'Micro ROC value': 0.8920289066253697, 'Macro ROC value': 0.8920289066253697}\n",
            "Training Epoch Loss after 47 is 6.1549792885780334\n",
            "{'Validation Epoch Loss': 1.729755014181137, 'Epoch': 47, 'Micro F1': 0.8097085789129012, 'Patience Count': 2, 'Best Val F1': 0.8111530128306581, 'Best Val Loss': 1.7233389616012573, 'Micro ROC value': 0.8929243524869315, 'Macro ROC value': 0.8929243524869315}\n",
            "Training Epoch Loss after 48 is 6.139015108346939\n",
            "{'Validation Epoch Loss': 1.7123591005802155, 'Epoch': 48, 'Micro F1': 0.8125781083721261, 'Patience Count': 0, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8937523089788437, 'Macro ROC value': 0.8937523089788437}\n",
            "Training Epoch Loss after 49 is 6.126746118068695\n",
            "{'Validation Epoch Loss': 1.7279459834098816, 'Epoch': 49, 'Micro F1': 0.8091355599214145, 'Patience Count': 1, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8935809622864026, 'Macro ROC value': 0.8935809622864026}\n",
            "Training Epoch Loss after 50 is 6.115968614816666\n",
            "{'Validation Epoch Loss': 1.735308974981308, 'Epoch': 50, 'Micro F1': 0.8082733027723203, 'Patience Count': 2, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8935917421983619, 'Macro ROC value': 0.8935917421983619}\n",
            "Training Epoch Loss after 51 is 6.172797858715057\n",
            "{'Validation Epoch Loss': 1.7213785648345947, 'Epoch': 51, 'Micro F1': 0.8094193407552936, 'Patience Count': 3, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8937778162089638, 'Macro ROC value': 0.8937778162089638}\n",
            "Training Epoch Loss after 52 is 6.198192745447159\n",
            "{'Validation Epoch Loss': 1.7457596361637115, 'Epoch': 52, 'Micro F1': 0.8044750054573238, 'Patience Count': 4, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8933037684220118, 'Macro ROC value': 0.8933037684220118}\n",
            "Training Epoch Loss after 53 is 6.191883146762848\n",
            "{'Validation Epoch Loss': 1.7685838639736176, 'Epoch': 53, 'Micro F1': 0.8068925998690243, 'Patience Count': 5, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8917061641216072, 'Macro ROC value': 0.8917061641216072}\n",
            "Training Epoch Loss after 54 is 6.21148020029068\n",
            "{'Validation Epoch Loss': 1.7548890113830566, 'Epoch': 54, 'Micro F1': 0.8039118309966765, 'Patience Count': 6, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8910402982093404, 'Macro ROC value': 0.8910402982093404}\n",
            "Training Epoch Loss after 55 is 6.2341194450855255\n",
            "{'Validation Epoch Loss': 1.7531250715255737, 'Epoch': 55, 'Micro F1': 0.8038618551749731, 'Patience Count': 7, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8908792691202825, 'Macro ROC value': 0.8908792691202825}\n",
            "Training Epoch Loss after 56 is 6.245923012495041\n",
            "{'Validation Epoch Loss': 1.7405843138694763, 'Epoch': 56, 'Micro F1': 0.8082311399271958, 'Patience Count': 8, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8928606326655365, 'Macro ROC value': 0.8928606326655365}\n",
            "Training Epoch Loss after 57 is 6.259032845497131\n",
            "{'Validation Epoch Loss': 1.7309410572052002, 'Epoch': 57, 'Micro F1': 0.8108255608978732, 'Patience Count': 9, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8917026058475677, 'Macro ROC value': 0.8917026058475677}\n",
            "Training Epoch Loss after 58 is 6.371881693601608\n",
            "{'Validation Epoch Loss': 1.7640841901302338, 'Epoch': 58, 'Micro F1': 0.8036946081641564, 'Patience Count': 10, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8856369464580909, 'Macro ROC value': 0.8856369464580909}\n",
            "Training Epoch Loss after 59 is 6.390149086713791\n",
            "{'Validation Epoch Loss': 1.744521975517273, 'Epoch': 59, 'Micro F1': 0.8060676512001048, 'Patience Count': 11, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8867422620911012, 'Macro ROC value': 0.8867422620911012}\n",
            "Epoch    60: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 60 is 6.224128991365433\n",
            "{'Validation Epoch Loss': 1.7189172804355621, 'Epoch': 60, 'Micro F1': 0.8122735210652695, 'Patience Count': 12, 'Best Val F1': 0.8125781083721261, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8935104574998018, 'Macro ROC value': 0.8935104574998018}\n",
            "Training Epoch Loss after 61 is 6.089713871479034\n",
            "{'Validation Epoch Loss': 1.7150360643863678, 'Epoch': 61, 'Micro F1': 0.8135996507312814, 'Patience Count': 0, 'Best Val F1': 0.8135996507312814, 'Best Val Loss': 1.7123591005802155, 'Micro ROC value': 0.8939834722550424, 'Macro ROC value': 0.8939834722550424}\n",
            "Training Epoch Loss after 62 is 6.056896686553955\n",
            "{'Validation Epoch Loss': 1.702443391084671, 'Epoch': 62, 'Micro F1': 0.8140962671905698, 'Patience Count': 0, 'Best Val F1': 0.8140962671905698, 'Best Val Loss': 1.702443391084671, 'Micro ROC value': 0.8950140615730978, 'Macro ROC value': 0.8950140615730978}\n",
            "Training Epoch Loss after 63 is 6.025455415248871\n",
            "{'Validation Epoch Loss': 1.705630511045456, 'Epoch': 63, 'Micro F1': 0.8144548198195739, 'Patience Count': 0, 'Best Val F1': 0.8144548198195739, 'Best Val Loss': 1.702443391084671, 'Micro ROC value': 0.8953394630492879, 'Macro ROC value': 0.8953394630492879}\n",
            "Training Epoch Loss after 64 is 6.014023095369339\n",
            "{'Validation Epoch Loss': 1.6975851655006409, 'Epoch': 64, 'Micro F1': 0.8154824274175944, 'Patience Count': 0, 'Best Val F1': 0.8154824274175944, 'Best Val Loss': 1.6975851655006409, 'Micro ROC value': 0.8964278275667732, 'Macro ROC value': 0.8964278275667732}\n",
            "Training Epoch Loss after 65 is 6.008692711591721\n",
            "{'Validation Epoch Loss': 1.692150354385376, 'Epoch': 65, 'Micro F1': 0.8158753547260423, 'Patience Count': 0, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8967844988752028, 'Macro ROC value': 0.8967844988752028}\n",
            "Training Epoch Loss after 66 is 5.992616176605225\n",
            "{'Validation Epoch Loss': 1.7095608711242676, 'Epoch': 66, 'Micro F1': 0.814603798297315, 'Patience Count': 1, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8953433273120504, 'Macro ROC value': 0.8953433273120504}\n",
            "Training Epoch Loss after 67 is 6.002277314662933\n",
            "{'Validation Epoch Loss': 1.6996521949768066, 'Epoch': 67, 'Micro F1': 0.8152423051735429, 'Patience Count': 2, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8958662965336023, 'Macro ROC value': 0.8958662965336023}\n",
            "Training Epoch Loss after 68 is 5.976613372564316\n",
            "{'Validation Epoch Loss': 1.711470514535904, 'Epoch': 68, 'Micro F1': 0.8144127919668194, 'Patience Count': 3, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8951959476446687, 'Macro ROC value': 0.8951959476446687}\n",
            "Training Epoch Loss after 69 is 5.971704691648483\n",
            "{'Validation Epoch Loss': 1.707453578710556, 'Epoch': 69, 'Micro F1': 0.8151222440515172, 'Patience Count': 4, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8961640689519809, 'Macro ROC value': 0.8961640689519809}\n",
            "Training Epoch Loss after 70 is 5.970719784498215\n",
            "{'Validation Epoch Loss': 1.706083595752716, 'Epoch': 70, 'Micro F1': 0.8149350826543253, 'Patience Count': 5, 'Best Val F1': 0.8158753547260423, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8958724146467194, 'Macro ROC value': 0.8958724146467194}\n",
            "Training Epoch Loss after 71 is 5.972592800855637\n",
            "{'Validation Epoch Loss': 1.694125920534134, 'Epoch': 71, 'Micro F1': 0.8163828858327876, 'Patience Count': 0, 'Best Val F1': 0.8163828858327876, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8971095144895102, 'Macro ROC value': 0.8971095144895102}\n",
            "Training Epoch Loss after 72 is 5.96476885676384\n",
            "{'Validation Epoch Loss': 1.693246603012085, 'Epoch': 72, 'Micro F1': 0.8168904169395329, 'Patience Count': 0, 'Best Val F1': 0.8168904169395329, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8970886733144756, 'Macro ROC value': 0.8970886733144756}\n",
            "Training Epoch Loss after 73 is 5.946219772100449\n",
            "{'Validation Epoch Loss': 1.6968378722667694, 'Epoch': 73, 'Micro F1': 0.8167266972276795, 'Patience Count': 1, 'Best Val F1': 0.8168904169395329, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.897241358951519, 'Macro ROC value': 0.897241358951519}\n",
            "Training Epoch Loss after 74 is 5.939889013767242\n",
            "{'Validation Epoch Loss': 1.6944343149662018, 'Epoch': 74, 'Micro F1': 0.8166175507531107, 'Patience Count': 2, 'Best Val F1': 0.8168904169395329, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8970655531915492, 'Macro ROC value': 0.8970655531915492}\n",
            "Training Epoch Loss after 75 is 5.944869935512543\n",
            "{'Validation Epoch Loss': 1.6975921988487244, 'Epoch': 75, 'Micro F1': 0.8154804134513584, 'Patience Count': 3, 'Best Val F1': 0.8168904169395329, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8967231297207506, 'Macro ROC value': 0.8967231297207506}\n",
            "Training Epoch Loss after 76 is 5.939144819974899\n",
            "{'Validation Epoch Loss': 1.6969407796859741, 'Epoch': 76, 'Micro F1': 0.8172118711592827, 'Patience Count': 0, 'Best Val F1': 0.8172118711592827, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8975762343936828, 'Macro ROC value': 0.8975762343936828}\n",
            "Training Epoch Loss after 77 is 5.93592169880867\n",
            "{'Validation Epoch Loss': 1.6947153210639954, 'Epoch': 77, 'Micro F1': 0.8167572701074142, 'Patience Count': 1, 'Best Val F1': 0.8172118711592827, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8972018678989909, 'Macro ROC value': 0.8972018678989909}\n",
            "Training Epoch Loss after 78 is 5.923803299665451\n",
            "{'Validation Epoch Loss': 1.6923246383666992, 'Epoch': 78, 'Micro F1': 0.8179367253715215, 'Patience Count': 0, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8980932319504561, 'Macro ROC value': 0.8980932319504561}\n",
            "Training Epoch Loss after 79 is 5.9250098168849945\n",
            "{'Validation Epoch Loss': 1.700993686914444, 'Epoch': 79, 'Micro F1': 0.8159353852870552, 'Patience Count': 1, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8967250232195364, 'Macro ROC value': 0.8967250232195364}\n",
            "Training Epoch Loss after 80 is 5.933488577604294\n",
            "{'Validation Epoch Loss': 1.6937408745288849, 'Epoch': 80, 'Micro F1': 0.8170213927090155, 'Patience Count': 2, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8978125310779159, 'Macro ROC value': 0.8978125310779159}\n",
            "Training Epoch Loss after 81 is 5.927394419908524\n",
            "{'Validation Epoch Loss': 1.6980485022068024, 'Epoch': 81, 'Micro F1': 0.8159735865531543, 'Patience Count': 3, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8970818039484386, 'Macro ROC value': 0.8970818039484386}\n",
            "Training Epoch Loss after 82 is 5.906116992235184\n",
            "{'Validation Epoch Loss': 1.693693459033966, 'Epoch': 82, 'Micro F1': 0.816977734119188, 'Patience Count': 4, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.692150354385376, 'Micro ROC value': 0.8978827290464143, 'Macro ROC value': 0.8978827290464143}\n",
            "Training Epoch Loss after 83 is 5.923179239034653\n",
            "{'Validation Epoch Loss': 1.6875700056552887, 'Epoch': 83, 'Micro F1': 0.8172342283344248, 'Patience Count': 0, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8979537500212647, 'Macro ROC value': 0.8979537500212647}\n",
            "Training Epoch Loss after 84 is 5.92015814781189\n",
            "{'Validation Epoch Loss': 1.693130910396576, 'Epoch': 84, 'Micro F1': 0.816928618205632, 'Patience Count': 1, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8978372907209866, 'Macro ROC value': 0.8978372907209866}\n",
            "Training Epoch Loss after 85 is 5.912607312202454\n",
            "{'Validation Epoch Loss': 1.6973207890987396, 'Epoch': 85, 'Micro F1': 0.8160172451429819, 'Patience Count': 2, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8971023239502628, 'Macro ROC value': 0.8971023239502628}\n",
            "Training Epoch Loss after 86 is 5.901440918445587\n",
            "{'Validation Epoch Loss': 1.6944173276424408, 'Epoch': 86, 'Micro F1': 0.8173652041039073, 'Patience Count': 3, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8979183172062924, 'Macro ROC value': 0.8979183172062924}\n",
            "Training Epoch Loss after 87 is 5.904640376567841\n",
            "{'Validation Epoch Loss': 1.7055110931396484, 'Epoch': 87, 'Micro F1': 0.8155370006548789, 'Patience Count': 4, 'Best Val F1': 0.8179367253715215, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8967210426254509, 'Macro ROC value': 0.8967210426254509}\n",
            "Training Epoch Loss after 88 is 5.904061436653137\n",
            "{'Validation Epoch Loss': 1.6878802180290222, 'Epoch': 88, 'Micro F1': 0.8183911809648547, 'Patience Count': 0, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8986821392932979, 'Macro ROC value': 0.8986821392932979}\n",
            "Training Epoch Loss after 89 is 5.889891415834427\n",
            "{'Validation Epoch Loss': 1.689401388168335, 'Epoch': 89, 'Micro F1': 0.8176271556428727, 'Patience Count': 1, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8980418810938626, 'Macro ROC value': 0.8980418810938626}\n",
            "Training Epoch Loss after 90 is 5.902221292257309\n",
            "{'Validation Epoch Loss': 1.6892742216587067, 'Epoch': 90, 'Micro F1': 0.8177199301462563, 'Patience Count': 2, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6875700056552887, 'Micro ROC value': 0.8979871738358579, 'Macro ROC value': 0.8979871738358579}\n",
            "Training Epoch Loss after 91 is 5.888637483119965\n",
            "{'Validation Epoch Loss': 1.6863724887371063, 'Epoch': 91, 'Micro F1': 0.818036454922506, 'Patience Count': 0, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6863724887371063, 'Micro ROC value': 0.8984918982508974, 'Macro ROC value': 0.8984918982508974}\n",
            "Training Epoch Loss after 92 is 5.887346386909485\n",
            "{'Validation Epoch Loss': 1.7053062617778778, 'Epoch': 92, 'Micro F1': 0.8166557520192097, 'Patience Count': 1, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6863724887371063, 'Micro ROC value': 0.8970268316867964, 'Macro ROC value': 0.8970268316867964}\n",
            "Training Epoch Loss after 93 is 5.8906421065330505\n",
            "{'Validation Epoch Loss': 1.6928690075874329, 'Epoch': 93, 'Micro F1': 0.8173666746709162, 'Patience Count': 2, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6863724887371063, 'Micro ROC value': 0.8978687691439982, 'Macro ROC value': 0.8978687691439982}\n",
            "Training Epoch Loss after 94 is 5.881718665361404\n",
            "{'Validation Epoch Loss': 1.6832585334777832, 'Epoch': 94, 'Micro F1': 0.8183410736797297, 'Patience Count': 0, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6832585334777832, 'Micro ROC value': 0.8991219932787387, 'Macro ROC value': 0.8991219932787387}\n",
            "Training Epoch Loss after 95 is 5.882031828165054\n",
            "{'Validation Epoch Loss': 1.6858525574207306, 'Epoch': 95, 'Micro F1': 0.818320235756385, 'Patience Count': 1, 'Best Val F1': 0.8183911809648547, 'Best Val Loss': 1.6832585334777832, 'Micro ROC value': 0.8990997154543545, 'Macro ROC value': 0.8990997154543545}\n",
            "Training Epoch Loss after 96 is 5.879819601774216\n",
            "{'Validation Epoch Loss': 1.6828236281871796, 'Epoch': 96, 'Micro F1': 0.8192588954376774, 'Patience Count': 0, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6828236281871796, 'Micro ROC value': 0.8989189081602346, 'Macro ROC value': 0.8989189081602346}\n",
            "Training Epoch Loss after 97 is 5.869890064001083\n",
            "{'Validation Epoch Loss': 1.6856678426265717, 'Epoch': 97, 'Micro F1': 0.8190897184020957, 'Patience Count': 1, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6828236281871796, 'Micro ROC value': 0.8994123783490944, 'Macro ROC value': 0.8994123783490944}\n",
            "Training Epoch Loss after 98 is 5.876423537731171\n",
            "{'Validation Epoch Loss': 1.6942889988422394, 'Epoch': 98, 'Micro F1': 0.8171741977734118, 'Patience Count': 2, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6828236281871796, 'Micro ROC value': 0.8977656040503278, 'Macro ROC value': 0.8977656040503278}\n",
            "Training Epoch Loss after 99 is 5.867820978164673\n",
            "{'Validation Epoch Loss': 1.6769455075263977, 'Epoch': 99, 'Micro F1': 0.8181644536856639, 'Patience Count': 0, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8985585254661895, 'Macro ROC value': 0.8985585254661895}\n",
            "Training Epoch Loss after 100 is 5.873552560806274\n",
            "{'Validation Epoch Loss': 1.693930745124817, 'Epoch': 100, 'Micro F1': 0.8179491377428509, 'Patience Count': 1, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8979901028735902, 'Macro ROC value': 0.8979901028735902}\n",
            "Training Epoch Loss after 101 is 5.868259429931641\n",
            "{'Validation Epoch Loss': 1.6860142648220062, 'Epoch': 101, 'Micro F1': 0.8186749617987339, 'Patience Count': 2, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8988097840224525, 'Macro ROC value': 0.8988097840224525}\n",
            "Training Epoch Loss after 102 is 5.858706951141357\n",
            "{'Validation Epoch Loss': 1.6868938505649567, 'Epoch': 102, 'Micro F1': 0.818614931237721, 'Patience Count': 3, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8986117777003912, 'Macro ROC value': 0.8986117777003912}\n",
            "Training Epoch Loss after 103 is 5.869185835123062\n",
            "{'Validation Epoch Loss': 1.6815576553344727, 'Epoch': 103, 'Micro F1': 0.8179436804191225, 'Patience Count': 4, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8987879565144815, 'Macro ROC value': 0.8987879565144815}\n",
            "Training Epoch Loss after 104 is 5.864965945482254\n",
            "{'Validation Epoch Loss': 1.6915872395038605, 'Epoch': 104, 'Micro F1': 0.817345841724106, 'Patience Count': 5, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8983568726575881, 'Macro ROC value': 0.8983568726575881}\n",
            "Training Epoch Loss after 105 is 5.869927674531937\n",
            "{'Validation Epoch Loss': 1.6829290688037872, 'Epoch': 105, 'Micro F1': 0.8188877974241432, 'Patience Count': 6, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8995393013319969, 'Macro ROC value': 0.8995393013319969}\n",
            "Training Epoch Loss after 106 is 5.862247079610825\n",
            "{'Validation Epoch Loss': 1.6944402754306793, 'Epoch': 106, 'Micro F1': 0.818282034490286, 'Patience Count': 7, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8978756685966126, 'Macro ROC value': 0.8978756685966126}\n",
            "Training Epoch Loss after 107 is 5.859489023685455\n",
            "{'Validation Epoch Loss': 1.6894327998161316, 'Epoch': 107, 'Micro F1': 0.8182602051953723, 'Patience Count': 8, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8985633988412318, 'Macro ROC value': 0.8985633988412318}\n",
            "Epoch   108: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 108 is 5.844110786914825\n",
            "{'Validation Epoch Loss': 1.6861101984977722, 'Epoch': 108, 'Micro F1': 0.8182110892818163, 'Patience Count': 9, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8983516841055618, 'Macro ROC value': 0.8983516841055618}\n",
            "Training Epoch Loss after 109 is 5.860991179943085\n",
            "{'Validation Epoch Loss': 1.6873672306537628, 'Epoch': 109, 'Micro F1': 0.8190897184020957, 'Patience Count': 10, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8990792130837008, 'Macro ROC value': 0.8990792130837008}\n",
            "Training Epoch Loss after 110 is 5.85686394572258\n",
            "{'Validation Epoch Loss': 1.6940533518791199, 'Epoch': 110, 'Micro F1': 0.8170149367765596, 'Patience Count': 11, 'Best Val F1': 0.8192588954376774, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8976160060813334, 'Macro ROC value': 0.8976160060813334}\n",
            "Training Epoch Loss after 111 is 5.852126926183701\n",
            "{'Validation Epoch Loss': 1.683410108089447, 'Epoch': 111, 'Micro F1': 0.8196856409976531, 'Patience Count': 0, 'Best Val F1': 0.8196856409976531, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8994435636832686, 'Macro ROC value': 0.8994435636832686}\n",
            "Training Epoch Loss after 112 is 5.851244747638702\n",
            "{'Validation Epoch Loss': 1.692666918039322, 'Epoch': 112, 'Micro F1': 0.8179674836681567, 'Patience Count': 1, 'Best Val F1': 0.8196856409976531, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8982558011873667, 'Macro ROC value': 0.8982558011873667}\n",
            "Training Epoch Loss after 113 is 5.839599758386612\n",
            "{'Validation Epoch Loss': 1.6866995096206665, 'Epoch': 113, 'Micro F1': 0.8181947173106309, 'Patience Count': 2, 'Best Val F1': 0.8196856409976531, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8986192496235864, 'Macro ROC value': 0.8986192496235864}\n",
            "Training Epoch Loss after 114 is 5.84239661693573\n",
            "{'Validation Epoch Loss': 1.687851220369339, 'Epoch': 114, 'Micro F1': 0.8185439860292513, 'Patience Count': 3, 'Best Val F1': 0.8196856409976531, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8986708360453171, 'Macro ROC value': 0.8986708360453171}\n",
            "Training Epoch Loss after 115 is 5.845346957445145\n",
            "{'Validation Epoch Loss': 1.6774441599845886, 'Epoch': 115, 'Micro F1': 0.8204404661190404, 'Patience Count': 0, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.9002454652913794, 'Macro ROC value': 0.9002454652913794}\n",
            "Training Epoch Loss after 116 is 5.848026126623154\n",
            "{'Validation Epoch Loss': 1.6858126819133759, 'Epoch': 116, 'Micro F1': 0.8190242305173542, 'Patience Count': 1, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.89899264743268, 'Macro ROC value': 0.89899264743268}\n",
            "Training Epoch Loss after 117 is 5.842938214540482\n",
            "{'Validation Epoch Loss': 1.6870291829109192, 'Epoch': 117, 'Micro F1': 0.8185876446190788, 'Patience Count': 2, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.899042187864313, 'Macro ROC value': 0.899042187864313}\n",
            "Training Epoch Loss after 118 is 5.851558089256287\n",
            "{'Validation Epoch Loss': 1.6826262474060059, 'Epoch': 118, 'Micro F1': 0.8190460598122681, 'Patience Count': 3, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8988441253131141, 'Macro ROC value': 0.8988441253131141}\n",
            "Training Epoch Loss after 119 is 5.840929746627808\n",
            "{'Validation Epoch Loss': 1.6893227994441986, 'Epoch': 119, 'Micro F1': 0.8181619733682602, 'Patience Count': 4, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8984857588510501, 'Macro ROC value': 0.8984857588510501}\n",
            "Training Epoch Loss after 120 is 5.852502822875977\n",
            "{'Validation Epoch Loss': 1.681672751903534, 'Epoch': 120, 'Micro F1': 0.8192752674088627, 'Patience Count': 5, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.898872230172927, 'Macro ROC value': 0.898872230172927}\n",
            "Training Epoch Loss after 121 is 5.841981500387192\n",
            "{'Validation Epoch Loss': 1.6898746490478516, 'Epoch': 121, 'Micro F1': 0.8176380702903296, 'Patience Count': 6, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8985258960911267, 'Macro ROC value': 0.8985258960911267}\n",
            "Training Epoch Loss after 122 is 5.852199554443359\n",
            "{'Validation Epoch Loss': 1.6891387701034546, 'Epoch': 122, 'Micro F1': 0.8191115476970093, 'Patience Count': 7, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8989385407066316, 'Macro ROC value': 0.8989385407066316}\n",
            "Training Epoch Loss after 123 is 5.851977288722992\n",
            "{'Validation Epoch Loss': 1.690442830324173, 'Epoch': 123, 'Micro F1': 0.819193407552936, 'Patience Count': 8, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.899301271598124, 'Macro ROC value': 0.899301271598124}\n",
            "Training Epoch Loss after 124 is 5.8554913103580475\n",
            "{'Validation Epoch Loss': 1.6920152306556702, 'Epoch': 124, 'Micro F1': 0.818345539680634, 'Patience Count': 9, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8985828941301022, 'Macro ROC value': 0.8985828941301022}\n",
            "Training Epoch Loss after 125 is 5.842199742794037\n",
            "{'Validation Epoch Loss': 1.679468184709549, 'Epoch': 125, 'Micro F1': 0.818996943898712, 'Patience Count': 10, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8993366982779255, 'Macro ROC value': 0.8993366982779255}\n",
            "Training Epoch Loss after 126 is 5.847195029258728\n",
            "{'Validation Epoch Loss': 1.6840397715568542, 'Epoch': 126, 'Micro F1': 0.8197718838681509, 'Patience Count': 11, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8996122641181715, 'Macro ROC value': 0.8996122641181715}\n",
            "Epoch   127: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 127 is 5.845802575349808\n",
            "{'Validation Epoch Loss': 1.6931101083755493, 'Epoch': 127, 'Micro F1': 0.8186348625856281, 'Patience Count': 12, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8988326581053453, 'Macro ROC value': 0.8988326581053453}\n",
            "Training Epoch Loss after 128 is 5.8380473256111145\n",
            "{'Validation Epoch Loss': 1.6839767098426819, 'Epoch': 128, 'Micro F1': 0.8187895655970312, 'Patience Count': 13, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8991733557301612, 'Macro ROC value': 0.8991733557301612}\n",
            "Training Epoch Loss after 129 is 5.851903557777405\n",
            "{'Validation Epoch Loss': 1.6801565289497375, 'Epoch': 129, 'Micro F1': 0.818664047151277, 'Patience Count': 14, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8989272603910851, 'Macro ROC value': 0.8989272603910851}\n",
            "Training Epoch Loss after 130 is 5.84715136885643\n",
            "{'Validation Epoch Loss': 1.6843999028205872, 'Epoch': 130, 'Micro F1': 0.818320235756385, 'Patience Count': 15, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8987081247196584, 'Macro ROC value': 0.8987081247196584}\n",
            "Training Epoch Loss after 131 is 5.853741884231567\n",
            "{'Validation Epoch Loss': 1.6969949901103973, 'Epoch': 131, 'Micro F1': 0.8178072473259114, 'Patience Count': 16, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8980965917470588, 'Macro ROC value': 0.8980965917470588}\n",
            "Training Epoch Loss after 132 is 5.839039504528046\n",
            "{'Validation Epoch Loss': 1.684228628873825, 'Epoch': 132, 'Micro F1': 0.8189751622286865, 'Patience Count': 17, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8990951056215923, 'Macro ROC value': 0.8990951056215923}\n",
            "Training Epoch Loss after 133 is 5.847052335739136\n",
            "{'Validation Epoch Loss': 1.6854381263256073, 'Epoch': 133, 'Micro F1': 0.8185112420868806, 'Patience Count': 18, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8990422303935549, 'Macro ROC value': 0.8990422303935549}\n",
            "Training Epoch Loss after 134 is 5.837699621915817\n",
            "{'Validation Epoch Loss': 1.6868042349815369, 'Epoch': 134, 'Micro F1': 0.8185603580004366, 'Patience Count': 19, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8985456390705971, 'Macro ROC value': 0.8985456390705971}\n",
            "Training Epoch Loss after 135 is 5.838884860277176\n",
            "{'Validation Epoch Loss': 1.6856686770915985, 'Epoch': 135, 'Micro F1': 0.8192588954376774, 'Patience Count': 20, 'Best Val F1': 0.8204404661190404, 'Best Val Loss': 1.6769455075263977, 'Micro ROC value': 0.8991975738103928, 'Macro ROC value': 0.8991975738103928}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.8196434787298195, 'Micro Recall': 0.8196434787298195, 'Micro Precision': 0.8196434787298195, 'Micro ROC_AUC_Score': 0.9000851505197361, 'Macro ROC_AUC_Score': 0.9000851505197361}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Epoch Loss after 0 is 9.64879584312439\n",
            "{'Validation Epoch Loss': 2.152032971382141, 'Epoch': 0, 'Micro F1': 0.7408418880625931, 'Patience Count': 0, 'Best Val F1': 0.7408418880625931, 'Best Val Loss': 2.152032971382141, 'Micro ROC value': 0.8220913309770166, 'Macro ROC value': 0.8220913309770166}\n",
            "Training Epoch Loss after 1 is 8.660727381706238\n",
            "{'Validation Epoch Loss': 2.07002192735672, 'Epoch': 1, 'Micro F1': 0.7518368794988377, 'Patience Count': 0, 'Best Val F1': 0.7518368794988377, 'Best Val Loss': 2.07002192735672, 'Micro ROC value': 0.8310562133604883, 'Macro ROC value': 0.8310562133604883}\n",
            "Training Epoch Loss after 2 is 8.21277117729187\n",
            "{'Validation Epoch Loss': 2.0354670882225037, 'Epoch': 2, 'Micro F1': 0.7527611543165501, 'Patience Count': 0, 'Best Val F1': 0.7527611543165501, 'Best Val Loss': 2.0354670882225037, 'Micro ROC value': 0.8419464827169993, 'Macro ROC value': 0.8419464827169993}\n",
            "Training Epoch Loss after 3 is 8.057310283184052\n",
            "{'Validation Epoch Loss': 1.9748220443725586, 'Epoch': 3, 'Micro F1': 0.7673301528321087, 'Patience Count': 0, 'Best Val F1': 0.7673301528321087, 'Best Val Loss': 1.9748220443725586, 'Micro ROC value': 0.8448086653049856, 'Macro ROC value': 0.8448086653049856}\n",
            "Training Epoch Loss after 4 is 7.808748096227646\n",
            "{'Validation Epoch Loss': 1.9573141634464264, 'Epoch': 4, 'Micro F1': 0.7695762181743491, 'Patience Count': 0, 'Best Val F1': 0.7695762181743491, 'Best Val Loss': 1.9573141634464264, 'Micro ROC value': 0.8493632462762487, 'Macro ROC value': 0.8493632462762487}\n",
            "Training Epoch Loss after 5 is 7.799204260110855\n",
            "{'Validation Epoch Loss': 1.9647085666656494, 'Epoch': 5, 'Micro F1': 0.7679229957707424, 'Patience Count': 1, 'Best Val F1': 0.7695762181743491, 'Best Val Loss': 1.9573141634464264, 'Micro ROC value': 0.8503520705936987, 'Macro ROC value': 0.8503520705936987}\n",
            "Training Epoch Loss after 6 is 7.761802971363068\n",
            "{'Validation Epoch Loss': 1.9384858012199402, 'Epoch': 6, 'Micro F1': 0.7719608630299409, 'Patience Count': 0, 'Best Val F1': 0.7719608630299409, 'Best Val Loss': 1.9384858012199402, 'Micro ROC value': 0.851494450350533, 'Macro ROC value': 0.851494450350533}\n",
            "Training Epoch Loss after 7 is 7.698257893323898\n",
            "{'Validation Epoch Loss': 1.933492749929428, 'Epoch': 7, 'Micro F1': 0.7744115917132695, 'Patience Count': 0, 'Best Val F1': 0.7744115917132695, 'Best Val Loss': 1.933492749929428, 'Micro ROC value': 0.8523178059594334, 'Macro ROC value': 0.8523178059594334}\n",
            "Training Epoch Loss after 8 is 7.636560022830963\n",
            "{'Validation Epoch Loss': 1.9250009059906006, 'Epoch': 8, 'Micro F1': 0.7758586886500921, 'Patience Count': 0, 'Best Val F1': 0.7758586886500921, 'Best Val Loss': 1.9250009059906006, 'Micro ROC value': 0.8535848780058142, 'Macro ROC value': 0.8535848780058142}\n",
            "Training Epoch Loss after 9 is 7.608863800764084\n",
            "{'Validation Epoch Loss': 1.9260213673114777, 'Epoch': 9, 'Micro F1': 0.77569530673787, 'Patience Count': 1, 'Best Val F1': 0.7758586886500921, 'Best Val Loss': 1.9250009059906006, 'Micro ROC value': 0.8538307693994911, 'Macro ROC value': 0.8538307693994911}\n",
            "Training Epoch Loss after 10 is 7.605283975601196\n",
            "{'Validation Epoch Loss': 1.9166970252990723, 'Epoch': 10, 'Micro F1': 0.776909000942947, 'Patience Count': 0, 'Best Val F1': 0.776909000942947, 'Best Val Loss': 1.9166970252990723, 'Micro ROC value': 0.8551458312666873, 'Macro ROC value': 0.8551458312666873}\n",
            "Training Epoch Loss after 11 is 7.623316079378128\n",
            "{'Validation Epoch Loss': 1.9310502409934998, 'Epoch': 11, 'Micro F1': 0.7737720682282867, 'Patience Count': 1, 'Best Val F1': 0.776909000942947, 'Best Val Loss': 1.9166970252990723, 'Micro ROC value': 0.8545540364623627, 'Macro ROC value': 0.8545540364623627}\n",
            "Training Epoch Loss after 12 is 7.660691857337952\n",
            "{'Validation Epoch Loss': 1.942742943763733, 'Epoch': 12, 'Micro F1': 0.773001839213526, 'Patience Count': 2, 'Best Val F1': 0.776909000942947, 'Best Val Loss': 1.9166970252990723, 'Micro ROC value': 0.8565223137172272, 'Macro ROC value': 0.8565223137172272}\n",
            "Training Epoch Loss after 13 is 7.672520250082016\n",
            "{'Validation Epoch Loss': 1.912425011396408, 'Epoch': 13, 'Micro F1': 0.7765308885175192, 'Patience Count': 0, 'Best Val F1': 0.776909000942947, 'Best Val Loss': 1.912425011396408, 'Micro ROC value': 0.8548103514972656, 'Macro ROC value': 0.8548103514972656}\n",
            "Training Epoch Loss after 14 is 7.584145605564117\n",
            "{'Validation Epoch Loss': 1.923307716846466, 'Epoch': 14, 'Micro F1': 0.7764702038072654, 'Patience Count': 1, 'Best Val F1': 0.776909000942947, 'Best Val Loss': 1.912425011396408, 'Micro ROC value': 0.8567113243219348, 'Macro ROC value': 0.8567113243219348}\n",
            "Training Epoch Loss after 15 is 7.48832231760025\n",
            "{'Validation Epoch Loss': 1.9070523381233215, 'Epoch': 15, 'Micro F1': 0.7778047505228561, 'Patience Count': 0, 'Best Val F1': 0.7778047505228561, 'Best Val Loss': 1.9070523381233215, 'Micro ROC value': 0.8568674574613351, 'Macro ROC value': 0.8568674574613351}\n",
            "Training Epoch Loss after 16 is 7.495120972394943\n",
            "{'Validation Epoch Loss': 1.903732419013977, 'Epoch': 16, 'Micro F1': 0.7779966576728814, 'Patience Count': 0, 'Best Val F1': 0.7779966576728814, 'Best Val Loss': 1.903732419013977, 'Micro ROC value': 0.8584315088395846, 'Macro ROC value': 0.8584315088395846}\n",
            "Training Epoch Loss after 17 is 7.493436574935913\n",
            "{'Validation Epoch Loss': 1.9202521443367004, 'Epoch': 17, 'Micro F1': 0.7767129426482807, 'Patience Count': 1, 'Best Val F1': 0.7779966576728814, 'Best Val Loss': 1.903732419013977, 'Micro ROC value': 0.8590228690930903, 'Macro ROC value': 0.8590228690930903}\n",
            "Training Epoch Loss after 18 is 7.486165016889572\n",
            "{'Validation Epoch Loss': 1.8989459872245789, 'Epoch': 18, 'Micro F1': 0.7781927159675477, 'Patience Count': 0, 'Best Val F1': 0.7781927159675477, 'Best Val Loss': 1.8989459872245789, 'Micro ROC value': 0.8602618305695376, 'Macro ROC value': 0.8602618305695376}\n",
            "Training Epoch Loss after 19 is 7.482504576444626\n",
            "{'Validation Epoch Loss': 1.8763235211372375, 'Epoch': 19, 'Micro F1': 0.7818244624735088, 'Patience Count': 0, 'Best Val F1': 0.7818244624735088, 'Best Val Loss': 1.8763235211372375, 'Micro ROC value': 0.8621371699513412, 'Macro ROC value': 0.8621371699513412}\n",
            "Training Epoch Loss after 20 is 7.403804033994675\n",
            "{'Validation Epoch Loss': 1.8765074610710144, 'Epoch': 20, 'Micro F1': 0.782230583226746, 'Patience Count': 0, 'Best Val F1': 0.782230583226746, 'Best Val Loss': 1.8763235211372375, 'Micro ROC value': 0.8629869424671601, 'Macro ROC value': 0.8629869424671601}\n",
            "Training Epoch Loss after 21 is 7.3232075572013855\n",
            "{'Validation Epoch Loss': 1.879632979631424, 'Epoch': 21, 'Micro F1': 0.7818920243680415, 'Patience Count': 1, 'Best Val F1': 0.782230583226746, 'Best Val Loss': 1.8763235211372375, 'Micro ROC value': 0.8631247121221027, 'Macro ROC value': 0.8631247121221027}\n",
            "Training Epoch Loss after 22 is 7.301984935998917\n",
            "{'Validation Epoch Loss': 1.8786961138248444, 'Epoch': 22, 'Micro F1': 0.7825806873243645, 'Patience Count': 0, 'Best Val F1': 0.7825806873243645, 'Best Val Loss': 1.8763235211372375, 'Micro ROC value': 0.8639409745805804, 'Macro ROC value': 0.8639409745805804}\n",
            "Training Epoch Loss after 23 is 7.264349043369293\n",
            "{'Validation Epoch Loss': 1.882140338420868, 'Epoch': 23, 'Micro F1': 0.7815350430861443, 'Patience Count': 1, 'Best Val F1': 0.7825806873243645, 'Best Val Loss': 1.8763235211372375, 'Micro ROC value': 0.8645726869154455, 'Macro ROC value': 0.8645726869154455}\n",
            "Training Epoch Loss after 24 is 7.224350839853287\n",
            "{'Validation Epoch Loss': 1.8578925132751465, 'Epoch': 24, 'Micro F1': 0.7849193826964551, 'Patience Count': 0, 'Best Val F1': 0.7849193826964551, 'Best Val Loss': 1.8578925132751465, 'Micro ROC value': 0.8665954135491714, 'Macro ROC value': 0.8665954135491714}\n",
            "Training Epoch Loss after 25 is 7.1838513016700745\n",
            "{'Validation Epoch Loss': 1.8551893830299377, 'Epoch': 25, 'Micro F1': 0.7856569353287711, 'Patience Count': 0, 'Best Val F1': 0.7856569353287711, 'Best Val Loss': 1.8551893830299377, 'Micro ROC value': 0.8671510878583684, 'Macro ROC value': 0.8671510878583684}\n",
            "Training Epoch Loss after 26 is 7.155117392539978\n",
            "{'Validation Epoch Loss': 1.8523196578025818, 'Epoch': 26, 'Micro F1': 0.78714405751097, 'Patience Count': 0, 'Best Val F1': 0.78714405751097, 'Best Val Loss': 1.8523196578025818, 'Micro ROC value': 0.8682513522312312, 'Macro ROC value': 0.8682513522312312}\n",
            "Training Epoch Loss after 27 is 7.115911215543747\n",
            "{'Validation Epoch Loss': 1.8405646085739136, 'Epoch': 27, 'Micro F1': 0.7880983279028297, 'Patience Count': 0, 'Best Val F1': 0.7880983279028297, 'Best Val Loss': 1.8405646085739136, 'Micro ROC value': 0.8694944632699977, 'Macro ROC value': 0.8694944632699977}\n",
            "Training Epoch Loss after 28 is 7.083472430706024\n",
            "{'Validation Epoch Loss': 1.839065283536911, 'Epoch': 28, 'Micro F1': 0.7888815755691552, 'Patience Count': 0, 'Best Val F1': 0.7888815755691552, 'Best Val Loss': 1.839065283536911, 'Micro ROC value': 0.8702753779435239, 'Macro ROC value': 0.8702753779435239}\n",
            "Training Epoch Loss after 29 is 7.0512294471263885\n",
            "{'Validation Epoch Loss': 1.817697286605835, 'Epoch': 29, 'Micro F1': 0.791398642529712, 'Patience Count': 0, 'Best Val F1': 0.791398642529712, 'Best Val Loss': 1.817697286605835, 'Micro ROC value': 0.8723375491995258, 'Macro ROC value': 0.8723375491995258}\n",
            "Training Epoch Loss after 30 is 7.032110512256622\n",
            "{'Validation Epoch Loss': 1.8410039246082306, 'Epoch': 30, 'Micro F1': 0.7892746776708276, 'Patience Count': 1, 'Best Val F1': 0.791398642529712, 'Best Val Loss': 1.817697286605835, 'Micro ROC value': 0.8737432103582021, 'Macro ROC value': 0.8737432103582021}\n",
            "Training Epoch Loss after 31 is 7.012299537658691\n",
            "{'Validation Epoch Loss': 1.7868632078170776, 'Epoch': 31, 'Micro F1': 0.794885679341991, 'Patience Count': 0, 'Best Val F1': 0.794885679341991, 'Best Val Loss': 1.7868632078170776, 'Micro ROC value': 0.8758866044021468, 'Macro ROC value': 0.8758866044021468}\n",
            "Training Epoch Loss after 32 is 7.04893684387207\n",
            "{'Validation Epoch Loss': 1.7967053651809692, 'Epoch': 32, 'Micro F1': 0.7954925264445295, 'Patience Count': 0, 'Best Val F1': 0.7954925264445295, 'Best Val Loss': 1.7868632078170776, 'Micro ROC value': 0.8754777698308878, 'Macro ROC value': 0.8754777698308878}\n",
            "Training Epoch Loss after 33 is 7.078219532966614\n",
            "{'Validation Epoch Loss': 1.8403911590576172, 'Epoch': 33, 'Micro F1': 0.7891299679771453, 'Patience Count': 1, 'Best Val F1': 0.7954925264445295, 'Best Val Loss': 1.7868632078170776, 'Micro ROC value': 0.8734257148302638, 'Macro ROC value': 0.8734257148302638}\n",
            "Training Epoch Loss after 34 is 6.927750468254089\n",
            "{'Validation Epoch Loss': 1.8038497567176819, 'Epoch': 34, 'Micro F1': 0.794726965484404, 'Patience Count': 2, 'Best Val F1': 0.7954925264445295, 'Best Val Loss': 1.7868632078170776, 'Micro ROC value': 0.878633883141914, 'Macro ROC value': 0.878633883141914}\n",
            "Training Epoch Loss after 35 is 6.85690039396286\n",
            "{'Validation Epoch Loss': 1.7874988615512848, 'Epoch': 35, 'Micro F1': 0.7976752870880404, 'Patience Count': 0, 'Best Val F1': 0.7976752870880404, 'Best Val Loss': 1.7868632078170776, 'Micro ROC value': 0.8800574976146813, 'Macro ROC value': 0.8800574976146813}\n",
            "Training Epoch Loss after 36 is 6.781797468662262\n",
            "{'Validation Epoch Loss': 1.7607879638671875, 'Epoch': 36, 'Micro F1': 0.8011175322795978, 'Patience Count': 0, 'Best Val F1': 0.8011175322795978, 'Best Val Loss': 1.7607879638671875, 'Micro ROC value': 0.8817992304944116, 'Macro ROC value': 0.8817992304944116}\n",
            "Training Epoch Loss after 37 is 6.755190581083298\n",
            "{'Validation Epoch Loss': 1.754179298877716, 'Epoch': 37, 'Micro F1': 0.8012809141918197, 'Patience Count': 0, 'Best Val F1': 0.8012809141918197, 'Best Val Loss': 1.754179298877716, 'Micro ROC value': 0.8824110460150634, 'Macro ROC value': 0.8824110460150634}\n",
            "Training Epoch Loss after 38 is 6.717861324548721\n",
            "{'Validation Epoch Loss': 1.743188351392746, 'Epoch': 38, 'Micro F1': 0.8021491723539133, 'Patience Count': 0, 'Best Val F1': 0.8021491723539133, 'Best Val Loss': 1.743188351392746, 'Micro ROC value': 0.8832689144423131, 'Macro ROC value': 0.8832689144423131}\n",
            "Training Epoch Loss after 39 is 6.686425656080246\n",
            "{'Validation Epoch Loss': 1.7549103498458862, 'Epoch': 39, 'Micro F1': 0.8025412889432457, 'Patience Count': 0, 'Best Val F1': 0.8025412889432457, 'Best Val Loss': 1.743188351392746, 'Micro ROC value': 0.8848704737773417, 'Macro ROC value': 0.8848704737773417}\n",
            "Training Epoch Loss after 40 is 6.721319705247879\n",
            "{'Validation Epoch Loss': 1.7481151521205902, 'Epoch': 40, 'Micro F1': 0.8028213722213403, 'Patience Count': 0, 'Best Val F1': 0.8028213722213403, 'Best Val Loss': 1.743188351392746, 'Micro ROC value': 0.8843005148139111, 'Macro ROC value': 0.8843005148139111}\n",
            "Training Epoch Loss after 41 is 6.682778507471085\n",
            "{'Validation Epoch Loss': 1.7291334867477417, 'Epoch': 41, 'Micro F1': 0.8053627362652239, 'Patience Count': 0, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.8856896237574707, 'Macro ROC value': 0.8856896237574707}\n",
            "Training Epoch Loss after 42 is 6.713024258613586\n",
            "{'Validation Epoch Loss': 1.7507791221141815, 'Epoch': 42, 'Micro F1': 0.804492850373704, 'Patience Count': 1, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.8846076910534806, 'Macro ROC value': 0.8846076910534806}\n",
            "Training Epoch Loss after 43 is 6.6413061916828156\n",
            "{'Validation Epoch Loss': 1.7311391830444336, 'Epoch': 43, 'Micro F1': 0.8050900467739075, 'Patience Count': 2, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.886252770126477, 'Macro ROC value': 0.886252770126477}\n",
            "Training Epoch Loss after 44 is 6.71307036280632\n",
            "{'Validation Epoch Loss': 1.7425068318843842, 'Epoch': 44, 'Micro F1': 0.803899692842005, 'Patience Count': 3, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.8852914755100414, 'Macro ROC value': 0.8852914755100414}\n",
            "Training Epoch Loss after 45 is 6.721548706293106\n",
            "{'Validation Epoch Loss': 1.78634312748909, 'Epoch': 45, 'Micro F1': 0.7981999561192623, 'Patience Count': 4, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.886703480821526, 'Macro ROC value': 0.886703480821526}\n",
            "Training Epoch Loss after 46 is 6.7352171540260315\n",
            "{'Validation Epoch Loss': 1.7805128693580627, 'Epoch': 46, 'Micro F1': 0.8006647309800113, 'Patience Count': 5, 'Best Val F1': 0.8053627362652239, 'Best Val Loss': 1.7291334867477417, 'Micro ROC value': 0.8855397942171683, 'Macro ROC value': 0.8855397942171683}\n",
            "Training Epoch Loss after 47 is 6.666617840528488\n",
            "{'Validation Epoch Loss': 1.7057958841323853, 'Epoch': 47, 'Micro F1': 0.80873133318084, 'Patience Count': 0, 'Best Val F1': 0.80873133318084, 'Best Val Loss': 1.7057958841323853, 'Micro ROC value': 0.8886836250901824, 'Macro ROC value': 0.8886836250901824}\n",
            "Training Epoch Loss after 48 is 6.629133194684982\n",
            "{'Validation Epoch Loss': 1.7261885702610016, 'Epoch': 48, 'Micro F1': 0.804737208184148, 'Patience Count': 1, 'Best Val F1': 0.80873133318084, 'Best Val Loss': 1.7057958841323853, 'Micro ROC value': 0.8854250132491462, 'Macro ROC value': 0.8854250132491462}\n",
            "Training Epoch Loss after 49 is 6.578263014554977\n",
            "{'Validation Epoch Loss': 1.7411417961120605, 'Epoch': 49, 'Micro F1': 0.8047306065670192, 'Patience Count': 2, 'Best Val F1': 0.80873133318084, 'Best Val Loss': 1.7057958841323853, 'Micro ROC value': 0.8887624216520464, 'Macro ROC value': 0.8887624216520464}\n",
            "Training Epoch Loss after 50 is 6.485784143209457\n",
            "{'Validation Epoch Loss': 1.6999074220657349, 'Epoch': 50, 'Micro F1': 0.8098514625015171, 'Patience Count': 0, 'Best Val F1': 0.8098514625015171, 'Best Val Loss': 1.6999074220657349, 'Micro ROC value': 0.8901813525833077, 'Macro ROC value': 0.8901813525833077}\n",
            "Training Epoch Loss after 51 is 6.449282109737396\n",
            "{'Validation Epoch Loss': 1.6885318160057068, 'Epoch': 51, 'Micro F1': 0.81077863878256, 'Patience Count': 0, 'Best Val F1': 0.81077863878256, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8915485783578658, 'Macro ROC value': 0.8915485783578658}\n",
            "Training Epoch Loss after 52 is 6.436245679855347\n",
            "{'Validation Epoch Loss': 1.7121621966362, 'Epoch': 52, 'Micro F1': 0.8094079973112005, 'Patience Count': 1, 'Best Val F1': 0.81077863878256, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8915352590924615, 'Macro ROC value': 0.8915352590924615}\n",
            "Training Epoch Loss after 53 is 6.41003343462944\n",
            "{'Validation Epoch Loss': 1.7005391418933868, 'Epoch': 53, 'Micro F1': 0.8103789526752622, 'Patience Count': 2, 'Best Val F1': 0.81077863878256, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8912017732623703, 'Macro ROC value': 0.8912017732623703}\n",
            "Training Epoch Loss after 54 is 6.404960066080093\n",
            "{'Validation Epoch Loss': 1.6917010247707367, 'Epoch': 54, 'Micro F1': 0.8116439954813232, 'Patience Count': 0, 'Best Val F1': 0.8116439954813232, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8915483588973205, 'Macro ROC value': 0.8915483588973205}\n",
            "Training Epoch Loss after 55 is 6.396906405687332\n",
            "{'Validation Epoch Loss': 1.6948400139808655, 'Epoch': 55, 'Micro F1': 0.8117840371203705, 'Patience Count': 0, 'Best Val F1': 0.8117840371203705, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8915767765858604, 'Macro ROC value': 0.8915767765858604}\n",
            "Training Epoch Loss after 56 is 6.389333873987198\n",
            "{'Validation Epoch Loss': 1.7087702453136444, 'Epoch': 56, 'Micro F1': 0.8100746425420479, 'Patience Count': 1, 'Best Val F1': 0.8117840371203705, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.892887589005618, 'Macro ROC value': 0.892887589005618}\n",
            "Training Epoch Loss after 57 is 6.368602216243744\n",
            "{'Validation Epoch Loss': 1.6912568509578705, 'Epoch': 57, 'Micro F1': 0.8125061270219173, 'Patience Count': 0, 'Best Val F1': 0.8125061270219173, 'Best Val Loss': 1.6885318160057068, 'Micro ROC value': 0.8934944816097712, 'Macro ROC value': 0.8934944816097712}\n",
            "Training Epoch Loss after 58 is 6.347614109516144\n",
            "{'Validation Epoch Loss': 1.6814920604228973, 'Epoch': 58, 'Micro F1': 0.8130210715986219, 'Patience Count': 0, 'Best Val F1': 0.8130210715986219, 'Best Val Loss': 1.6814920604228973, 'Micro ROC value': 0.8924918789862974, 'Macro ROC value': 0.8924918789862974}\n",
            "Training Epoch Loss after 59 is 6.343823403120041\n",
            "{'Validation Epoch Loss': 1.676339030265808, 'Epoch': 59, 'Micro F1': 0.8140620477822073, 'Patience Count': 0, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8937076367303097, 'Macro ROC value': 0.8937076367303097}\n",
            "Training Epoch Loss after 60 is 6.328133970499039\n",
            "{'Validation Epoch Loss': 1.6951528191566467, 'Epoch': 60, 'Micro F1': 0.8120034356882113, 'Patience Count': 1, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8925487926792752, 'Macro ROC value': 0.8925487926792752}\n",
            "Training Epoch Loss after 61 is 6.3342545330524445\n",
            "{'Validation Epoch Loss': 1.6900894045829773, 'Epoch': 61, 'Micro F1': 0.8127830008122415, 'Patience Count': 2, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8929098946039624, 'Macro ROC value': 0.8929098946039624}\n",
            "Training Epoch Loss after 62 is 6.301568955183029\n",
            "{'Validation Epoch Loss': 1.6797526180744171, 'Epoch': 62, 'Micro F1': 0.8135392256630971, 'Patience Count': 3, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8937239303594807, 'Macro ROC value': 0.8937239303594807}\n",
            "Training Epoch Loss after 63 is 6.321859240531921\n",
            "{'Validation Epoch Loss': 1.6852364838123322, 'Epoch': 63, 'Micro F1': 0.8127409883205272, 'Patience Count': 4, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8929606107070992, 'Macro ROC value': 0.8929606107070992}\n",
            "Training Epoch Loss after 64 is 6.3165504932403564\n",
            "{'Validation Epoch Loss': 1.7062892615795135, 'Epoch': 64, 'Micro F1': 0.8134085201333198, 'Patience Count': 5, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8928965701802165, 'Macro ROC value': 0.8928965701802165}\n",
            "Training Epoch Loss after 65 is 6.290584415197372\n",
            "{'Validation Epoch Loss': 1.6776868402957916, 'Epoch': 65, 'Micro F1': 0.8138006367226522, 'Patience Count': 6, 'Best Val F1': 0.8140620477822073, 'Best Val Loss': 1.676339030265808, 'Micro ROC value': 0.8942115005185418, 'Macro ROC value': 0.8942115005185418}\n",
            "Training Epoch Loss after 66 is 6.267345070838928\n",
            "{'Validation Epoch Loss': 1.6635605990886688, 'Epoch': 66, 'Micro F1': 0.8163540626079486, 'Patience Count': 0, 'Best Val F1': 0.8163540626079486, 'Best Val Loss': 1.6635605990886688, 'Micro ROC value': 0.8959808700351894, 'Macro ROC value': 0.8959808700351894}\n",
            "Training Epoch Loss after 67 is 6.250980377197266\n",
            "{'Validation Epoch Loss': 1.660190612077713, 'Epoch': 67, 'Micro F1': 0.8155604933200137, 'Patience Count': 0, 'Best Val F1': 0.8163540626079486, 'Best Val Loss': 1.660190612077713, 'Micro ROC value': 0.8949685991342524, 'Macro ROC value': 0.8949685991342524}\n",
            "Training Epoch Loss after 68 is 6.2164290845394135\n",
            "{'Validation Epoch Loss': 1.668316125869751, 'Epoch': 68, 'Micro F1': 0.8159142897156996, 'Patience Count': 1, 'Best Val F1': 0.8163540626079486, 'Best Val Loss': 1.660190612077713, 'Micro ROC value': 0.8953681291760889, 'Macro ROC value': 0.8953681291760889}\n",
            "Training Epoch Loss after 69 is 6.2371048629283905\n",
            "{'Validation Epoch Loss': 1.6805011630058289, 'Epoch': 69, 'Micro F1': 0.8152010531131256, 'Patience Count': 2, 'Best Val F1': 0.8163540626079486, 'Best Val Loss': 1.660190612077713, 'Micro ROC value': 0.8948824638053917, 'Macro ROC value': 0.8948824638053917}\n",
            "Training Epoch Loss after 70 is 6.2285444140434265\n",
            "{'Validation Epoch Loss': 1.6772507727146149, 'Epoch': 70, 'Micro F1': 0.8157378793961403, 'Patience Count': 3, 'Best Val F1': 0.8163540626079486, 'Best Val Loss': 1.660190612077713, 'Micro ROC value': 0.8952979771930631, 'Macro ROC value': 0.8952979771930631}\n",
            "Training Epoch Loss after 71 is 6.229647666215897\n",
            "{'Validation Epoch Loss': 1.6684120297431946, 'Epoch': 71, 'Micro F1': 0.8164894361923611, 'Patience Count': 0, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.660190612077713, 'Micro ROC value': 0.8954228016652027, 'Macro ROC value': 0.8954228016652027}\n",
            "Training Epoch Loss after 72 is 6.210937142372131\n",
            "{'Validation Epoch Loss': 1.6572186052799225, 'Epoch': 72, 'Micro F1': 0.8159508916067594, 'Patience Count': 0, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8954764883042438, 'Macro ROC value': 0.8954764883042438}\n",
            "Training Epoch Loss after 73 is 6.201534956693649\n",
            "{'Validation Epoch Loss': 1.6786912381649017, 'Epoch': 73, 'Micro F1': 0.8142767782954131, 'Patience Count': 1, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8949008238498751, 'Macro ROC value': 0.8949008238498751}\n",
            "Training Epoch Loss after 74 is 6.2178599536418915\n",
            "{'Validation Epoch Loss': 1.6850199699401855, 'Epoch': 74, 'Micro F1': 0.815555825265379, 'Patience Count': 2, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8947009124740282, 'Macro ROC value': 0.8947009124740282}\n",
            "Training Epoch Loss after 75 is 6.2455847561359406\n",
            "{'Validation Epoch Loss': 1.6595237255096436, 'Epoch': 75, 'Micro F1': 0.8155744974839185, 'Patience Count': 3, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8954285894584987, 'Macro ROC value': 0.8954285894584987}\n",
            "Training Epoch Loss after 76 is 6.202297955751419\n",
            "{'Validation Epoch Loss': 1.6668146550655365, 'Epoch': 76, 'Micro F1': 0.814813604578428, 'Patience Count': 4, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8950873951126472, 'Macro ROC value': 0.8950873951126472}\n",
            "Training Epoch Loss after 77 is 6.193670243024826\n",
            "{'Validation Epoch Loss': 1.6670973896980286, 'Epoch': 77, 'Micro F1': 0.8158079002156641, 'Patience Count': 5, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8958542953748705, 'Macro ROC value': 0.8958542953748705}\n",
            "Training Epoch Loss after 78 is 6.212976425886154\n",
            "{'Validation Epoch Loss': 1.6588504016399384, 'Epoch': 78, 'Micro F1': 0.8162244931304766, 'Patience Count': 6, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8958366983956134, 'Macro ROC value': 0.8958366983956134}\n",
            "Training Epoch Loss after 79 is 6.212056428194046\n",
            "{'Validation Epoch Loss': 1.6678749918937683, 'Epoch': 79, 'Micro F1': 0.8153270905882682, 'Patience Count': 7, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8952728865138018, 'Macro ROC value': 0.8952728865138018}\n",
            "Training Epoch Loss after 80 is 6.2106437385082245\n",
            "{'Validation Epoch Loss': 1.683581292629242, 'Epoch': 80, 'Micro F1': 0.8124655730970676, 'Patience Count': 8, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.895812760971862, 'Macro ROC value': 0.895812760971862}\n",
            "Training Epoch Loss after 81 is 6.224528878927231\n",
            "{'Validation Epoch Loss': 1.6722891330718994, 'Epoch': 81, 'Micro F1': 0.8147109073764599, 'Patience Count': 9, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8964606012391559, 'Macro ROC value': 0.8964606012391559}\n",
            "Training Epoch Loss after 82 is 6.1942673325538635\n",
            "{'Validation Epoch Loss': 1.738460123538971, 'Epoch': 82, 'Micro F1': 0.8066165006395235, 'Patience Count': 10, 'Best Val F1': 0.8164894361923611, 'Best Val Loss': 1.6572186052799225, 'Micro ROC value': 0.8957051408072517, 'Macro ROC value': 0.8957051408072517}\n",
            "Epoch    83: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Training Epoch Loss after 83 is 6.273928463459015\n",
            "{'Validation Epoch Loss': 1.641576737165451, 'Epoch': 83, 'Micro F1': 0.8188374676737217, 'Patience Count': 0, 'Best Val F1': 0.8188374676737217, 'Best Val Loss': 1.641576737165451, 'Micro ROC value': 0.8981669916991591, 'Macro ROC value': 0.8981669916991591}\n",
            "Training Epoch Loss after 84 is 6.046986162662506\n",
            "{'Validation Epoch Loss': 1.6516188383102417, 'Epoch': 84, 'Micro F1': 0.818501367740008, 'Patience Count': 1, 'Best Val F1': 0.8188374676737217, 'Best Val Loss': 1.641576737165451, 'Micro ROC value': 0.8982964818289529, 'Macro ROC value': 0.8982964818289529}\n",
            "Training Epoch Loss after 85 is 5.991290807723999\n",
            "{'Validation Epoch Loss': 1.6218494474887848, 'Epoch': 85, 'Micro F1': 0.8219650642791123, 'Patience Count': 0, 'Best Val F1': 0.8219650642791123, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9003581565234601, 'Macro ROC value': 0.9003581565234601}\n",
            "Training Epoch Loss after 86 is 5.986097872257233\n",
            "{'Validation Epoch Loss': 1.6362324655056, 'Epoch': 86, 'Micro F1': 0.8208867436584478, 'Patience Count': 1, 'Best Val F1': 0.8219650642791123, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.8995045846624108, 'Macro ROC value': 0.8995045846624108}\n",
            "Training Epoch Loss after 87 is 5.9817474484443665\n",
            "{'Validation Epoch Loss': 1.6303936541080475, 'Epoch': 87, 'Micro F1': 0.8206126469297631, 'Patience Count': 2, 'Best Val F1': 0.8219650642791123, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.900244263613053, 'Macro ROC value': 0.900244263613053}\n",
            "Training Epoch Loss after 88 is 5.984990239143372\n",
            "{'Validation Epoch Loss': 1.626112550497055, 'Epoch': 88, 'Micro F1': 0.8213255407941294, 'Patience Count': 3, 'Best Val F1': 0.8219650642791123, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9004478341540518, 'Macro ROC value': 0.9004478341540518}\n",
            "Training Epoch Loss after 89 is 5.961739599704742\n",
            "{'Validation Epoch Loss': 1.6233512461185455, 'Epoch': 89, 'Micro F1': 0.8220584253718105, 'Patience Count': 0, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9009018789636689, 'Macro ROC value': 0.9009018789636689}\n",
            "Training Epoch Loss after 90 is 5.961516469717026\n",
            "{'Validation Epoch Loss': 1.6288810968399048, 'Epoch': 90, 'Micro F1': 0.8208447311667336, 'Patience Count': 1, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.8999577737499008, 'Macro ROC value': 0.8999577737499008}\n",
            "Training Epoch Loss after 91 is 5.940132111310959\n",
            "{'Validation Epoch Loss': 1.627713918685913, 'Epoch': 91, 'Micro F1': 0.820770042292575, 'Patience Count': 2, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9000629421954388, 'Macro ROC value': 0.9000629421954388}\n",
            "Training Epoch Loss after 92 is 5.942693501710892\n",
            "{'Validation Epoch Loss': 1.6267155706882477, 'Epoch': 92, 'Micro F1': 0.8211574908272726, 'Patience Count': 3, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9004218656132777, 'Macro ROC value': 0.9004218656132777}\n",
            "Training Epoch Loss after 93 is 5.9432953000068665\n",
            "{'Validation Epoch Loss': 1.6368406414985657, 'Epoch': 93, 'Micro F1': 0.8199437966221956, 'Patience Count': 4, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.8989598152210111, 'Macro ROC value': 0.8989598152210111}\n",
            "Training Epoch Loss after 94 is 5.943408370018005\n",
            "{'Validation Epoch Loss': 1.6279590427875519, 'Epoch': 94, 'Micro F1': 0.8210828019531139, 'Patience Count': 5, 'Best Val F1': 0.8220584253718105, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9001347146849118, 'Macro ROC value': 0.9001347146849118}\n",
            "Training Epoch Loss after 95 is 5.9175290167331696\n",
            "{'Validation Epoch Loss': 1.6226926445960999, 'Epoch': 95, 'Micro F1': 0.8222171392293975, 'Patience Count': 0, 'Best Val F1': 0.8222171392293975, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9007012808310471, 'Macro ROC value': 0.9007012808310471}\n",
            "Training Epoch Loss after 96 is 5.935365617275238\n",
            "{'Validation Epoch Loss': 1.6260280907154083, 'Epoch': 96, 'Micro F1': 0.8222264753386673, 'Patience Count': 0, 'Best Val F1': 0.8222264753386673, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9007048935168493, 'Macro ROC value': 0.9007048935168493}\n",
            "Training Epoch Loss after 97 is 5.921176761388779\n",
            "{'Validation Epoch Loss': 1.6259194016456604, 'Epoch': 97, 'Micro F1': 0.8219650642791123, 'Patience Count': 1, 'Best Val F1': 0.8222264753386673, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9007347106063313, 'Macro ROC value': 0.9007347106063313}\n",
            "Training Epoch Loss after 98 is 5.925404608249664\n",
            "{'Validation Epoch Loss': 1.6273762881755829, 'Epoch': 98, 'Micro F1': 0.821666308782478, 'Patience Count': 2, 'Best Val F1': 0.8222264753386673, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9006856157338097, 'Macro ROC value': 0.9006856157338097}\n",
            "Training Epoch Loss after 99 is 5.916398257017136\n",
            "{'Validation Epoch Loss': 1.6227542459964752, 'Epoch': 99, 'Micro F1': 0.8219697323337473, 'Patience Count': 3, 'Best Val F1': 0.8222264753386673, 'Best Val Loss': 1.6218494474887848, 'Micro ROC value': 0.9010167960170727, 'Macro ROC value': 0.9010167960170727}\n",
            "Training Epoch Loss after 100 is 5.90434804558754\n",
            "{'Validation Epoch Loss': 1.6173976957798004, 'Epoch': 100, 'Micro F1': 0.823020293075519, 'Patience Count': 0, 'Best Val F1': 0.823020293075519, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.901765181579349, 'Macro ROC value': 0.901765181579349}\n",
            "Training Epoch Loss after 101 is 5.905586838722229\n",
            "{'Validation Epoch Loss': 1.6178112030029297, 'Epoch': 101, 'Micro F1': 0.8234028251066651, 'Patience Count': 0, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9018841711750403, 'Macro ROC value': 0.9018841711750403}\n",
            "Training Epoch Loss after 102 is 5.908019989728928\n",
            "{'Validation Epoch Loss': 1.6254412531852722, 'Epoch': 102, 'Micro F1': 0.8219370559513028, 'Patience Count': 1, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9007104627490826, 'Macro ROC value': 0.9007104627490826}\n",
            "Training Epoch Loss after 103 is 5.913664996623993\n",
            "{'Validation Epoch Loss': 1.622974008321762, 'Epoch': 103, 'Micro F1': 0.8229360196431739, 'Patience Count': 2, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.901637947690576, 'Macro ROC value': 0.901637947690576}\n",
            "Training Epoch Loss after 104 is 5.906568735837936\n",
            "{'Validation Epoch Loss': 1.621831625699997, 'Epoch': 104, 'Micro F1': 0.8228800029875549, 'Patience Count': 3, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9014033903549947, 'Macro ROC value': 0.9014033903549947}\n",
            "Training Epoch Loss after 105 is 5.900726765394211\n",
            "{'Validation Epoch Loss': 1.6260015368461609, 'Epoch': 105, 'Micro F1': 0.8219230517873981, 'Patience Count': 4, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9007392139665982, 'Macro ROC value': 0.9007392139665982}\n",
            "Training Epoch Loss after 106 is 5.887302666902542\n",
            "{'Validation Epoch Loss': 1.6251592636108398, 'Epoch': 106, 'Micro F1': 0.8214889227063513, 'Patience Count': 5, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9005942800462623, 'Macro ROC value': 0.9005942800462623}\n",
            "Training Epoch Loss after 107 is 5.886903285980225\n",
            "{'Validation Epoch Loss': 1.6234217882156372, 'Epoch': 107, 'Micro F1': 0.8220886102539924, 'Patience Count': 6, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9010946415792915, 'Macro ROC value': 0.9010946415792915}\n",
            "Training Epoch Loss after 108 is 5.892572671175003\n",
            "{'Validation Epoch Loss': 1.6207764148712158, 'Epoch': 108, 'Micro F1': 0.8220641233917801, 'Patience Count': 7, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9012851414571984, 'Macro ROC value': 0.9012851414571984}\n",
            "Training Epoch Loss after 109 is 5.901933819055557\n",
            "{'Validation Epoch Loss': 1.6198690831661224, 'Epoch': 109, 'Micro F1': 0.8219554094932218, 'Patience Count': 8, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9013903136838181, 'Macro ROC value': 0.9013903136838181}\n",
            "Training Epoch Loss after 110 is 5.88738825917244\n",
            "{'Validation Epoch Loss': 1.6284766793251038, 'Epoch': 110, 'Micro F1': 0.8218576990225094, 'Patience Count': 9, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6173976957798004, 'Micro ROC value': 0.9009742873503437, 'Macro ROC value': 0.9009742873503437}\n",
            "Training Epoch Loss after 111 is 5.888012766838074\n",
            "{'Validation Epoch Loss': 1.6168958246707916, 'Epoch': 111, 'Micro F1': 0.8225999197094602, 'Patience Count': 0, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9017735636751143, 'Macro ROC value': 0.9017735636751143}\n",
            "Training Epoch Loss after 112 is 5.877794981002808\n",
            "{'Validation Epoch Loss': 1.6188545525074005, 'Epoch': 112, 'Micro F1': 0.8226979488567934, 'Patience Count': 1, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9015823508988212, 'Macro ROC value': 0.9015823508988212}\n",
            "Epoch   113: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Training Epoch Loss after 113 is 5.872698873281479\n",
            "{'Validation Epoch Loss': 1.6193671524524689, 'Epoch': 113, 'Micro F1': 0.8223268972213923, 'Patience Count': 2, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9015627617737538, 'Macro ROC value': 0.9015627617737538}\n",
            "Training Epoch Loss after 114 is 5.87240731716156\n",
            "{'Validation Epoch Loss': 1.6225588023662567, 'Epoch': 114, 'Micro F1': 0.8222731558850165, 'Patience Count': 3, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9012107250519498, 'Macro ROC value': 0.9012107250519498}\n",
            "Training Epoch Loss after 115 is 5.87466162443161\n",
            "{'Validation Epoch Loss': 1.6183356046676636, 'Epoch': 115, 'Micro F1': 0.8227492974577775, 'Patience Count': 4, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9016689981356067, 'Macro ROC value': 0.9016689981356067}\n",
            "Training Epoch Loss after 116 is 5.865462690591812\n",
            "{'Validation Epoch Loss': 1.6232589185237885, 'Epoch': 116, 'Micro F1': 0.8223431767045402, 'Patience Count': 5, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9015539731744877, 'Macro ROC value': 0.9015539731744877}\n",
            "Training Epoch Loss after 117 is 5.867570877075195\n",
            "{'Validation Epoch Loss': 1.624354988336563, 'Epoch': 117, 'Micro F1': 0.8216476365639384, 'Patience Count': 6, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9008805741629642, 'Macro ROC value': 0.9008805741629642}\n",
            "Training Epoch Loss after 118 is 5.870138704776764\n",
            "{'Validation Epoch Loss': 1.6195384860038757, 'Epoch': 118, 'Micro F1': 0.8229593599163485, 'Patience Count': 7, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9014969795966787, 'Macro ROC value': 0.9014969795966787}\n",
            "Training Epoch Loss after 119 is 5.86014199256897\n",
            "{'Validation Epoch Loss': 1.6214474439620972, 'Epoch': 119, 'Micro F1': 0.8224925544528573, 'Patience Count': 8, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9014343190769849, 'Macro ROC value': 0.9014343190769849}\n",
            "Training Epoch Loss after 120 is 5.875632703304291\n",
            "{'Validation Epoch Loss': 1.6210305988788605, 'Epoch': 120, 'Micro F1': 0.8225392349992064, 'Patience Count': 9, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9017626962873796, 'Macro ROC value': 0.9017626962873796}\n",
            "Training Epoch Loss after 121 is 5.881654232740402\n",
            "{'Validation Epoch Loss': 1.622478723526001, 'Epoch': 121, 'Micro F1': 0.8224925544528573, 'Patience Count': 10, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9010087686592916, 'Macro ROC value': 0.9010087686592916}\n",
            "Training Epoch Loss after 122 is 5.875243037939072\n",
            "{'Validation Epoch Loss': 1.6173779666423798, 'Epoch': 122, 'Micro F1': 0.8230444606277891, 'Patience Count': 11, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9019376214841104, 'Macro ROC value': 0.9019376214841104}\n",
            "Training Epoch Loss after 123 is 5.866873621940613\n",
            "{'Validation Epoch Loss': 1.620811641216278, 'Epoch': 123, 'Micro F1': 0.8223945253055241, 'Patience Count': 12, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9014968651517434, 'Macro ROC value': 0.9014968651517434}\n",
            "Epoch   124: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Training Epoch Loss after 124 is 5.8799382746219635\n",
            "{'Validation Epoch Loss': 1.622545450925827, 'Epoch': 124, 'Micro F1': 0.8231414140471099, 'Patience Count': 13, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9015227385105912, 'Macro ROC value': 0.9015227385105912}\n",
            "Training Epoch Loss after 125 is 5.86439111828804\n",
            "{'Validation Epoch Loss': 1.6260827481746674, 'Epoch': 125, 'Micro F1': 0.8222171392293975, 'Patience Count': 14, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9010179816509138, 'Macro ROC value': 0.9010179816509138}\n",
            "Training Epoch Loss after 126 is 5.873319029808044\n",
            "{'Validation Epoch Loss': 1.6234160363674164, 'Epoch': 126, 'Micro F1': 0.8221004378635247, 'Patience Count': 15, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9006936325884572, 'Macro ROC value': 0.9006936325884572}\n",
            "Training Epoch Loss after 127 is 5.868245780467987\n",
            "{'Validation Epoch Loss': 1.6216380894184113, 'Epoch': 127, 'Micro F1': 0.8218662035934853, 'Patience Count': 16, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.901202990322241, 'Macro ROC value': 0.901202990322241}\n",
            "Training Epoch Loss after 128 is 5.876859277486801\n",
            "{'Validation Epoch Loss': 1.6198568940162659, 'Epoch': 128, 'Micro F1': 0.8232581154129828, 'Patience Count': 17, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9017077109000414, 'Macro ROC value': 0.9017077109000414}\n",
            "Training Epoch Loss after 129 is 5.876934975385666\n",
            "{'Validation Epoch Loss': 1.6179036796092987, 'Epoch': 129, 'Micro F1': 0.8233841528881255, 'Patience Count': 18, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9019191604259993, 'Macro ROC value': 0.9019191604259993}\n",
            "Training Epoch Loss after 130 is 5.866884082555771\n",
            "{'Validation Epoch Loss': 1.6205567419528961, 'Epoch': 130, 'Micro F1': 0.8231647543202846, 'Patience Count': 19, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9018673991956747, 'Macro ROC value': 0.9018673991956747}\n",
            "Training Epoch Loss after 131 is 5.862154334783554\n",
            "{'Validation Epoch Loss': 1.6192336976528168, 'Epoch': 131, 'Micro F1': 0.8228800029875549, 'Patience Count': 20, 'Best Val F1': 0.8234028251066651, 'Best Val Loss': 1.6168958246707916, 'Micro ROC value': 0.9019977755982048, 'Macro ROC value': 0.9019977755982048}\n",
            "Testing stats.\n",
            "\n",
            "\n",
            "\n",
            "{'Micro F1': 0.8271730486274097, 'Micro Recall': 0.8271730486274097, 'Micro Precision': 0.8271730486274097, 'Micro ROC_AUC_Score': 0.9065509491134653, 'Macro ROC_AUC_Score': 0.9065509491134653}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Average Statistics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "oZbh4OoTo3m_",
        "outputId": "92ba143b-b2e5-4536-d590-42b144f39767"
      },
      "source": [
        "average[\"avg_test_micro_f1\"] = average[\"avg_test_micro_f1\"]/5\n",
        "average[\"avg_test_loss\"] = average[\"avg_test_loss\"]/5\n",
        "average[\"avg_test_micro_recall\"] = average[\"avg_test_micro_recall\"]/5\n",
        "average[\"avg_test_micro_precision\"] = average[\"avg_test_micro_precision\"]/5\n",
        "average[\"avg_test_micro_roc_auc_score\"] = average[\"avg_test_micro_roc_auc_score\"]/5\n",
        "\n",
        "\n",
        "print(average)\n",
        "time_taken = (end - begin)\n",
        "print(f\"Time taken is :{time_taken}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41afce006f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_f1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_f1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_recall\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_recall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_precision\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_roc_auc_score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_test_micro_roc_auc_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'average' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "hHjNn-0mRt5L",
        "outputId": "4f05ce3d-9d4e-455d-e9a6-3505a02ee0c9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "filepath = config['path']+\"/2_4_4_50_64_64\" #\"numlayers_heads_emdedsize\"\n",
        "\n",
        "plt.title(\"Validation accuracy vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_acc_fin[i]))\n",
        "  y_axis= val_acc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_acc_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Validation f1 score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_f1_fin[i]))\n",
        "  y_axis= val_f1_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_f1_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.title(\"Validation roc_auc score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(val_roc_auc_fin[i]))\n",
        "  y_axis= val_roc_auc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"val_roc_auc_vs_epoch.png\" )\n",
        "plt.show()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "9vzPXRs4UQHW",
        "outputId": "a558d18c-8953-47fb-8ad3-ecf4f5a4ccb9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Training accuracy vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_acc_fin[i]))\n",
        "  y_axis= train_acc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_acc_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Training f1 score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_f1_fin[i]))\n",
        "  y_axis= train_f1_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_f1_vs_epoch.png\" )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.title(\"Training roc_auc score vs epoch\")\n",
        "colors = [\"red\",\"blue\",\"green\",\"orange\",'purple']\n",
        "for i in range(0,5):\n",
        "  x_axis = np.arange(len(train_roc_auc_fin[i]))\n",
        "  y_axis= train_roc_auc_fin[i]\n",
        "  plt.plot(x_axis, y_axis, color = colors[i],label=str(i));\n",
        "  plt.legend()\n",
        "plt.savefig(filepath+\"train_roc_auc_vs_epoch.png\" )\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}